{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  0 Loss:  515.25\n",
      "Training Step:  1 Loss:  91.30376\n",
      "Training Step:  2 Loss:  18.027546\n",
      "Training Step:  3 Loss:  5.3485394\n",
      "Training Step:  4 Loss:  3.1410758\n",
      "Training Step:  5 Loss:  2.7432587\n",
      "Training Step:  6 Loss:  2.6582754\n",
      "Training Step:  7 Loss:  2.6274714\n",
      "Training Step:  8 Loss:  2.6061482\n",
      "Training Step:  9 Loss:  2.5865815\n",
      "Training Step:  10 Loss:  2.5674355\n",
      "Training Step:  11 Loss:  2.548478\n",
      "Training Step:  12 Loss:  2.5296695\n",
      "Training Step:  13 Loss:  2.5110002\n",
      "Training Step:  14 Loss:  2.4924705\n",
      "Training Step:  15 Loss:  2.4740758\n",
      "Training Step:  16 Loss:  2.4558182\n",
      "Training Step:  17 Loss:  2.4376943\n",
      "Training Step:  18 Loss:  2.4197052\n",
      "Training Step:  19 Loss:  2.4018486\n",
      "Training Step:  20 Loss:  2.3841238\n",
      "Training Step:  21 Loss:  2.3665295\n",
      "Training Step:  22 Loss:  2.3490648\n",
      "Training Step:  23 Loss:  2.3317294\n",
      "Training Step:  24 Loss:  2.3145218\n",
      "Training Step:  25 Loss:  2.2974412\n",
      "Training Step:  26 Loss:  2.280486\n",
      "Training Step:  27 Loss:  2.2636576\n",
      "Training Step:  28 Loss:  2.2469518\n",
      "Training Step:  29 Loss:  2.2303693\n",
      "Training Step:  30 Loss:  2.2139103\n",
      "Training Step:  31 Loss:  2.1975713\n",
      "Training Step:  32 Loss:  2.1813543\n",
      "Training Step:  33 Loss:  2.165257\n",
      "Training Step:  34 Loss:  2.1492774\n",
      "Training Step:  35 Loss:  2.1334167\n",
      "Training Step:  36 Loss:  2.117672\n",
      "Training Step:  37 Loss:  2.1020448\n",
      "Training Step:  38 Loss:  2.086532\n",
      "Training Step:  39 Loss:  2.0711339\n",
      "Training Step:  40 Loss:  2.0558488\n",
      "Training Step:  41 Loss:  2.040678\n",
      "Training Step:  42 Loss:  2.0256178\n",
      "Training Step:  43 Loss:  2.010669\n",
      "Training Step:  44 Loss:  1.9958313\n",
      "Training Step:  45 Loss:  1.9811025\n",
      "Training Step:  46 Loss:  1.9664824\n",
      "Training Step:  47 Loss:  1.9519701\n",
      "Training Step:  48 Loss:  1.9375653\n",
      "Training Step:  49 Loss:  1.9232666\n",
      "Training Step:  50 Loss:  1.9090732\n",
      "Training Step:  51 Loss:  1.8949842\n",
      "Training Step:  52 Loss:  1.881\n",
      "Training Step:  53 Loss:  1.867119\n",
      "Training Step:  54 Loss:  1.8533397\n",
      "Training Step:  55 Loss:  1.839663\n",
      "Training Step:  56 Loss:  1.8260864\n",
      "Training Step:  57 Loss:  1.81261\n",
      "Training Step:  58 Loss:  1.7992336\n",
      "Training Step:  59 Loss:  1.7859559\n",
      "Training Step:  60 Loss:  1.7727761\n",
      "Training Step:  61 Loss:  1.7596931\n",
      "Training Step:  62 Loss:  1.7467073\n",
      "Training Step:  63 Loss:  1.7338164\n",
      "Training Step:  64 Loss:  1.7210218\n",
      "Training Step:  65 Loss:  1.7083211\n",
      "Training Step:  66 Loss:  1.6957146\n",
      "Training Step:  67 Loss:  1.6832\n",
      "Training Step:  68 Loss:  1.6707785\n",
      "Training Step:  69 Loss:  1.6584485\n",
      "Training Step:  70 Loss:  1.64621\n",
      "Training Step:  71 Loss:  1.6340607\n",
      "Training Step:  72 Loss:  1.6220018\n",
      "Training Step:  73 Loss:  1.6100321\n",
      "Training Step:  74 Loss:  1.5981501\n",
      "Training Step:  75 Loss:  1.5863562\n",
      "Training Step:  76 Loss:  1.5746496\n",
      "Training Step:  77 Loss:  1.563029\n",
      "Training Step:  78 Loss:  1.5514946\n",
      "Training Step:  79 Loss:  1.5400444\n",
      "Training Step:  80 Loss:  1.5286791\n",
      "Training Step:  81 Loss:  1.5173981\n",
      "Training Step:  82 Loss:  1.5061996\n",
      "Training Step:  83 Loss:  1.4950848\n",
      "Training Step:  84 Loss:  1.4840512\n",
      "Training Step:  85 Loss:  1.4730996\n",
      "Training Step:  86 Loss:  1.4622284\n",
      "Training Step:  87 Loss:  1.4514376\n",
      "Training Step:  88 Loss:  1.4407256\n",
      "Training Step:  89 Loss:  1.430094\n",
      "Training Step:  90 Loss:  1.4195403\n",
      "Training Step:  91 Loss:  1.4090643\n",
      "Training Step:  92 Loss:  1.3986661\n",
      "Training Step:  93 Loss:  1.3883437\n",
      "Training Step:  94 Loss:  1.3780986\n",
      "Training Step:  95 Loss:  1.367928\n",
      "Training Step:  96 Loss:  1.3578335\n",
      "Training Step:  97 Loss:  1.3478128\n",
      "Training Step:  98 Loss:  1.3378661\n",
      "Training Step:  99 Loss:  1.327993\n",
      "Training Step:  100 Loss:  1.3181932\n",
      "Training Step:  101 Loss:  1.3084652\n",
      "Training Step:  102 Loss:  1.298809\n",
      "Training Step:  103 Loss:  1.2892241\n",
      "Training Step:  104 Loss:  1.2797098\n",
      "Training Step:  105 Loss:  1.2702658\n",
      "Training Step:  106 Loss:  1.2608917\n",
      "Training Step:  107 Loss:  1.2515867\n",
      "Training Step:  108 Loss:  1.24235\n",
      "Training Step:  109 Loss:  1.2331816\n",
      "Training Step:  110 Loss:  1.224081\n",
      "Training Step:  111 Loss:  1.215048\n",
      "Training Step:  112 Loss:  1.2060813\n",
      "Training Step:  113 Loss:  1.1971803\n",
      "Training Step:  114 Loss:  1.1883452\n",
      "Training Step:  115 Loss:  1.1795757\n",
      "Training Step:  116 Loss:  1.1708709\n",
      "Training Step:  117 Loss:  1.1622305\n",
      "Training Step:  118 Loss:  1.1536534\n",
      "Training Step:  119 Loss:  1.1451396\n",
      "Training Step:  120 Loss:  1.1366886\n",
      "Training Step:  121 Loss:  1.1283008\n",
      "Training Step:  122 Loss:  1.1199738\n",
      "Training Step:  123 Loss:  1.1117089\n",
      "Training Step:  124 Loss:  1.1035041\n",
      "Training Step:  125 Loss:  1.0953605\n",
      "Training Step:  126 Loss:  1.0872769\n",
      "Training Step:  127 Loss:  1.0792538\n",
      "Training Step:  128 Loss:  1.0712885\n",
      "Training Step:  129 Loss:  1.0633831\n",
      "Training Step:  130 Loss:  1.0555357\n",
      "Training Step:  131 Loss:  1.047746\n",
      "Training Step:  132 Loss:  1.0400136\n",
      "Training Step:  133 Loss:  1.0323384\n",
      "Training Step:  134 Loss:  1.02472\n",
      "Training Step:  135 Loss:  1.017158\n",
      "Training Step:  136 Loss:  1.0096517\n",
      "Training Step:  137 Loss:  1.0022008\n",
      "Training Step:  138 Loss:  0.9948046\n",
      "Training Step:  139 Loss:  0.987463\n",
      "Training Step:  140 Loss:  0.9801761\n",
      "Training Step:  141 Loss:  0.9729426\n",
      "Training Step:  142 Loss:  0.96576226\n",
      "Training Step:  143 Loss:  0.95863557\n",
      "Training Step:  144 Loss:  0.95156115\n",
      "Training Step:  145 Loss:  0.94453865\n",
      "Training Step:  146 Loss:  0.937568\n",
      "Training Step:  147 Loss:  0.93064916\n",
      "Training Step:  148 Loss:  0.92378104\n",
      "Training Step:  149 Loss:  0.916964\n",
      "Training Step:  150 Loss:  0.9101972\n",
      "Training Step:  151 Loss:  0.9034805\n",
      "Training Step:  152 Loss:  0.8968128\n",
      "Training Step:  153 Loss:  0.8901945\n",
      "Training Step:  154 Loss:  0.88362473\n",
      "Training Step:  155 Loss:  0.87710416\n",
      "Training Step:  156 Loss:  0.870631\n",
      "Training Step:  157 Loss:  0.8642061\n",
      "Training Step:  158 Loss:  0.8578282\n",
      "Training Step:  159 Loss:  0.8514977\n",
      "Training Step:  160 Loss:  0.84521437\n",
      "Training Step:  161 Loss:  0.8389768\n",
      "Training Step:  162 Loss:  0.8327854\n",
      "Training Step:  163 Loss:  0.82663953\n",
      "Training Step:  164 Loss:  0.8205392\n",
      "Training Step:  165 Loss:  0.81448376\n",
      "Training Step:  166 Loss:  0.80847275\n",
      "Training Step:  167 Loss:  0.8025069\n",
      "Training Step:  168 Loss:  0.7965847\n",
      "Training Step:  169 Loss:  0.7907059\n",
      "Training Step:  170 Loss:  0.78487056\n",
      "Training Step:  171 Loss:  0.7790786\n",
      "Training Step:  172 Loss:  0.7733294\n",
      "Training Step:  173 Loss:  0.76762223\n",
      "Training Step:  174 Loss:  0.76195765\n",
      "Training Step:  175 Loss:  0.7563342\n",
      "Training Step:  176 Loss:  0.7507527\n",
      "Training Step:  177 Loss:  0.7452123\n",
      "Training Step:  178 Loss:  0.7397127\n",
      "Training Step:  179 Loss:  0.7342539\n",
      "Training Step:  180 Loss:  0.7288353\n",
      "Training Step:  181 Loss:  0.72345704\n",
      "Training Step:  182 Loss:  0.71811783\n",
      "Training Step:  183 Loss:  0.7128182\n",
      "Training Step:  184 Loss:  0.7075579\n",
      "Training Step:  185 Loss:  0.70233625\n",
      "Training Step:  186 Loss:  0.6971533\n",
      "Training Step:  187 Loss:  0.69200814\n",
      "Training Step:  188 Loss:  0.68690157\n",
      "Training Step:  189 Loss:  0.6818323\n",
      "Training Step:  190 Loss:  0.6768003\n",
      "Training Step:  191 Loss:  0.67180556\n",
      "Training Step:  192 Loss:  0.6668477\n",
      "Training Step:  193 Loss:  0.66192675\n",
      "Training Step:  194 Loss:  0.657042\n",
      "Training Step:  195 Loss:  0.6521931\n",
      "Training Step:  196 Loss:  0.6473802\n",
      "Training Step:  197 Loss:  0.6426027\n",
      "Training Step:  198 Loss:  0.63786054\n",
      "Training Step:  199 Loss:  0.6331535\n",
      "Training Step:  200 Loss:  0.6284805\n",
      "Training Step:  201 Loss:  0.62384254\n",
      "Training Step:  202 Loss:  0.61923873\n",
      "Training Step:  203 Loss:  0.6146691\n",
      "Training Step:  204 Loss:  0.6101329\n",
      "Training Step:  205 Loss:  0.6056302\n",
      "Training Step:  206 Loss:  0.60116065\n",
      "Training Step:  207 Loss:  0.59672403\n",
      "Training Step:  208 Loss:  0.5923206\n",
      "Training Step:  209 Loss:  0.5879495\n",
      "Training Step:  210 Loss:  0.58361036\n",
      "Training Step:  211 Loss:  0.5793038\n",
      "Training Step:  212 Loss:  0.5750287\n",
      "Training Step:  213 Loss:  0.5707849\n",
      "Training Step:  214 Loss:  0.566573\n",
      "Training Step:  215 Loss:  0.56239146\n",
      "Training Step:  216 Loss:  0.5582412\n",
      "Training Step:  217 Loss:  0.55412173\n",
      "Training Step:  218 Loss:  0.5500322\n",
      "Training Step:  219 Loss:  0.54597336\n",
      "Training Step:  220 Loss:  0.5419441\n",
      "Training Step:  221 Loss:  0.5379447\n",
      "Training Step:  222 Loss:  0.5339751\n",
      "Training Step:  223 Loss:  0.5300339\n",
      "Training Step:  224 Loss:  0.52612257\n",
      "Training Step:  225 Loss:  0.5222398\n",
      "Training Step:  226 Loss:  0.51838607\n",
      "Training Step:  227 Loss:  0.51456\n",
      "Training Step:  228 Loss:  0.51076335\n",
      "Training Step:  229 Loss:  0.5069937\n",
      "Training Step:  230 Loss:  0.50325245\n",
      "Training Step:  231 Loss:  0.49953833\n",
      "Training Step:  232 Loss:  0.49585167\n",
      "Training Step:  233 Loss:  0.4921928\n",
      "Training Step:  234 Loss:  0.48856053\n",
      "Training Step:  235 Loss:  0.48495468\n",
      "Training Step:  236 Loss:  0.48137593\n",
      "Training Step:  237 Loss:  0.47782353\n",
      "Training Step:  238 Loss:  0.47429729\n",
      "Training Step:  239 Loss:  0.47079718\n",
      "Training Step:  240 Loss:  0.4673229\n",
      "Training Step:  241 Loss:  0.46387404\n",
      "Training Step:  242 Loss:  0.4604507\n",
      "Training Step:  243 Loss:  0.45705268\n",
      "Training Step:  244 Loss:  0.45367965\n",
      "Training Step:  245 Loss:  0.45033178\n",
      "Training Step:  246 Loss:  0.44700873\n",
      "Training Step:  247 Loss:  0.44370946\n",
      "Training Step:  248 Loss:  0.44043496\n",
      "Training Step:  249 Loss:  0.43718475\n",
      "Training Step:  250 Loss:  0.43395838\n",
      "Training Step:  251 Loss:  0.43075573\n",
      "Training Step:  252 Loss:  0.42757723\n",
      "Training Step:  253 Loss:  0.42442158\n",
      "Training Step:  254 Loss:  0.42128956\n",
      "Training Step:  255 Loss:  0.4181805\n",
      "Training Step:  256 Loss:  0.41509438\n",
      "Training Step:  257 Loss:  0.41203132\n",
      "Training Step:  258 Loss:  0.40899026\n",
      "Training Step:  259 Loss:  0.40597236\n",
      "Training Step:  260 Loss:  0.40297616\n",
      "Training Step:  261 Loss:  0.40000233\n",
      "Training Step:  262 Loss:  0.39705038\n",
      "Training Step:  263 Loss:  0.39412042\n",
      "Training Step:  264 Loss:  0.39121193\n",
      "Training Step:  265 Loss:  0.38832477\n",
      "Training Step:  266 Loss:  0.38545913\n",
      "Training Step:  267 Loss:  0.38261437\n",
      "Training Step:  268 Loss:  0.37979123\n",
      "Training Step:  269 Loss:  0.37698805\n",
      "Training Step:  270 Loss:  0.37420607\n",
      "Training Step:  271 Loss:  0.37144473\n",
      "Training Step:  272 Loss:  0.36870334\n",
      "Training Step:  273 Loss:  0.36598256\n",
      "Training Step:  274 Loss:  0.36328158\n",
      "Training Step:  275 Loss:  0.36060047\n",
      "Training Step:  276 Loss:  0.35793948\n",
      "Training Step:  277 Loss:  0.3552979\n",
      "Training Step:  278 Loss:  0.35267597\n",
      "Training Step:  279 Loss:  0.35007337\n",
      "Training Step:  280 Loss:  0.34748992\n",
      "Training Step:  281 Loss:  0.34492552\n",
      "Training Step:  282 Loss:  0.3423799\n",
      "Training Step:  283 Loss:  0.33985335\n",
      "Training Step:  284 Loss:  0.33734542\n",
      "Training Step:  285 Loss:  0.33485565\n",
      "Training Step:  286 Loss:  0.3323845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  287 Loss:  0.32993189\n",
      "Training Step:  288 Loss:  0.32749683\n",
      "Training Step:  289 Loss:  0.3250801\n",
      "Training Step:  290 Loss:  0.32268113\n",
      "Training Step:  291 Loss:  0.32029998\n",
      "Training Step:  292 Loss:  0.3179362\n",
      "Training Step:  293 Loss:  0.31558967\n",
      "Training Step:  294 Loss:  0.31326076\n",
      "Training Step:  295 Loss:  0.31094894\n",
      "Training Step:  296 Loss:  0.30865416\n",
      "Training Step:  297 Loss:  0.30637646\n",
      "Training Step:  298 Loss:  0.30411553\n",
      "Training Step:  299 Loss:  0.30187124\n",
      "Training Step:  300 Loss:  0.2996432\n",
      "Training Step:  301 Loss:  0.2974322\n",
      "Training Step:  302 Loss:  0.295237\n",
      "Training Step:  303 Loss:  0.29305825\n",
      "Training Step:  304 Loss:  0.2908958\n",
      "Training Step:  305 Loss:  0.2887491\n",
      "Training Step:  306 Loss:  0.28661823\n",
      "Training Step:  307 Loss:  0.2845033\n",
      "Training Step:  308 Loss:  0.28240326\n",
      "Training Step:  309 Loss:  0.2803194\n",
      "Training Step:  310 Loss:  0.2782507\n",
      "Training Step:  311 Loss:  0.27619737\n",
      "Training Step:  312 Loss:  0.27415872\n",
      "Training Step:  313 Loss:  0.27213562\n",
      "Training Step:  314 Loss:  0.27012748\n",
      "Training Step:  315 Loss:  0.26813376\n",
      "Training Step:  316 Loss:  0.2661552\n",
      "Training Step:  317 Loss:  0.26419127\n",
      "Training Step:  318 Loss:  0.26224148\n",
      "Training Step:  319 Loss:  0.260306\n",
      "Training Step:  320 Loss:  0.25838485\n",
      "Training Step:  321 Loss:  0.2564782\n",
      "Training Step:  322 Loss:  0.25458553\n",
      "Training Step:  323 Loss:  0.25270674\n",
      "Training Step:  324 Loss:  0.25084212\n",
      "Training Step:  325 Loss:  0.24899061\n",
      "Training Step:  326 Loss:  0.24715298\n",
      "Training Step:  327 Loss:  0.24532938\n",
      "Training Step:  328 Loss:  0.24351867\n",
      "Training Step:  329 Loss:  0.24172173\n",
      "Training Step:  330 Loss:  0.23993756\n",
      "Training Step:  331 Loss:  0.23816697\n",
      "Training Step:  332 Loss:  0.23640943\n",
      "Training Step:  333 Loss:  0.23466486\n",
      "Training Step:  334 Loss:  0.23293293\n",
      "Training Step:  335 Loss:  0.23121391\n",
      "Training Step:  336 Loss:  0.22950765\n",
      "Training Step:  337 Loss:  0.22781393\n",
      "Training Step:  338 Loss:  0.22613272\n",
      "Training Step:  339 Loss:  0.22446412\n",
      "Training Step:  340 Loss:  0.22280744\n",
      "Training Step:  341 Loss:  0.22116306\n",
      "Training Step:  342 Loss:  0.21953121\n",
      "Training Step:  343 Loss:  0.21791084\n",
      "Training Step:  344 Loss:  0.21630274\n",
      "Training Step:  345 Loss:  0.21470648\n",
      "Training Step:  346 Loss:  0.21312222\n",
      "Training Step:  347 Loss:  0.21154925\n",
      "Training Step:  348 Loss:  0.20998822\n",
      "Training Step:  349 Loss:  0.20843859\n",
      "Training Step:  350 Loss:  0.20690037\n",
      "Training Step:  351 Loss:  0.20537348\n",
      "Training Step:  352 Loss:  0.20385797\n",
      "Training Step:  353 Loss:  0.20235346\n",
      "Training Step:  354 Loss:  0.20086011\n",
      "Training Step:  355 Loss:  0.19937782\n",
      "Training Step:  356 Loss:  0.19790642\n",
      "Training Step:  357 Loss:  0.19644606\n",
      "Training Step:  358 Loss:  0.19499621\n",
      "Training Step:  359 Loss:  0.19355728\n",
      "Training Step:  360 Loss:  0.19212858\n",
      "Training Step:  361 Loss:  0.19071093\n",
      "Training Step:  362 Loss:  0.18930346\n",
      "Training Step:  363 Loss:  0.18790653\n",
      "Training Step:  364 Loss:  0.18651977\n",
      "Training Step:  365 Loss:  0.1851433\n",
      "Training Step:  366 Loss:  0.18377703\n",
      "Training Step:  367 Loss:  0.18242069\n",
      "Training Step:  368 Loss:  0.18107425\n",
      "Training Step:  369 Loss:  0.1797381\n",
      "Training Step:  370 Loss:  0.17841159\n",
      "Training Step:  371 Loss:  0.17709512\n",
      "Training Step:  372 Loss:  0.17578825\n",
      "Training Step:  373 Loss:  0.17449078\n",
      "Training Step:  374 Loss:  0.17320313\n",
      "Training Step:  375 Loss:  0.17192507\n",
      "Training Step:  376 Loss:  0.17065631\n",
      "Training Step:  377 Loss:  0.1693969\n",
      "Training Step:  378 Loss:  0.16814661\n",
      "Training Step:  379 Loss:  0.16690598\n",
      "Training Step:  380 Loss:  0.16567418\n",
      "Training Step:  381 Loss:  0.16445139\n",
      "Training Step:  382 Loss:  0.16323797\n",
      "Training Step:  383 Loss:  0.16203311\n",
      "Training Step:  384 Loss:  0.1608374\n",
      "Training Step:  385 Loss:  0.15965037\n",
      "Training Step:  386 Loss:  0.1584724\n",
      "Training Step:  387 Loss:  0.15730269\n",
      "Training Step:  388 Loss:  0.15614182\n",
      "Training Step:  389 Loss:  0.15498969\n",
      "Training Step:  390 Loss:  0.1538458\n",
      "Training Step:  391 Loss:  0.15271059\n",
      "Training Step:  392 Loss:  0.1515835\n",
      "Training Step:  393 Loss:  0.150465\n",
      "Training Step:  394 Loss:  0.14935438\n",
      "Training Step:  395 Loss:  0.14825237\n",
      "Training Step:  396 Loss:  0.14715827\n",
      "Training Step:  397 Loss:  0.14607228\n",
      "Training Step:  398 Loss:  0.14499433\n",
      "Training Step:  399 Loss:  0.14392437\n",
      "Training Step:  400 Loss:  0.14286232\n",
      "Training Step:  401 Loss:  0.14180784\n",
      "Training Step:  402 Loss:  0.14076126\n",
      "Training Step:  403 Loss:  0.13972254\n",
      "Training Step:  404 Loss:  0.13869154\n",
      "Training Step:  405 Loss:  0.13766783\n",
      "Training Step:  406 Loss:  0.13665198\n",
      "Training Step:  407 Loss:  0.13564342\n",
      "Training Step:  408 Loss:  0.13464254\n",
      "Training Step:  409 Loss:  0.13364887\n",
      "Training Step:  410 Loss:  0.13266264\n",
      "Training Step:  411 Loss:  0.13168347\n",
      "Training Step:  412 Loss:  0.13071167\n",
      "Training Step:  413 Loss:  0.12974706\n",
      "Training Step:  414 Loss:  0.12878971\n",
      "Training Step:  415 Loss:  0.12783922\n",
      "Training Step:  416 Loss:  0.1268957\n",
      "Training Step:  417 Loss:  0.12595952\n",
      "Training Step:  418 Loss:  0.12502982\n",
      "Training Step:  419 Loss:  0.12410706\n",
      "Training Step:  420 Loss:  0.123191245\n",
      "Training Step:  421 Loss:  0.12228198\n",
      "Training Step:  422 Loss:  0.121379726\n",
      "Training Step:  423 Loss:  0.12048393\n",
      "Training Step:  424 Loss:  0.11959461\n",
      "Training Step:  425 Loss:  0.11871222\n",
      "Training Step:  426 Loss:  0.117835954\n",
      "Training Step:  427 Loss:  0.11696646\n",
      "Training Step:  428 Loss:  0.116103336\n",
      "Training Step:  429 Loss:  0.11524641\n",
      "Training Step:  430 Loss:  0.11439585\n",
      "Training Step:  431 Loss:  0.113551855\n",
      "Training Step:  432 Loss:  0.112713695\n",
      "Training Step:  433 Loss:  0.11188195\n",
      "Training Step:  434 Loss:  0.111056134\n",
      "Training Step:  435 Loss:  0.110236675\n",
      "Training Step:  436 Loss:  0.109423116\n",
      "Training Step:  437 Loss:  0.10861572\n",
      "Training Step:  438 Loss:  0.10781406\n",
      "Training Step:  439 Loss:  0.10701852\n",
      "Training Step:  440 Loss:  0.106228575\n",
      "Training Step:  441 Loss:  0.105444804\n",
      "Training Step:  442 Loss:  0.10466664\n",
      "Training Step:  443 Loss:  0.10389422\n",
      "Training Step:  444 Loss:  0.10312749\n",
      "Training Step:  445 Loss:  0.10236624\n",
      "Training Step:  446 Loss:  0.10161098\n",
      "Training Step:  447 Loss:  0.100861\n",
      "Training Step:  448 Loss:  0.10011682\n",
      "Training Step:  449 Loss:  0.0993779\n",
      "Training Step:  450 Loss:  0.0986446\n",
      "Training Step:  451 Loss:  0.09791668\n",
      "Training Step:  452 Loss:  0.097193964\n",
      "Training Step:  453 Loss:  0.096476726\n",
      "Training Step:  454 Loss:  0.0957648\n",
      "Training Step:  455 Loss:  0.09505795\n",
      "Training Step:  456 Loss:  0.09435654\n",
      "Training Step:  457 Loss:  0.093660206\n",
      "Training Step:  458 Loss:  0.09296896\n",
      "Training Step:  459 Loss:  0.092282936\n",
      "Training Step:  460 Loss:  0.09160188\n",
      "Training Step:  461 Loss:  0.090925835\n",
      "Training Step:  462 Loss:  0.09025492\n",
      "Training Step:  463 Loss:  0.08958879\n",
      "Training Step:  464 Loss:  0.088927835\n",
      "Training Step:  465 Loss:  0.0882714\n",
      "Training Step:  466 Loss:  0.08762004\n",
      "Training Step:  467 Loss:  0.086973354\n",
      "Training Step:  468 Loss:  0.08633145\n",
      "Training Step:  469 Loss:  0.08569443\n",
      "Training Step:  470 Loss:  0.08506206\n",
      "Training Step:  471 Loss:  0.08443426\n",
      "Training Step:  472 Loss:  0.0838111\n",
      "Training Step:  473 Loss:  0.08319276\n",
      "Training Step:  474 Loss:  0.08257875\n",
      "Training Step:  475 Loss:  0.081969276\n",
      "Training Step:  476 Loss:  0.08136429\n",
      "Training Step:  477 Loss:  0.080763906\n",
      "Training Step:  478 Loss:  0.08016793\n",
      "Training Step:  479 Loss:  0.079576194\n",
      "Training Step:  480 Loss:  0.07898907\n",
      "Training Step:  481 Loss:  0.0784061\n",
      "Training Step:  482 Loss:  0.077827685\n",
      "Training Step:  483 Loss:  0.07725321\n",
      "Training Step:  484 Loss:  0.076683104\n",
      "Training Step:  485 Loss:  0.07611705\n",
      "Training Step:  486 Loss:  0.07555539\n",
      "Training Step:  487 Loss:  0.07499781\n",
      "Training Step:  488 Loss:  0.07444429\n",
      "Training Step:  489 Loss:  0.07389503\n",
      "Training Step:  490 Loss:  0.07334971\n",
      "Training Step:  491 Loss:  0.07280835\n",
      "Training Step:  492 Loss:  0.072271064\n",
      "Training Step:  493 Loss:  0.07173764\n",
      "Training Step:  494 Loss:  0.071208395\n",
      "Training Step:  495 Loss:  0.07068287\n",
      "Training Step:  496 Loss:  0.07016122\n",
      "Training Step:  497 Loss:  0.06964338\n",
      "Training Step:  498 Loss:  0.06912945\n",
      "Training Step:  499 Loss:  0.0686192\n",
      "Training Step:  500 Loss:  0.06811281\n",
      "Training Step:  501 Loss:  0.0676102\n",
      "Training Step:  502 Loss:  0.06711122\n",
      "Training Step:  503 Loss:  0.0666159\n",
      "Training Step:  504 Loss:  0.06612435\n",
      "Training Step:  505 Loss:  0.065636404\n",
      "Training Step:  506 Loss:  0.06515202\n",
      "Training Step:  507 Loss:  0.06467119\n",
      "Training Step:  508 Loss:  0.06419385\n",
      "Training Step:  509 Loss:  0.06372025\n",
      "Training Step:  510 Loss:  0.063249916\n",
      "Training Step:  511 Loss:  0.06278314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  512 Loss:  0.062319756\n",
      "Training Step:  513 Loss:  0.06185987\n",
      "Training Step:  514 Loss:  0.06140346\n",
      "Training Step:  515 Loss:  0.06095031\n",
      "Training Step:  516 Loss:  0.060500395\n",
      "Training Step:  517 Loss:  0.06005399\n",
      "Training Step:  518 Loss:  0.059610743\n",
      "Training Step:  519 Loss:  0.05917077\n",
      "Training Step:  520 Loss:  0.058734145\n",
      "Training Step:  521 Loss:  0.058300808\n",
      "Training Step:  522 Loss:  0.057870597\n",
      "Training Step:  523 Loss:  0.057443496\n",
      "Training Step:  524 Loss:  0.05701946\n",
      "Training Step:  525 Loss:  0.056598727\n",
      "Training Step:  526 Loss:  0.056181144\n",
      "Training Step:  527 Loss:  0.055766493\n",
      "Training Step:  528 Loss:  0.055354945\n",
      "Training Step:  529 Loss:  0.05494646\n",
      "Training Step:  530 Loss:  0.054540977\n",
      "Training Step:  531 Loss:  0.0541384\n",
      "Training Step:  532 Loss:  0.053738926\n",
      "Training Step:  533 Loss:  0.053342313\n",
      "Training Step:  534 Loss:  0.05294867\n",
      "Training Step:  535 Loss:  0.052557826\n",
      "Training Step:  536 Loss:  0.05216998\n",
      "Training Step:  537 Loss:  0.051785037\n",
      "Training Step:  538 Loss:  0.051402867\n",
      "Training Step:  539 Loss:  0.05102349\n",
      "Training Step:  540 Loss:  0.050647035\n",
      "Training Step:  541 Loss:  0.05027321\n",
      "Training Step:  542 Loss:  0.049902216\n",
      "Training Step:  543 Loss:  0.049533963\n",
      "Training Step:  544 Loss:  0.049168423\n",
      "Training Step:  545 Loss:  0.04880553\n",
      "Training Step:  546 Loss:  0.04844545\n",
      "Training Step:  547 Loss:  0.04808798\n",
      "Training Step:  548 Loss:  0.047733072\n",
      "Training Step:  549 Loss:  0.047380712\n",
      "Training Step:  550 Loss:  0.047031157\n",
      "Training Step:  551 Loss:  0.046684004\n",
      "Training Step:  552 Loss:  0.046339434\n",
      "Training Step:  553 Loss:  0.045997545\n",
      "Training Step:  554 Loss:  0.04565806\n",
      "Training Step:  555 Loss:  0.04532121\n",
      "Training Step:  556 Loss:  0.044986717\n",
      "Training Step:  557 Loss:  0.044654716\n",
      "Training Step:  558 Loss:  0.04432514\n",
      "Training Step:  559 Loss:  0.04399804\n",
      "Training Step:  560 Loss:  0.043673337\n",
      "Training Step:  561 Loss:  0.043351118\n",
      "Training Step:  562 Loss:  0.043031193\n",
      "Training Step:  563 Loss:  0.042713553\n",
      "Training Step:  564 Loss:  0.042398352\n",
      "Training Step:  565 Loss:  0.042085443\n",
      "Training Step:  566 Loss:  0.041774992\n",
      "Training Step:  567 Loss:  0.04146665\n",
      "Training Step:  568 Loss:  0.041160595\n",
      "Training Step:  569 Loss:  0.040856928\n",
      "Training Step:  570 Loss:  0.040555302\n",
      "Training Step:  571 Loss:  0.04025611\n",
      "Training Step:  572 Loss:  0.03995897\n",
      "Training Step:  573 Loss:  0.039664134\n",
      "Training Step:  574 Loss:  0.03937137\n",
      "Training Step:  575 Loss:  0.039080873\n",
      "Training Step:  576 Loss:  0.038792383\n",
      "Training Step:  577 Loss:  0.03850616\n",
      "Training Step:  578 Loss:  0.038221996\n",
      "Training Step:  579 Loss:  0.03793987\n",
      "Training Step:  580 Loss:  0.03765997\n",
      "Training Step:  581 Loss:  0.03738196\n",
      "Training Step:  582 Loss:  0.037106205\n",
      "Training Step:  583 Loss:  0.036832284\n",
      "Training Step:  584 Loss:  0.0365604\n",
      "Training Step:  585 Loss:  0.03629066\n",
      "Training Step:  586 Loss:  0.036022764\n",
      "Training Step:  587 Loss:  0.03575696\n",
      "Training Step:  588 Loss:  0.035493042\n",
      "Training Step:  589 Loss:  0.035231195\n",
      "Training Step:  590 Loss:  0.03497121\n",
      "Training Step:  591 Loss:  0.034713067\n",
      "Training Step:  592 Loss:  0.03445694\n",
      "Training Step:  593 Loss:  0.034202646\n",
      "Training Step:  594 Loss:  0.033950288\n",
      "Training Step:  595 Loss:  0.033699743\n",
      "Training Step:  596 Loss:  0.03345101\n",
      "Training Step:  597 Loss:  0.033204168\n",
      "Training Step:  598 Loss:  0.032959055\n",
      "Training Step:  599 Loss:  0.0327159\n",
      "Training Step:  600 Loss:  0.03247442\n",
      "Training Step:  601 Loss:  0.032234743\n",
      "Training Step:  602 Loss:  0.031996876\n",
      "Training Step:  603 Loss:  0.03176069\n",
      "Training Step:  604 Loss:  0.03152643\n",
      "Training Step:  605 Loss:  0.031293705\n",
      "Training Step:  606 Loss:  0.031062739\n",
      "Training Step:  607 Loss:  0.030833567\n",
      "Training Step:  608 Loss:  0.030606046\n",
      "Training Step:  609 Loss:  0.030380107\n",
      "Training Step:  610 Loss:  0.030155862\n",
      "Training Step:  611 Loss:  0.029933311\n",
      "Training Step:  612 Loss:  0.02971248\n",
      "Training Step:  613 Loss:  0.029493187\n",
      "Training Step:  614 Loss:  0.029275578\n",
      "Training Step:  615 Loss:  0.029059488\n",
      "Training Step:  616 Loss:  0.028845087\n",
      "Training Step:  617 Loss:  0.028632192\n",
      "Training Step:  618 Loss:  0.02842092\n",
      "Training Step:  619 Loss:  0.028211154\n",
      "Training Step:  620 Loss:  0.02800301\n",
      "Training Step:  621 Loss:  0.027796363\n",
      "Training Step:  622 Loss:  0.027591221\n",
      "Training Step:  623 Loss:  0.0273876\n",
      "Training Step:  624 Loss:  0.02718554\n",
      "Training Step:  625 Loss:  0.026984805\n",
      "Training Step:  626 Loss:  0.026785705\n",
      "Training Step:  627 Loss:  0.026588012\n",
      "Training Step:  628 Loss:  0.026391812\n",
      "Training Step:  629 Loss:  0.02619708\n",
      "Training Step:  630 Loss:  0.026003782\n",
      "Training Step:  631 Loss:  0.02581181\n",
      "Training Step:  632 Loss:  0.025621317\n",
      "Training Step:  633 Loss:  0.025432277\n",
      "Training Step:  634 Loss:  0.025244553\n",
      "Training Step:  635 Loss:  0.0250583\n",
      "Training Step:  636 Loss:  0.024873318\n",
      "Training Step:  637 Loss:  0.02468984\n",
      "Training Step:  638 Loss:  0.024507627\n",
      "Training Step:  639 Loss:  0.024326755\n",
      "Training Step:  640 Loss:  0.024147237\n",
      "Training Step:  641 Loss:  0.023969023\n",
      "Training Step:  642 Loss:  0.02379214\n",
      "Training Step:  643 Loss:  0.023616571\n",
      "Training Step:  644 Loss:  0.023442328\n",
      "Training Step:  645 Loss:  0.023269288\n",
      "Training Step:  646 Loss:  0.023097487\n",
      "Training Step:  647 Loss:  0.022927042\n",
      "Training Step:  648 Loss:  0.022757925\n",
      "Training Step:  649 Loss:  0.022589862\n",
      "Training Step:  650 Loss:  0.022423256\n",
      "Training Step:  651 Loss:  0.022257771\n",
      "Training Step:  652 Loss:  0.022093493\n",
      "Training Step:  653 Loss:  0.021930471\n",
      "Training Step:  654 Loss:  0.021768635\n",
      "Training Step:  655 Loss:  0.021607984\n",
      "Training Step:  656 Loss:  0.021448486\n",
      "Training Step:  657 Loss:  0.02129024\n",
      "Training Step:  658 Loss:  0.021133102\n",
      "Training Step:  659 Loss:  0.020977145\n",
      "Training Step:  660 Loss:  0.02082238\n",
      "Training Step:  661 Loss:  0.020668657\n",
      "Training Step:  662 Loss:  0.020516153\n",
      "Training Step:  663 Loss:  0.020364806\n",
      "Training Step:  664 Loss:  0.020214463\n",
      "Training Step:  665 Loss:  0.020065289\n",
      "Training Step:  666 Loss:  0.019917171\n",
      "Training Step:  667 Loss:  0.01977027\n",
      "Training Step:  668 Loss:  0.019624274\n",
      "Training Step:  669 Loss:  0.019479452\n",
      "Training Step:  670 Loss:  0.019335743\n",
      "Training Step:  671 Loss:  0.019193033\n",
      "Training Step:  672 Loss:  0.01905146\n",
      "Training Step:  673 Loss:  0.018910784\n",
      "Training Step:  674 Loss:  0.018771213\n",
      "Training Step:  675 Loss:  0.018632706\n",
      "Training Step:  676 Loss:  0.01849523\n",
      "Training Step:  677 Loss:  0.018358722\n",
      "Training Step:  678 Loss:  0.018223235\n",
      "Training Step:  679 Loss:  0.01808874\n",
      "Training Step:  680 Loss:  0.017955262\n",
      "Training Step:  681 Loss:  0.017822742\n",
      "Training Step:  682 Loss:  0.017691191\n",
      "Training Step:  683 Loss:  0.017560685\n",
      "Training Step:  684 Loss:  0.017431071\n",
      "Training Step:  685 Loss:  0.017302537\n",
      "Training Step:  686 Loss:  0.017174764\n",
      "Training Step:  687 Loss:  0.017048003\n",
      "Training Step:  688 Loss:  0.016922235\n",
      "Training Step:  689 Loss:  0.016797315\n",
      "Training Step:  690 Loss:  0.01667337\n",
      "Training Step:  691 Loss:  0.016550297\n",
      "Training Step:  692 Loss:  0.016428184\n",
      "Training Step:  693 Loss:  0.016306948\n",
      "Training Step:  694 Loss:  0.016186617\n",
      "Training Step:  695 Loss:  0.016067075\n",
      "Training Step:  696 Loss:  0.01594852\n",
      "Training Step:  697 Loss:  0.015830895\n",
      "Training Step:  698 Loss:  0.01571404\n",
      "Training Step:  699 Loss:  0.015598083\n",
      "Training Step:  700 Loss:  0.015482982\n",
      "Training Step:  701 Loss:  0.015368672\n",
      "Training Step:  702 Loss:  0.01525525\n",
      "Training Step:  703 Loss:  0.015142666\n",
      "Training Step:  704 Loss:  0.015030987\n",
      "Training Step:  705 Loss:  0.0149200605\n",
      "Training Step:  706 Loss:  0.0148098925\n",
      "Training Step:  707 Loss:  0.01470063\n",
      "Training Step:  708 Loss:  0.014592157\n",
      "Training Step:  709 Loss:  0.014484483\n",
      "Training Step:  710 Loss:  0.014377585\n",
      "Training Step:  711 Loss:  0.014271469\n",
      "Training Step:  712 Loss:  0.014166194\n",
      "Training Step:  713 Loss:  0.0140615925\n",
      "Training Step:  714 Loss:  0.013957836\n",
      "Training Step:  715 Loss:  0.013854827\n",
      "Training Step:  716 Loss:  0.013752578\n",
      "Training Step:  717 Loss:  0.01365104\n",
      "Training Step:  718 Loss:  0.013550313\n",
      "Training Step:  719 Loss:  0.013450384\n",
      "Training Step:  720 Loss:  0.013351083\n",
      "Training Step:  721 Loss:  0.013252576\n",
      "Training Step:  722 Loss:  0.01315475\n",
      "Training Step:  723 Loss:  0.013057705\n",
      "Training Step:  724 Loss:  0.012961379\n",
      "Training Step:  725 Loss:  0.012865679\n",
      "Training Step:  726 Loss:  0.012770749\n",
      "Training Step:  727 Loss:  0.012676515\n",
      "Training Step:  728 Loss:  0.012582995\n",
      "Training Step:  729 Loss:  0.012490118\n",
      "Training Step:  730 Loss:  0.012397916\n",
      "Training Step:  731 Loss:  0.012306451\n",
      "Training Step:  732 Loss:  0.012215618\n",
      "Training Step:  733 Loss:  0.012125478\n",
      "Training Step:  734 Loss:  0.012035948\n",
      "Training Step:  735 Loss:  0.011947171\n",
      "Training Step:  736 Loss:  0.011858988\n",
      "Training Step:  737 Loss:  0.011771506\n",
      "Training Step:  738 Loss:  0.011684638\n",
      "Training Step:  739 Loss:  0.0115984045\n",
      "Training Step:  740 Loss:  0.011512773\n",
      "Training Step:  741 Loss:  0.0114278\n",
      "Training Step:  742 Loss:  0.011343462\n",
      "Training Step:  743 Loss:  0.011259811\n",
      "Training Step:  744 Loss:  0.011176697\n",
      "Training Step:  745 Loss:  0.0110942125\n",
      "Training Step:  746 Loss:  0.011012317\n",
      "Training Step:  747 Loss:  0.010931045\n",
      "Training Step:  748 Loss:  0.010850425\n",
      "Training Step:  749 Loss:  0.010770363\n",
      "Training Step:  750 Loss:  0.010690851\n",
      "Training Step:  751 Loss:  0.010611967\n",
      "Training Step:  752 Loss:  0.010533653\n",
      "Training Step:  753 Loss:  0.010455941\n",
      "Training Step:  754 Loss:  0.010378738\n",
      "Training Step:  755 Loss:  0.010302162\n",
      "Training Step:  756 Loss:  0.010226138\n",
      "Training Step:  757 Loss:  0.010150655\n",
      "Training Step:  758 Loss:  0.0100757815\n",
      "Training Step:  759 Loss:  0.010001383\n",
      "Training Step:  760 Loss:  0.00992762\n",
      "Training Step:  761 Loss:  0.009854324\n",
      "Training Step:  762 Loss:  0.009781638\n",
      "Training Step:  763 Loss:  0.009709417\n",
      "Training Step:  764 Loss:  0.009637784\n",
      "Training Step:  765 Loss:  0.0095666405\n",
      "Training Step:  766 Loss:  0.009496061\n",
      "Training Step:  767 Loss:  0.009425976\n",
      "Training Step:  768 Loss:  0.009356419\n",
      "Training Step:  769 Loss:  0.009287373\n",
      "Training Step:  770 Loss:  0.009218807\n",
      "Training Step:  771 Loss:  0.009150806\n",
      "Training Step:  772 Loss:  0.009083247\n",
      "Training Step:  773 Loss:  0.009016216\n",
      "Training Step:  774 Loss:  0.008949712\n",
      "Training Step:  775 Loss:  0.008883632\n",
      "Training Step:  776 Loss:  0.008818076\n",
      "Training Step:  777 Loss:  0.008753029\n",
      "Training Step:  778 Loss:  0.008688425\n",
      "Training Step:  779 Loss:  0.0086243\n",
      "Training Step:  780 Loss:  0.008560622\n",
      "Training Step:  781 Loss:  0.008497454\n",
      "Training Step:  782 Loss:  0.008434739\n",
      "Training Step:  783 Loss:  0.00837253\n",
      "Training Step:  784 Loss:  0.008310715\n",
      "Training Step:  785 Loss:  0.008249397\n",
      "Training Step:  786 Loss:  0.0081884945\n",
      "Training Step:  787 Loss:  0.008128069\n",
      "Training Step:  788 Loss:  0.008068084\n",
      "Training Step:  789 Loss:  0.008008557\n",
      "Training Step:  790 Loss:  0.007949446\n",
      "Training Step:  791 Loss:  0.007890777\n",
      "Training Step:  792 Loss:  0.007832544\n",
      "Training Step:  793 Loss:  0.00777475\n",
      "Training Step:  794 Loss:  0.0077173477\n",
      "Training Step:  795 Loss:  0.007660424\n",
      "Training Step:  796 Loss:  0.0076039038\n",
      "Training Step:  797 Loss:  0.007547776\n",
      "Training Step:  798 Loss:  0.0074920347\n",
      "Training Step:  799 Loss:  0.0074367914\n",
      "Training Step:  800 Loss:  0.0073818825\n",
      "Training Step:  801 Loss:  0.007327381\n",
      "Training Step:  802 Loss:  0.0072733024\n",
      "Training Step:  803 Loss:  0.0072196773\n",
      "Training Step:  804 Loss:  0.0071664066\n",
      "Training Step:  805 Loss:  0.007113504\n",
      "Training Step:  806 Loss:  0.0070610032\n",
      "Training Step:  807 Loss:  0.007008914\n",
      "Training Step:  808 Loss:  0.0069571957\n",
      "Training Step:  809 Loss:  0.006905864\n",
      "Training Step:  810 Loss:  0.006854871\n",
      "Training Step:  811 Loss:  0.006804286\n",
      "Training Step:  812 Loss:  0.0067540575\n",
      "Training Step:  813 Loss:  0.0067042033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  814 Loss:  0.0066547412\n",
      "Training Step:  815 Loss:  0.0066056303\n",
      "Training Step:  816 Loss:  0.006556883\n",
      "Training Step:  817 Loss:  0.006508506\n",
      "Training Step:  818 Loss:  0.006460496\n",
      "Training Step:  819 Loss:  0.0064127883\n",
      "Training Step:  820 Loss:  0.0063654366\n",
      "Training Step:  821 Loss:  0.0063184905\n",
      "Training Step:  822 Loss:  0.0062718336\n",
      "Training Step:  823 Loss:  0.0062255473\n",
      "Training Step:  824 Loss:  0.006179632\n",
      "Training Step:  825 Loss:  0.0061340374\n",
      "Training Step:  826 Loss:  0.0060887653\n",
      "Training Step:  827 Loss:  0.0060438137\n",
      "Training Step:  828 Loss:  0.0059991917\n",
      "Training Step:  829 Loss:  0.0059549357\n",
      "Training Step:  830 Loss:  0.0059109824\n",
      "Training Step:  831 Loss:  0.0058673494\n",
      "Training Step:  832 Loss:  0.005824036\n",
      "Training Step:  833 Loss:  0.0057810647\n",
      "Training Step:  834 Loss:  0.005738388\n",
      "Training Step:  835 Loss:  0.0056960327\n",
      "Training Step:  836 Loss:  0.005654011\n",
      "Training Step:  837 Loss:  0.005612277\n",
      "Training Step:  838 Loss:  0.0055708657\n",
      "Training Step:  839 Loss:  0.00552978\n",
      "Training Step:  840 Loss:  0.0054889377\n",
      "Training Step:  841 Loss:  0.005448448\n",
      "Training Step:  842 Loss:  0.005408225\n",
      "Training Step:  843 Loss:  0.0053683273\n",
      "Training Step:  844 Loss:  0.005328699\n",
      "Training Step:  845 Loss:  0.0052893874\n",
      "Training Step:  846 Loss:  0.0052503594\n",
      "Training Step:  847 Loss:  0.005211584\n",
      "Training Step:  848 Loss:  0.005173114\n",
      "Training Step:  849 Loss:  0.0051349704\n",
      "Training Step:  850 Loss:  0.0050970856\n",
      "Training Step:  851 Loss:  0.0050594746\n",
      "Training Step:  852 Loss:  0.0050221155\n",
      "Training Step:  853 Loss:  0.0049850447\n",
      "Training Step:  854 Loss:  0.0049482696\n",
      "Training Step:  855 Loss:  0.0049117394\n",
      "Training Step:  856 Loss:  0.0048754932\n",
      "Training Step:  857 Loss:  0.0048395265\n",
      "Training Step:  858 Loss:  0.004803806\n",
      "Training Step:  859 Loss:  0.0047683674\n",
      "Training Step:  860 Loss:  0.0047331597\n",
      "Training Step:  861 Loss:  0.0046982192\n",
      "Training Step:  862 Loss:  0.004663566\n",
      "Training Step:  863 Loss:  0.004629153\n",
      "Training Step:  864 Loss:  0.004594985\n",
      "Training Step:  865 Loss:  0.0045610596\n",
      "Training Step:  866 Loss:  0.0045274124\n",
      "Training Step:  867 Loss:  0.004494022\n",
      "Training Step:  868 Loss:  0.004460851\n",
      "Training Step:  869 Loss:  0.004427912\n",
      "Training Step:  870 Loss:  0.004395233\n",
      "Training Step:  871 Loss:  0.0043628365\n",
      "Training Step:  872 Loss:  0.0043306206\n",
      "Training Step:  873 Loss:  0.0042986614\n",
      "Training Step:  874 Loss:  0.004266904\n",
      "Training Step:  875 Loss:  0.0042354325\n",
      "Training Step:  876 Loss:  0.004204199\n",
      "Training Step:  877 Loss:  0.0041731736\n",
      "Training Step:  878 Loss:  0.004142378\n",
      "Training Step:  879 Loss:  0.0041117957\n",
      "Training Step:  880 Loss:  0.0040814397\n",
      "Training Step:  881 Loss:  0.0040513445\n",
      "Training Step:  882 Loss:  0.004021414\n",
      "Training Step:  883 Loss:  0.003991763\n",
      "Training Step:  884 Loss:  0.0039622774\n",
      "Training Step:  885 Loss:  0.003933058\n",
      "Training Step:  886 Loss:  0.0039040197\n",
      "Training Step:  887 Loss:  0.0038752279\n",
      "Training Step:  888 Loss:  0.003846635\n",
      "Training Step:  889 Loss:  0.003818244\n",
      "Training Step:  890 Loss:  0.0037900666\n",
      "Training Step:  891 Loss:  0.0037620976\n",
      "Training Step:  892 Loss:  0.0037343202\n",
      "Training Step:  893 Loss:  0.0037067682\n",
      "Training Step:  894 Loss:  0.0036794099\n",
      "Training Step:  895 Loss:  0.003652289\n",
      "Training Step:  896 Loss:  0.0036252919\n",
      "Training Step:  897 Loss:  0.003598556\n",
      "Training Step:  898 Loss:  0.0035719827\n",
      "Training Step:  899 Loss:  0.0035456356\n",
      "Training Step:  900 Loss:  0.003519461\n",
      "Training Step:  901 Loss:  0.0034934906\n",
      "Training Step:  902 Loss:  0.0034677046\n",
      "Training Step:  903 Loss:  0.0034421198\n",
      "Training Step:  904 Loss:  0.003416704\n",
      "Training Step:  905 Loss:  0.0033914838\n",
      "Training Step:  906 Loss:  0.0033664545\n",
      "Training Step:  907 Loss:  0.0033416147\n",
      "Training Step:  908 Loss:  0.0033169442\n",
      "Training Step:  909 Loss:  0.003292512\n",
      "Training Step:  910 Loss:  0.0032681713\n",
      "Training Step:  911 Loss:  0.003244075\n",
      "Training Step:  912 Loss:  0.00322014\n",
      "Training Step:  913 Loss:  0.0031963708\n",
      "Training Step:  914 Loss:  0.0031727906\n",
      "Training Step:  915 Loss:  0.0031493716\n",
      "Training Step:  916 Loss:  0.0031261067\n",
      "Training Step:  917 Loss:  0.0031030562\n",
      "Training Step:  918 Loss:  0.003080146\n",
      "Training Step:  919 Loss:  0.0030574254\n",
      "Training Step:  920 Loss:  0.0030348692\n",
      "Training Step:  921 Loss:  0.0030124735\n",
      "Training Step:  922 Loss:  0.0029902363\n",
      "Training Step:  923 Loss:  0.0029681816\n",
      "Training Step:  924 Loss:  0.0029462331\n",
      "Training Step:  925 Loss:  0.0029245003\n",
      "Training Step:  926 Loss:  0.0029029448\n",
      "Training Step:  927 Loss:  0.0028815272\n",
      "Training Step:  928 Loss:  0.0028602632\n",
      "Training Step:  929 Loss:  0.0028391269\n",
      "Training Step:  930 Loss:  0.0028182021\n",
      "Training Step:  931 Loss:  0.0027973927\n",
      "Training Step:  932 Loss:  0.002776751\n",
      "Training Step:  933 Loss:  0.0027562426\n",
      "Training Step:  934 Loss:  0.0027359053\n",
      "Training Step:  935 Loss:  0.0027156884\n",
      "Training Step:  936 Loss:  0.0026956878\n",
      "Training Step:  937 Loss:  0.0026757657\n",
      "Training Step:  938 Loss:  0.002656023\n",
      "Training Step:  939 Loss:  0.0026364326\n",
      "Training Step:  940 Loss:  0.002616983\n",
      "Training Step:  941 Loss:  0.0025976717\n",
      "Training Step:  942 Loss:  0.0025784897\n",
      "Training Step:  943 Loss:  0.0025594619\n",
      "Training Step:  944 Loss:  0.0025405735\n",
      "Training Step:  945 Loss:  0.002521821\n",
      "Training Step:  946 Loss:  0.0025032226\n",
      "Training Step:  947 Loss:  0.0024847577\n",
      "Training Step:  948 Loss:  0.002466389\n",
      "Training Step:  949 Loss:  0.0024482065\n",
      "Training Step:  950 Loss:  0.002430162\n",
      "Training Step:  951 Loss:  0.0024122216\n",
      "Training Step:  952 Loss:  0.0023943966\n",
      "Training Step:  953 Loss:  0.00237673\n",
      "Training Step:  954 Loss:  0.0023591788\n",
      "Training Step:  955 Loss:  0.0023417617\n",
      "Training Step:  956 Loss:  0.00232449\n",
      "Training Step:  957 Loss:  0.0023073389\n",
      "Training Step:  958 Loss:  0.0022903315\n",
      "Training Step:  959 Loss:  0.0022734143\n",
      "Training Step:  960 Loss:  0.0022566456\n",
      "Training Step:  961 Loss:  0.0022399812\n",
      "Training Step:  962 Loss:  0.0022234658\n",
      "Training Step:  963 Loss:  0.00220704\n",
      "Training Step:  964 Loss:  0.0021907697\n",
      "Training Step:  965 Loss:  0.0021746175\n",
      "Training Step:  966 Loss:  0.002158538\n",
      "Training Step:  967 Loss:  0.002142639\n",
      "Training Step:  968 Loss:  0.0021268309\n",
      "Training Step:  969 Loss:  0.0021111001\n",
      "Training Step:  970 Loss:  0.0020955321\n",
      "Training Step:  971 Loss:  0.0020800869\n",
      "Training Step:  972 Loss:  0.0020647305\n",
      "Training Step:  973 Loss:  0.0020494994\n",
      "Training Step:  974 Loss:  0.0020343773\n",
      "Training Step:  975 Loss:  0.0020193683\n",
      "Training Step:  976 Loss:  0.0020044516\n",
      "Training Step:  977 Loss:  0.0019896512\n",
      "Training Step:  978 Loss:  0.0019749722\n",
      "Training Step:  979 Loss:  0.0019603865\n",
      "Training Step:  980 Loss:  0.0019459431\n",
      "Training Step:  981 Loss:  0.0019315677\n",
      "Training Step:  982 Loss:  0.001917319\n",
      "Training Step:  983 Loss:  0.0019031586\n",
      "Training Step:  984 Loss:  0.0018891165\n",
      "Training Step:  985 Loss:  0.0018751882\n",
      "Training Step:  986 Loss:  0.0018613347\n",
      "Training Step:  987 Loss:  0.0018476122\n",
      "Training Step:  988 Loss:  0.0018339844\n",
      "Training Step:  989 Loss:  0.0018204323\n",
      "Training Step:  990 Loss:  0.001807007\n",
      "Training Step:  991 Loss:  0.0017936612\n",
      "Training Step:  992 Loss:  0.0017804261\n",
      "Training Step:  993 Loss:  0.0017672955\n",
      "Training Step:  994 Loss:  0.001754244\n",
      "Training Step:  995 Loss:  0.001741321\n",
      "Training Step:  996 Loss:  0.0017284426\n",
      "Training Step:  997 Loss:  0.0017156932\n",
      "Training Step:  998 Loss:  0.0017030512\n",
      "Training Step:  999 Loss:  0.001690468\n",
      "Training Step:  1000 Loss:  0.0016779827\n",
      "Training Step:  1001 Loss:  0.0016656053\n",
      "Training Step:  1002 Loss:  0.0016533188\n",
      "Training Step:  1003 Loss:  0.0016411254\n",
      "Training Step:  1004 Loss:  0.0016290009\n",
      "Training Step:  1005 Loss:  0.0016169832\n",
      "Training Step:  1006 Loss:  0.001605056\n",
      "Training Step:  1007 Loss:  0.0015931985\n",
      "Training Step:  1008 Loss:  0.0015814499\n",
      "Training Step:  1009 Loss:  0.0015697738\n",
      "Training Step:  1010 Loss:  0.0015581847\n",
      "Training Step:  1011 Loss:  0.0015467159\n",
      "Training Step:  1012 Loss:  0.001535286\n",
      "Training Step:  1013 Loss:  0.0015239631\n",
      "Training Step:  1014 Loss:  0.0015127179\n",
      "Training Step:  1015 Loss:  0.001501563\n",
      "Training Step:  1016 Loss:  0.0014904726\n",
      "Training Step:  1017 Loss:  0.0014794616\n",
      "Training Step:  1018 Loss:  0.0014685569\n",
      "Training Step:  1019 Loss:  0.0014577302\n",
      "Training Step:  1020 Loss:  0.0014469549\n",
      "Training Step:  1021 Loss:  0.0014362707\n",
      "Training Step:  1022 Loss:  0.0014256695\n",
      "Training Step:  1023 Loss:  0.0014151565\n",
      "Training Step:  1024 Loss:  0.0014047106\n",
      "Training Step:  1025 Loss:  0.0013943508\n",
      "Training Step:  1026 Loss:  0.0013840698\n",
      "Training Step:  1027 Loss:  0.0013738463\n",
      "Training Step:  1028 Loss:  0.0013636907\n",
      "Training Step:  1029 Loss:  0.0013536367\n",
      "Training Step:  1030 Loss:  0.001343662\n",
      "Training Step:  1031 Loss:  0.0013337374\n",
      "Training Step:  1032 Loss:  0.0013239129\n",
      "Training Step:  1033 Loss:  0.0013141239\n",
      "Training Step:  1034 Loss:  0.0013044369\n",
      "Training Step:  1035 Loss:  0.0012947998\n",
      "Training Step:  1036 Loss:  0.0012852666\n",
      "Training Step:  1037 Loss:  0.0012757614\n",
      "Training Step:  1038 Loss:  0.0012663694\n",
      "Training Step:  1039 Loss:  0.0012570049\n",
      "Training Step:  1040 Loss:  0.0012477476\n",
      "Training Step:  1041 Loss:  0.0012385211\n",
      "Training Step:  1042 Loss:  0.0012293754\n",
      "Training Step:  1043 Loss:  0.0012203023\n",
      "Training Step:  1044 Loss:  0.0012113159\n",
      "Training Step:  1045 Loss:  0.0012023742\n",
      "Training Step:  1046 Loss:  0.0011934927\n",
      "Training Step:  1047 Loss:  0.0011846949\n",
      "Training Step:  1048 Loss:  0.0011759603\n",
      "Training Step:  1049 Loss:  0.0011672706\n",
      "Training Step:  1050 Loss:  0.0011586495\n",
      "Training Step:  1051 Loss:  0.0011500941\n",
      "Training Step:  1052 Loss:  0.0011416241\n",
      "Training Step:  1053 Loss:  0.0011331956\n",
      "Training Step:  1054 Loss:  0.0011248245\n",
      "Training Step:  1055 Loss:  0.001116535\n",
      "Training Step:  1056 Loss:  0.0011083023\n",
      "Training Step:  1057 Loss:  0.001100106\n",
      "Training Step:  1058 Loss:  0.0010919934\n",
      "Training Step:  1059 Loss:  0.0010839445\n",
      "Training Step:  1060 Loss:  0.0010759428\n",
      "Training Step:  1061 Loss:  0.00106799\n",
      "Training Step:  1062 Loss:  0.0010601092\n",
      "Training Step:  1063 Loss:  0.0010522902\n",
      "Training Step:  1064 Loss:  0.0010445234\n",
      "Training Step:  1065 Loss:  0.0010368054\n",
      "Training Step:  1066 Loss:  0.0010291672\n",
      "Training Step:  1067 Loss:  0.0010215786\n",
      "Training Step:  1068 Loss:  0.0010140394\n",
      "Training Step:  1069 Loss:  0.001006563\n",
      "Training Step:  1070 Loss:  0.0009991181\n",
      "Training Step:  1071 Loss:  0.0009917492\n",
      "Training Step:  1072 Loss:  0.0009844428\n",
      "Training Step:  1073 Loss:  0.0009771497\n",
      "Training Step:  1074 Loss:  0.0009699564\n",
      "Training Step:  1075 Loss:  0.00096279895\n",
      "Training Step:  1076 Loss:  0.0009556871\n",
      "Training Step:  1077 Loss:  0.0009486362\n",
      "Training Step:  1078 Loss:  0.0009416315\n",
      "Training Step:  1079 Loss:  0.000934683\n",
      "Training Step:  1080 Loss:  0.0009277876\n",
      "Training Step:  1081 Loss:  0.0009209454\n",
      "Training Step:  1082 Loss:  0.00091415405\n",
      "Training Step:  1083 Loss:  0.00090740464\n",
      "Training Step:  1084 Loss:  0.00090070517\n",
      "Training Step:  1085 Loss:  0.00089405535\n",
      "Training Step:  1086 Loss:  0.0008874641\n",
      "Training Step:  1087 Loss:  0.0008809052\n",
      "Training Step:  1088 Loss:  0.00087441696\n",
      "Training Step:  1089 Loss:  0.0008679645\n",
      "Training Step:  1090 Loss:  0.0008615565\n",
      "Training Step:  1091 Loss:  0.0008551996\n",
      "Training Step:  1092 Loss:  0.0008488933\n",
      "Training Step:  1093 Loss:  0.00084262463\n",
      "Training Step:  1094 Loss:  0.0008364109\n",
      "Training Step:  1095 Loss:  0.00083023193\n",
      "Training Step:  1096 Loss:  0.00082409824\n",
      "Training Step:  1097 Loss:  0.0008180205\n",
      "Training Step:  1098 Loss:  0.00081200077\n",
      "Training Step:  1099 Loss:  0.00080599287\n",
      "Training Step:  1100 Loss:  0.0008000498\n",
      "Training Step:  1101 Loss:  0.00079413893\n",
      "Training Step:  1102 Loss:  0.0007882734\n",
      "Training Step:  1103 Loss:  0.0007824609\n",
      "Training Step:  1104 Loss:  0.00077669637\n",
      "Training Step:  1105 Loss:  0.0007709476\n",
      "Training Step:  1106 Loss:  0.00076525914\n",
      "Training Step:  1107 Loss:  0.00075962744\n",
      "Training Step:  1108 Loss:  0.0007540171\n",
      "Training Step:  1109 Loss:  0.00074844254\n",
      "Training Step:  1110 Loss:  0.0007429238\n",
      "Training Step:  1111 Loss:  0.0007374381\n",
      "Training Step:  1112 Loss:  0.0007319996\n",
      "Training Step:  1113 Loss:  0.0007265872\n",
      "Training Step:  1114 Loss:  0.0007212415\n",
      "Training Step:  1115 Loss:  0.0007159071\n",
      "Training Step:  1116 Loss:  0.00071063306\n",
      "Training Step:  1117 Loss:  0.00070538337\n",
      "Training Step:  1118 Loss:  0.0007001746\n",
      "Training Step:  1119 Loss:  0.00069500424\n",
      "Training Step:  1120 Loss:  0.00068987295\n",
      "Training Step:  1121 Loss:  0.00068478275\n",
      "Training Step:  1122 Loss:  0.0006797418\n",
      "Training Step:  1123 Loss:  0.00067472213\n",
      "Training Step:  1124 Loss:  0.0006697368\n",
      "Training Step:  1125 Loss:  0.0006647921\n",
      "Training Step:  1126 Loss:  0.000659896\n",
      "Training Step:  1127 Loss:  0.0006550151\n",
      "Training Step:  1128 Loss:  0.0006501853\n",
      "Training Step:  1129 Loss:  0.00064538844\n",
      "Training Step:  1130 Loss:  0.00064062735\n",
      "Training Step:  1131 Loss:  0.0006359003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  1132 Loss:  0.0006312014\n",
      "Training Step:  1133 Loss:  0.0006265467\n",
      "Training Step:  1134 Loss:  0.0006219134\n",
      "Training Step:  1135 Loss:  0.00061732816\n",
      "Training Step:  1136 Loss:  0.0006127814\n",
      "Training Step:  1137 Loss:  0.0006082535\n",
      "Training Step:  1138 Loss:  0.00060375896\n",
      "Training Step:  1139 Loss:  0.00059932144\n",
      "Training Step:  1140 Loss:  0.000594886\n",
      "Training Step:  1141 Loss:  0.00059049495\n",
      "Training Step:  1142 Loss:  0.0005861312\n",
      "Training Step:  1143 Loss:  0.00058181357\n",
      "Training Step:  1144 Loss:  0.00057753\n",
      "Training Step:  1145 Loss:  0.0005732522\n",
      "Training Step:  1146 Loss:  0.0005690246\n",
      "Training Step:  1147 Loss:  0.00056482584\n",
      "Training Step:  1148 Loss:  0.0005606533\n",
      "Training Step:  1149 Loss:  0.0005565097\n",
      "Training Step:  1150 Loss:  0.00055240217\n",
      "Training Step:  1151 Loss:  0.00054833665\n",
      "Training Step:  1152 Loss:  0.0005442917\n",
      "Training Step:  1153 Loss:  0.00054027\n",
      "Training Step:  1154 Loss:  0.0005362771\n",
      "Training Step:  1155 Loss:  0.00053232786\n",
      "Training Step:  1156 Loss:  0.0005283936\n",
      "Training Step:  1157 Loss:  0.0005245047\n",
      "Training Step:  1158 Loss:  0.00052063185\n",
      "Training Step:  1159 Loss:  0.00051677256\n",
      "Training Step:  1160 Loss:  0.0005129621\n",
      "Training Step:  1161 Loss:  0.0005091859\n",
      "Training Step:  1162 Loss:  0.0005054238\n",
      "Training Step:  1163 Loss:  0.00050170824\n",
      "Training Step:  1164 Loss:  0.0004980001\n",
      "Training Step:  1165 Loss:  0.0004943171\n",
      "Training Step:  1166 Loss:  0.0004906716\n",
      "Training Step:  1167 Loss:  0.00048704087\n",
      "Training Step:  1168 Loss:  0.0004834584\n",
      "Training Step:  1169 Loss:  0.0004798838\n",
      "Training Step:  1170 Loss:  0.0004763546\n",
      "Training Step:  1171 Loss:  0.000472836\n",
      "Training Step:  1172 Loss:  0.0004693371\n",
      "Training Step:  1173 Loss:  0.00046588192\n",
      "Training Step:  1174 Loss:  0.00046244625\n",
      "Training Step:  1175 Loss:  0.00045903027\n",
      "Training Step:  1176 Loss:  0.00045563484\n",
      "Training Step:  1177 Loss:  0.0004522774\n",
      "Training Step:  1178 Loss:  0.00044893322\n",
      "Training Step:  1179 Loss:  0.0004456312\n",
      "Training Step:  1180 Loss:  0.00044234318\n",
      "Training Step:  1181 Loss:  0.00043906772\n",
      "Training Step:  1182 Loss:  0.00043583574\n",
      "Training Step:  1183 Loss:  0.00043261505\n",
      "Training Step:  1184 Loss:  0.00042941907\n",
      "Training Step:  1185 Loss:  0.0004262555\n",
      "Training Step:  1186 Loss:  0.00042310997\n",
      "Training Step:  1187 Loss:  0.00041997852\n",
      "Training Step:  1188 Loss:  0.00041688263\n",
      "Training Step:  1189 Loss:  0.00041380408\n",
      "Training Step:  1190 Loss:  0.0004107611\n",
      "Training Step:  1191 Loss:  0.00040771958\n",
      "Training Step:  1192 Loss:  0.0004047121\n",
      "Training Step:  1193 Loss:  0.00040173097\n",
      "Training Step:  1194 Loss:  0.00039876648\n",
      "Training Step:  1195 Loss:  0.00039582248\n",
      "Training Step:  1196 Loss:  0.00039289743\n",
      "Training Step:  1197 Loss:  0.00039000172\n",
      "Training Step:  1198 Loss:  0.00038711706\n",
      "Training Step:  1199 Loss:  0.00038426532\n",
      "Training Step:  1200 Loss:  0.0003814323\n",
      "Training Step:  1201 Loss:  0.0003786236\n",
      "Training Step:  1202 Loss:  0.0003758176\n",
      "Training Step:  1203 Loss:  0.00037304466\n",
      "Training Step:  1204 Loss:  0.00037030765\n",
      "Training Step:  1205 Loss:  0.00036756598\n",
      "Training Step:  1206 Loss:  0.00036485132\n",
      "Training Step:  1207 Loss:  0.00036216257\n",
      "Training Step:  1208 Loss:  0.00035949075\n",
      "Training Step:  1209 Loss:  0.0003568286\n",
      "Training Step:  1210 Loss:  0.00035419685\n",
      "Training Step:  1211 Loss:  0.00035158615\n",
      "Training Step:  1212 Loss:  0.00034899096\n",
      "Training Step:  1213 Loss:  0.00034641026\n",
      "Training Step:  1214 Loss:  0.00034385495\n",
      "Training Step:  1215 Loss:  0.00034131407\n",
      "Training Step:  1216 Loss:  0.00033880118\n",
      "Training Step:  1217 Loss:  0.0003363075\n",
      "Training Step:  1218 Loss:  0.00033381992\n",
      "Training Step:  1219 Loss:  0.00033135977\n",
      "Training Step:  1220 Loss:  0.00032891068\n",
      "Training Step:  1221 Loss:  0.00032648354\n",
      "Training Step:  1222 Loss:  0.00032407753\n",
      "Training Step:  1223 Loss:  0.00032169092\n",
      "Training Step:  1224 Loss:  0.00031931477\n",
      "Training Step:  1225 Loss:  0.00031695922\n",
      "Training Step:  1226 Loss:  0.0003146178\n",
      "Training Step:  1227 Loss:  0.00031229193\n",
      "Training Step:  1228 Loss:  0.00030998688\n",
      "Training Step:  1229 Loss:  0.00030770764\n",
      "Training Step:  1230 Loss:  0.00030544173\n",
      "Training Step:  1231 Loss:  0.00030317652\n",
      "Training Step:  1232 Loss:  0.0003009446\n",
      "Training Step:  1233 Loss:  0.00029871776\n",
      "Training Step:  1234 Loss:  0.00029651026\n",
      "Training Step:  1235 Loss:  0.00029432029\n",
      "Training Step:  1236 Loss:  0.00029214987\n",
      "Training Step:  1237 Loss:  0.00029000442\n",
      "Training Step:  1238 Loss:  0.00028786046\n",
      "Training Step:  1239 Loss:  0.00028573355\n",
      "Training Step:  1240 Loss:  0.0002836282\n",
      "Training Step:  1241 Loss:  0.00028153847\n",
      "Training Step:  1242 Loss:  0.0002794577\n",
      "Training Step:  1243 Loss:  0.00027739478\n",
      "Training Step:  1244 Loss:  0.0002753464\n",
      "Training Step:  1245 Loss:  0.00027331093\n",
      "Training Step:  1246 Loss:  0.0002712953\n",
      "Training Step:  1247 Loss:  0.00026928852\n",
      "Training Step:  1248 Loss:  0.000267312\n",
      "Training Step:  1249 Loss:  0.0002653337\n",
      "Training Step:  1250 Loss:  0.00026337497\n",
      "Training Step:  1251 Loss:  0.0002614321\n",
      "Training Step:  1252 Loss:  0.0002595058\n",
      "Training Step:  1253 Loss:  0.00025758872\n",
      "Training Step:  1254 Loss:  0.00025568565\n",
      "Training Step:  1255 Loss:  0.0002537986\n",
      "Training Step:  1256 Loss:  0.00025193024\n",
      "Training Step:  1257 Loss:  0.0002500682\n",
      "Training Step:  1258 Loss:  0.00024822174\n",
      "Training Step:  1259 Loss:  0.0002463959\n",
      "Training Step:  1260 Loss:  0.00024457567\n",
      "Training Step:  1261 Loss:  0.00024277116\n",
      "Training Step:  1262 Loss:  0.00024097937\n",
      "Training Step:  1263 Loss:  0.00023920297\n",
      "Training Step:  1264 Loss:  0.00023743516\n",
      "Training Step:  1265 Loss:  0.0002356833\n",
      "Training Step:  1266 Loss:  0.00023393963\n",
      "Training Step:  1267 Loss:  0.00023221411\n",
      "Training Step:  1268 Loss:  0.00023050247\n",
      "Training Step:  1269 Loss:  0.00022880035\n",
      "Training Step:  1270 Loss:  0.0002271154\n",
      "Training Step:  1271 Loss:  0.00022543345\n",
      "Training Step:  1272 Loss:  0.00022377557\n",
      "Training Step:  1273 Loss:  0.00022212492\n",
      "Training Step:  1274 Loss:  0.00022048972\n",
      "Training Step:  1275 Loss:  0.000218858\n",
      "Training Step:  1276 Loss:  0.00021724246\n",
      "Training Step:  1277 Loss:  0.00021563943\n",
      "Training Step:  1278 Loss:  0.00021404518\n",
      "Training Step:  1279 Loss:  0.00021246608\n",
      "Training Step:  1280 Loss:  0.00021090044\n",
      "Training Step:  1281 Loss:  0.0002093482\n",
      "Training Step:  1282 Loss:  0.00020780132\n",
      "Training Step:  1283 Loss:  0.00020626656\n",
      "Training Step:  1284 Loss:  0.000204747\n",
      "Training Step:  1285 Loss:  0.00020323422\n",
      "Training Step:  1286 Loss:  0.00020172683\n",
      "Training Step:  1287 Loss:  0.00020024457\n",
      "Training Step:  1288 Loss:  0.00019876711\n",
      "Training Step:  1289 Loss:  0.00019729495\n",
      "Training Step:  1290 Loss:  0.00019583863\n",
      "Training Step:  1291 Loss:  0.00019440356\n",
      "Training Step:  1292 Loss:  0.00019296829\n",
      "Training Step:  1293 Loss:  0.00019153973\n",
      "Training Step:  1294 Loss:  0.00019013105\n",
      "Training Step:  1295 Loss:  0.00018872875\n",
      "Training Step:  1296 Loss:  0.00018733383\n",
      "Training Step:  1297 Loss:  0.00018595057\n",
      "Training Step:  1298 Loss:  0.00018457841\n",
      "Training Step:  1299 Loss:  0.00018321138\n",
      "Training Step:  1300 Loss:  0.00018186272\n",
      "Training Step:  1301 Loss:  0.00018051988\n",
      "Training Step:  1302 Loss:  0.00017918543\n",
      "Training Step:  1303 Loss:  0.00017786624\n",
      "Training Step:  1304 Loss:  0.00017654864\n",
      "Training Step:  1305 Loss:  0.00017525011\n",
      "Training Step:  1306 Loss:  0.00017396067\n",
      "Training Step:  1307 Loss:  0.0001726709\n",
      "Training Step:  1308 Loss:  0.00017139784\n",
      "Training Step:  1309 Loss:  0.0001701357\n",
      "Training Step:  1310 Loss:  0.00016888381\n",
      "Training Step:  1311 Loss:  0.00016763432\n",
      "Training Step:  1312 Loss:  0.00016640377\n",
      "Training Step:  1313 Loss:  0.00016516767\n",
      "Training Step:  1314 Loss:  0.00016395352\n",
      "Training Step:  1315 Loss:  0.00016274073\n",
      "Training Step:  1316 Loss:  0.00016153863\n",
      "Training Step:  1317 Loss:  0.00016034767\n",
      "Training Step:  1318 Loss:  0.00015916434\n",
      "Training Step:  1319 Loss:  0.00015798763\n",
      "Training Step:  1320 Loss:  0.00015681781\n",
      "Training Step:  1321 Loss:  0.000155662\n",
      "Training Step:  1322 Loss:  0.00015451327\n",
      "Training Step:  1323 Loss:  0.00015337714\n",
      "Training Step:  1324 Loss:  0.00015224429\n",
      "Training Step:  1325 Loss:  0.00015111554\n",
      "Training Step:  1326 Loss:  0.00015001059\n",
      "Training Step:  1327 Loss:  0.00014889943\n",
      "Training Step:  1328 Loss:  0.00014779578\n",
      "Training Step:  1329 Loss:  0.00014670558\n",
      "Training Step:  1330 Loss:  0.00014562038\n",
      "Training Step:  1331 Loss:  0.00014454986\n",
      "Training Step:  1332 Loss:  0.00014348362\n",
      "Training Step:  1333 Loss:  0.00014241919\n",
      "Training Step:  1334 Loss:  0.00014137148\n",
      "Training Step:  1335 Loss:  0.00014033022\n",
      "Training Step:  1336 Loss:  0.00013928734\n",
      "Training Step:  1337 Loss:  0.00013826347\n",
      "Training Step:  1338 Loss:  0.00013724338\n",
      "Training Step:  1339 Loss:  0.00013622531\n",
      "Training Step:  1340 Loss:  0.00013522158\n",
      "Training Step:  1341 Loss:  0.00013421712\n",
      "Training Step:  1342 Loss:  0.00013323565\n",
      "Training Step:  1343 Loss:  0.00013225175\n",
      "Training Step:  1344 Loss:  0.00013127105\n",
      "Training Step:  1345 Loss:  0.00013030633\n",
      "Training Step:  1346 Loss:  0.00012934614\n",
      "Training Step:  1347 Loss:  0.00012839012\n",
      "Training Step:  1348 Loss:  0.00012744541\n",
      "Training Step:  1349 Loss:  0.00012650211\n",
      "Training Step:  1350 Loss:  0.0001255647\n",
      "Training Step:  1351 Loss:  0.00012464548\n",
      "Training Step:  1352 Loss:  0.000123718\n",
      "Training Step:  1353 Loss:  0.00012281556\n",
      "Training Step:  1354 Loss:  0.00012190252\n",
      "Training Step:  1355 Loss:  0.00012100671\n",
      "Training Step:  1356 Loss:  0.00012011055\n",
      "Training Step:  1357 Loss:  0.00011922239\n",
      "Training Step:  1358 Loss:  0.000118339965\n",
      "Training Step:  1359 Loss:  0.00011747195\n",
      "Training Step:  1360 Loss:  0.000116606745\n",
      "Training Step:  1361 Loss:  0.00011573863\n",
      "Training Step:  1362 Loss:  0.00011488742\n",
      "Training Step:  1363 Loss:  0.00011403818\n",
      "Training Step:  1364 Loss:  0.0001132016\n",
      "Training Step:  1365 Loss:  0.00011236543\n",
      "Training Step:  1366 Loss:  0.00011153563\n",
      "Training Step:  1367 Loss:  0.00011071465\n",
      "Training Step:  1368 Loss:  0.00010989324\n",
      "Training Step:  1369 Loss:  0.00010908449\n",
      "Training Step:  1370 Loss:  0.00010827906\n",
      "Training Step:  1371 Loss:  0.00010747495\n",
      "Training Step:  1372 Loss:  0.00010668327\n",
      "Training Step:  1373 Loss:  0.00010589838\n",
      "Training Step:  1374 Loss:  0.00010511449\n",
      "Training Step:  1375 Loss:  0.00010434122\n",
      "Training Step:  1376 Loss:  0.00010356904\n",
      "Training Step:  1377 Loss:  0.00010280787\n",
      "Training Step:  1378 Loss:  0.00010205114\n",
      "Training Step:  1379 Loss:  0.00010129366\n",
      "Training Step:  1380 Loss:  0.00010054955\n",
      "Training Step:  1381 Loss:  9.980607e-05\n",
      "Training Step:  1382 Loss:  9.906884e-05\n",
      "Training Step:  1383 Loss:  9.833974e-05\n",
      "Training Step:  1384 Loss:  9.7611475e-05\n",
      "Training Step:  1385 Loss:  9.688888e-05\n",
      "Training Step:  1386 Loss:  9.617883e-05\n",
      "Training Step:  1387 Loss:  9.546335e-05\n",
      "Training Step:  1388 Loss:  9.47629e-05\n",
      "Training Step:  1389 Loss:  9.406342e-05\n",
      "Training Step:  1390 Loss:  9.336861e-05\n",
      "Training Step:  1391 Loss:  9.267736e-05\n",
      "Training Step:  1392 Loss:  9.199379e-05\n",
      "Training Step:  1393 Loss:  9.1315014e-05\n",
      "Training Step:  1394 Loss:  9.063931e-05\n",
      "Training Step:  1395 Loss:  8.997316e-05\n",
      "Training Step:  1396 Loss:  8.930971e-05\n",
      "Training Step:  1397 Loss:  8.864571e-05\n",
      "Training Step:  1398 Loss:  8.799476e-05\n",
      "Training Step:  1399 Loss:  8.734457e-05\n",
      "Training Step:  1400 Loss:  8.670376e-05\n",
      "Training Step:  1401 Loss:  8.60625e-05\n",
      "Training Step:  1402 Loss:  8.543118e-05\n",
      "Training Step:  1403 Loss:  8.479956e-05\n",
      "Training Step:  1404 Loss:  8.4170126e-05\n",
      "Training Step:  1405 Loss:  8.354768e-05\n",
      "Training Step:  1406 Loss:  8.2934515e-05\n",
      "Training Step:  1407 Loss:  8.232001e-05\n",
      "Training Step:  1408 Loss:  8.171494e-05\n",
      "Training Step:  1409 Loss:  8.1112295e-05\n",
      "Training Step:  1410 Loss:  8.051536e-05\n",
      "Training Step:  1411 Loss:  7.992098e-05\n",
      "Training Step:  1412 Loss:  7.9329395e-05\n",
      "Training Step:  1413 Loss:  7.874513e-05\n",
      "Training Step:  1414 Loss:  7.816391e-05\n",
      "Training Step:  1415 Loss:  7.758574e-05\n",
      "Training Step:  1416 Loss:  7.70109e-05\n",
      "Training Step:  1417 Loss:  7.6444296e-05\n",
      "Training Step:  1418 Loss:  7.588063e-05\n",
      "Training Step:  1419 Loss:  7.5322176e-05\n",
      "Training Step:  1420 Loss:  7.476379e-05\n",
      "Training Step:  1421 Loss:  7.421535e-05\n",
      "Training Step:  1422 Loss:  7.366835e-05\n",
      "Training Step:  1423 Loss:  7.312067e-05\n",
      "Training Step:  1424 Loss:  7.258266e-05\n",
      "Training Step:  1425 Loss:  7.204992e-05\n",
      "Training Step:  1426 Loss:  7.1515344e-05\n",
      "Training Step:  1427 Loss:  7.098558e-05\n",
      "Training Step:  1428 Loss:  7.046559e-05\n",
      "Training Step:  1429 Loss:  6.994278e-05\n",
      "Training Step:  1430 Loss:  6.942553e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  1431 Loss:  6.891194e-05\n",
      "Training Step:  1432 Loss:  6.840659e-05\n",
      "Training Step:  1433 Loss:  6.790189e-05\n",
      "Training Step:  1434 Loss:  6.73986e-05\n",
      "Training Step:  1435 Loss:  6.690475e-05\n",
      "Training Step:  1436 Loss:  6.640814e-05\n",
      "Training Step:  1437 Loss:  6.5922024e-05\n",
      "Training Step:  1438 Loss:  6.5432556e-05\n",
      "Training Step:  1439 Loss:  6.495035e-05\n",
      "Training Step:  1440 Loss:  6.447153e-05\n",
      "Training Step:  1441 Loss:  6.39944e-05\n",
      "Training Step:  1442 Loss:  6.352208e-05\n",
      "Training Step:  1443 Loss:  6.305441e-05\n",
      "Training Step:  1444 Loss:  6.259066e-05\n",
      "Training Step:  1445 Loss:  6.212801e-05\n",
      "Training Step:  1446 Loss:  6.167049e-05\n",
      "Training Step:  1447 Loss:  6.1213315e-05\n",
      "Training Step:  1448 Loss:  6.0764527e-05\n",
      "Training Step:  1449 Loss:  6.031662e-05\n",
      "Training Step:  1450 Loss:  5.986924e-05\n",
      "Training Step:  1451 Loss:  5.942979e-05\n",
      "Training Step:  1452 Loss:  5.8987847e-05\n",
      "Training Step:  1453 Loss:  5.855603e-05\n",
      "Training Step:  1454 Loss:  5.812249e-05\n",
      "Training Step:  1455 Loss:  5.7698133e-05\n",
      "Training Step:  1456 Loss:  5.7268095e-05\n",
      "Training Step:  1457 Loss:  5.684508e-05\n",
      "Training Step:  1458 Loss:  5.642478e-05\n",
      "Training Step:  1459 Loss:  5.6010264e-05\n",
      "Training Step:  1460 Loss:  5.5597226e-05\n",
      "Training Step:  1461 Loss:  5.518517e-05\n",
      "Training Step:  1462 Loss:  5.4779157e-05\n",
      "Training Step:  1463 Loss:  5.4374017e-05\n",
      "Training Step:  1464 Loss:  5.397291e-05\n",
      "Training Step:  1465 Loss:  5.357436e-05\n",
      "Training Step:  1466 Loss:  5.3178155e-05\n",
      "Training Step:  1467 Loss:  5.2791052e-05\n",
      "Training Step:  1468 Loss:  5.239575e-05\n",
      "Training Step:  1469 Loss:  5.201066e-05\n",
      "Training Step:  1470 Loss:  5.1625713e-05\n",
      "Training Step:  1471 Loss:  5.1245268e-05\n",
      "Training Step:  1472 Loss:  5.0869116e-05\n",
      "Training Step:  1473 Loss:  5.0495604e-05\n",
      "Training Step:  1474 Loss:  5.0120434e-05\n",
      "Training Step:  1475 Loss:  4.9750233e-05\n",
      "Training Step:  1476 Loss:  4.9382063e-05\n",
      "Training Step:  1477 Loss:  4.901778e-05\n",
      "Training Step:  1478 Loss:  4.8657268e-05\n",
      "Training Step:  1479 Loss:  4.8300157e-05\n",
      "Training Step:  1480 Loss:  4.7944555e-05\n",
      "Training Step:  1481 Loss:  4.7591268e-05\n",
      "Training Step:  1482 Loss:  4.7238995e-05\n",
      "Training Step:  1483 Loss:  4.6885863e-05\n",
      "Training Step:  1484 Loss:  4.6542405e-05\n",
      "Training Step:  1485 Loss:  4.6195626e-05\n",
      "Training Step:  1486 Loss:  4.5857283e-05\n",
      "Training Step:  1487 Loss:  4.5520574e-05\n",
      "Training Step:  1488 Loss:  4.518354e-05\n",
      "Training Step:  1489 Loss:  4.485058e-05\n",
      "Training Step:  1490 Loss:  4.4517103e-05\n",
      "Training Step:  1491 Loss:  4.4189946e-05\n",
      "Training Step:  1492 Loss:  4.386242e-05\n",
      "Training Step:  1493 Loss:  4.3540233e-05\n",
      "Training Step:  1494 Loss:  4.3218468e-05\n",
      "Training Step:  1495 Loss:  4.2901374e-05\n",
      "Training Step:  1496 Loss:  4.2584405e-05\n",
      "Training Step:  1497 Loss:  4.227026e-05\n",
      "Training Step:  1498 Loss:  4.1957337e-05\n",
      "Training Step:  1499 Loss:  4.164669e-05\n",
      "Training Step:  1500 Loss:  4.1339164e-05\n",
      "Training Step:  1501 Loss:  4.103789e-05\n",
      "Training Step:  1502 Loss:  4.0733623e-05\n",
      "Training Step:  1503 Loss:  4.0430506e-05\n",
      "Training Step:  1504 Loss:  4.0133884e-05\n",
      "Training Step:  1505 Loss:  3.9839462e-05\n",
      "Training Step:  1506 Loss:  3.9545022e-05\n",
      "Training Step:  1507 Loss:  3.9251783e-05\n",
      "Training Step:  1508 Loss:  3.8963586e-05\n",
      "Training Step:  1509 Loss:  3.8674127e-05\n",
      "Training Step:  1510 Loss:  3.8391554e-05\n",
      "Training Step:  1511 Loss:  3.8107613e-05\n",
      "Training Step:  1512 Loss:  3.7822585e-05\n",
      "Training Step:  1513 Loss:  3.7547732e-05\n",
      "Training Step:  1514 Loss:  3.727164e-05\n",
      "Training Step:  1515 Loss:  3.699464e-05\n",
      "Training Step:  1516 Loss:  3.671964e-05\n",
      "Training Step:  1517 Loss:  3.645011e-05\n",
      "Training Step:  1518 Loss:  3.618262e-05\n",
      "Training Step:  1519 Loss:  3.5917215e-05\n",
      "Training Step:  1520 Loss:  3.5648536e-05\n",
      "Training Step:  1521 Loss:  3.538784e-05\n",
      "Training Step:  1522 Loss:  3.5126326e-05\n",
      "Training Step:  1523 Loss:  3.486551e-05\n",
      "Training Step:  1524 Loss:  3.4611225e-05\n",
      "Training Step:  1525 Loss:  3.435508e-05\n",
      "Training Step:  1526 Loss:  3.41006e-05\n",
      "Training Step:  1527 Loss:  3.3849283e-05\n",
      "Training Step:  1528 Loss:  3.3599783e-05\n",
      "Training Step:  1529 Loss:  3.3351225e-05\n",
      "Training Step:  1530 Loss:  3.310572e-05\n",
      "Training Step:  1531 Loss:  3.2860386e-05\n",
      "Training Step:  1532 Loss:  3.2618405e-05\n",
      "Training Step:  1533 Loss:  3.2377873e-05\n",
      "Training Step:  1534 Loss:  3.2139444e-05\n",
      "Training Step:  1535 Loss:  3.1902753e-05\n",
      "Training Step:  1536 Loss:  3.1668074e-05\n",
      "Training Step:  1537 Loss:  3.143513e-05\n",
      "Training Step:  1538 Loss:  3.120224e-05\n",
      "Training Step:  1539 Loss:  3.09711e-05\n",
      "Training Step:  1540 Loss:  3.0743373e-05\n",
      "Training Step:  1541 Loss:  3.0514197e-05\n",
      "Training Step:  1542 Loss:  3.0291389e-05\n",
      "Training Step:  1543 Loss:  3.0068159e-05\n",
      "Training Step:  1544 Loss:  2.9845658e-05\n",
      "Training Step:  1545 Loss:  2.9625462e-05\n",
      "Training Step:  1546 Loss:  2.9406927e-05\n",
      "Training Step:  1547 Loss:  2.9187744e-05\n",
      "Training Step:  1548 Loss:  2.8974926e-05\n",
      "Training Step:  1549 Loss:  2.8762206e-05\n",
      "Training Step:  1550 Loss:  2.854662e-05\n",
      "Training Step:  1551 Loss:  2.8337294e-05\n",
      "Training Step:  1552 Loss:  2.8128205e-05\n",
      "Training Step:  1553 Loss:  2.7919956e-05\n",
      "Training Step:  1554 Loss:  2.7714803e-05\n",
      "Training Step:  1555 Loss:  2.7510141e-05\n",
      "Training Step:  1556 Loss:  2.7306454e-05\n",
      "Training Step:  1557 Loss:  2.7104828e-05\n",
      "Training Step:  1558 Loss:  2.690516e-05\n",
      "Training Step:  1559 Loss:  2.6708396e-05\n",
      "Training Step:  1560 Loss:  2.6509708e-05\n",
      "Training Step:  1561 Loss:  2.6314436e-05\n",
      "Training Step:  1562 Loss:  2.6119611e-05\n",
      "Training Step:  1563 Loss:  2.5928379e-05\n",
      "Training Step:  1564 Loss:  2.5737529e-05\n",
      "Training Step:  1565 Loss:  2.5544665e-05\n",
      "Training Step:  1566 Loss:  2.5357507e-05\n",
      "Training Step:  1567 Loss:  2.5170364e-05\n",
      "Training Step:  1568 Loss:  2.498396e-05\n",
      "Training Step:  1569 Loss:  2.4798079e-05\n",
      "Training Step:  1570 Loss:  2.4616953e-05\n",
      "Training Step:  1571 Loss:  2.443538e-05\n",
      "Training Step:  1572 Loss:  2.4254212e-05\n",
      "Training Step:  1573 Loss:  2.4075838e-05\n",
      "Training Step:  1574 Loss:  2.3897805e-05\n",
      "Training Step:  1575 Loss:  2.3723325e-05\n",
      "Training Step:  1576 Loss:  2.354816e-05\n",
      "Training Step:  1577 Loss:  2.3372613e-05\n",
      "Training Step:  1578 Loss:  2.3201184e-05\n",
      "Training Step:  1579 Loss:  2.3027404e-05\n",
      "Training Step:  1580 Loss:  2.2858112e-05\n",
      "Training Step:  1581 Loss:  2.2690916e-05\n",
      "Training Step:  1582 Loss:  2.2522912e-05\n",
      "Training Step:  1583 Loss:  2.2359014e-05\n",
      "Training Step:  1584 Loss:  2.2191653e-05\n",
      "Training Step:  1585 Loss:  2.20299e-05\n",
      "Training Step:  1586 Loss:  2.18659e-05\n",
      "Training Step:  1587 Loss:  2.1706068e-05\n",
      "Training Step:  1588 Loss:  2.1544603e-05\n",
      "Training Step:  1589 Loss:  2.1384387e-05\n",
      "Training Step:  1590 Loss:  2.1228176e-05\n",
      "Training Step:  1591 Loss:  2.1070811e-05\n",
      "Training Step:  1592 Loss:  2.0915759e-05\n",
      "Training Step:  1593 Loss:  2.076116e-05\n",
      "Training Step:  1594 Loss:  2.0609585e-05\n",
      "Training Step:  1595 Loss:  2.0456559e-05\n",
      "Training Step:  1596 Loss:  2.0304276e-05\n",
      "Training Step:  1597 Loss:  2.0156338e-05\n",
      "Training Step:  1598 Loss:  2.0006675e-05\n",
      "Training Step:  1599 Loss:  1.9857569e-05\n",
      "Training Step:  1600 Loss:  1.9711235e-05\n",
      "Training Step:  1601 Loss:  1.9566081e-05\n",
      "Training Step:  1602 Loss:  1.9423347e-05\n",
      "Training Step:  1603 Loss:  1.927978e-05\n",
      "Training Step:  1604 Loss:  1.9135288e-05\n",
      "Training Step:  1605 Loss:  1.8993753e-05\n",
      "Training Step:  1606 Loss:  1.8855346e-05\n",
      "Training Step:  1607 Loss:  1.8715651e-05\n",
      "Training Step:  1608 Loss:  1.8576819e-05\n",
      "Training Step:  1609 Loss:  1.8438837e-05\n",
      "Training Step:  1610 Loss:  1.8302278e-05\n",
      "Training Step:  1611 Loss:  1.8167564e-05\n",
      "Training Step:  1612 Loss:  1.8032428e-05\n",
      "Training Step:  1613 Loss:  1.7901832e-05\n",
      "Training Step:  1614 Loss:  1.7769036e-05\n",
      "Training Step:  1615 Loss:  1.7638526e-05\n",
      "Training Step:  1616 Loss:  1.7508075e-05\n",
      "Training Step:  1617 Loss:  1.7380056e-05\n",
      "Training Step:  1618 Loss:  1.7251923e-05\n",
      "Training Step:  1619 Loss:  1.712386e-05\n",
      "Training Step:  1620 Loss:  1.6996662e-05\n",
      "Training Step:  1621 Loss:  1.6872114e-05\n",
      "Training Step:  1622 Loss:  1.674565e-05\n",
      "Training Step:  1623 Loss:  1.6623748e-05\n",
      "Training Step:  1624 Loss:  1.6501079e-05\n",
      "Training Step:  1625 Loss:  1.6378659e-05\n",
      "Training Step:  1626 Loss:  1.6256356e-05\n",
      "Training Step:  1627 Loss:  1.6137854e-05\n",
      "Training Step:  1628 Loss:  1.6017246e-05\n",
      "Training Step:  1629 Loss:  1.5900152e-05\n",
      "Training Step:  1630 Loss:  1.5783457e-05\n",
      "Training Step:  1631 Loss:  1.5667545e-05\n",
      "Training Step:  1632 Loss:  1.5549405e-05\n",
      "Training Step:  1633 Loss:  1.543803e-05\n",
      "Training Step:  1634 Loss:  1.532154e-05\n",
      "Training Step:  1635 Loss:  1.5211019e-05\n",
      "Training Step:  1636 Loss:  1.5097421e-05\n",
      "Training Step:  1637 Loss:  1.4985684e-05\n",
      "Training Step:  1638 Loss:  1.4874701e-05\n",
      "Training Step:  1639 Loss:  1.4766287e-05\n",
      "Training Step:  1640 Loss:  1.4657977e-05\n",
      "Training Step:  1641 Loss:  1.45483455e-05\n",
      "Training Step:  1642 Loss:  1.4442823e-05\n",
      "Training Step:  1643 Loss:  1.4334783e-05\n",
      "Training Step:  1644 Loss:  1.422978e-05\n",
      "Training Step:  1645 Loss:  1.4123982e-05\n",
      "Training Step:  1646 Loss:  1.4019134e-05\n",
      "Training Step:  1647 Loss:  1.3915443e-05\n",
      "Training Step:  1648 Loss:  1.3813729e-05\n",
      "Training Step:  1649 Loss:  1.37109055e-05\n",
      "Training Step:  1650 Loss:  1.3610124e-05\n",
      "Training Step:  1651 Loss:  1.35094415e-05\n",
      "Training Step:  1652 Loss:  1.3409109e-05\n",
      "Training Step:  1653 Loss:  1.3310415e-05\n",
      "Training Step:  1654 Loss:  1.3212807e-05\n",
      "Training Step:  1655 Loss:  1.311423e-05\n",
      "Training Step:  1656 Loss:  1.3017276e-05\n",
      "Training Step:  1657 Loss:  1.29230575e-05\n",
      "Training Step:  1658 Loss:  1.2826011e-05\n",
      "Training Step:  1659 Loss:  1.273131e-05\n",
      "Training Step:  1660 Loss:  1.2637547e-05\n",
      "Training Step:  1661 Loss:  1.254369e-05\n",
      "Training Step:  1662 Loss:  1.245259e-05\n",
      "Training Step:  1663 Loss:  1.2360983e-05\n",
      "Training Step:  1664 Loss:  1.2268698e-05\n",
      "Training Step:  1665 Loss:  1.2179402e-05\n",
      "Training Step:  1666 Loss:  1.2089247e-05\n",
      "Training Step:  1667 Loss:  1.1999465e-05\n",
      "Training Step:  1668 Loss:  1.1909824e-05\n",
      "Training Step:  1669 Loss:  1.182211e-05\n",
      "Training Step:  1670 Loss:  1.1734814e-05\n",
      "Training Step:  1671 Loss:  1.1649942e-05\n",
      "Training Step:  1672 Loss:  1.1563095e-05\n",
      "Training Step:  1673 Loss:  1.1478082e-05\n",
      "Training Step:  1674 Loss:  1.1392093e-05\n",
      "Training Step:  1675 Loss:  1.1309074e-05\n",
      "Training Step:  1676 Loss:  1.122436e-05\n",
      "Training Step:  1677 Loss:  1.11425925e-05\n",
      "Training Step:  1678 Loss:  1.1060014e-05\n",
      "Training Step:  1679 Loss:  1.097819e-05\n",
      "Training Step:  1680 Loss:  1.0897948e-05\n",
      "Training Step:  1681 Loss:  1.0817299e-05\n",
      "Training Step:  1682 Loss:  1.0737865e-05\n",
      "Training Step:  1683 Loss:  1.0658977e-05\n",
      "Training Step:  1684 Loss:  1.0578809e-05\n",
      "Training Step:  1685 Loss:  1.0499636e-05\n",
      "Training Step:  1686 Loss:  1.0422915e-05\n",
      "Training Step:  1687 Loss:  1.0345786e-05\n",
      "Training Step:  1688 Loss:  1.02696995e-05\n",
      "Training Step:  1689 Loss:  1.0193295e-05\n",
      "Training Step:  1690 Loss:  1.0119824e-05\n",
      "Training Step:  1691 Loss:  1.0045037e-05\n",
      "Training Step:  1692 Loss:  9.971723e-06\n",
      "Training Step:  1693 Loss:  9.895707e-06\n",
      "Training Step:  1694 Loss:  9.824207e-06\n",
      "Training Step:  1695 Loss:  9.752823e-06\n",
      "Training Step:  1696 Loss:  9.679015e-06\n",
      "Training Step:  1697 Loss:  9.60764e-06\n",
      "Training Step:  1698 Loss:  9.536488e-06\n",
      "Training Step:  1699 Loss:  9.465573e-06\n",
      "Training Step:  1700 Loss:  9.396498e-06\n",
      "Training Step:  1701 Loss:  9.326577e-06\n",
      "Training Step:  1702 Loss:  9.258614e-06\n",
      "Training Step:  1703 Loss:  9.18981e-06\n",
      "Training Step:  1704 Loss:  9.122191e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  1705 Loss:  9.0542435e-06\n",
      "Training Step:  1706 Loss:  8.987238e-06\n",
      "Training Step:  1707 Loss:  8.921651e-06\n",
      "Training Step:  1708 Loss:  8.856012e-06\n",
      "Training Step:  1709 Loss:  8.789457e-06\n",
      "Training Step:  1710 Loss:  8.724379e-06\n",
      "Training Step:  1711 Loss:  8.66078e-06\n",
      "Training Step:  1712 Loss:  8.595465e-06\n",
      "Training Step:  1713 Loss:  8.533278e-06\n",
      "Training Step:  1714 Loss:  8.469528e-06\n",
      "Training Step:  1715 Loss:  8.407243e-06\n",
      "Training Step:  1716 Loss:  8.3449495e-06\n",
      "Training Step:  1717 Loss:  8.283623e-06\n",
      "Training Step:  1718 Loss:  8.2228435e-06\n",
      "Training Step:  1719 Loss:  8.1612525e-06\n",
      "Training Step:  1720 Loss:  8.101146e-06\n",
      "Training Step:  1721 Loss:  8.041357e-06\n",
      "Training Step:  1722 Loss:  7.982037e-06\n",
      "Training Step:  1723 Loss:  7.923984e-06\n",
      "Training Step:  1724 Loss:  7.866606e-06\n",
      "Training Step:  1725 Loss:  7.806202e-06\n",
      "Training Step:  1726 Loss:  7.749676e-06\n",
      "Training Step:  1727 Loss:  7.69288e-06\n",
      "Training Step:  1728 Loss:  7.634026e-06\n",
      "Training Step:  1729 Loss:  7.5792414e-06\n",
      "Training Step:  1730 Loss:  7.523907e-06\n",
      "Training Step:  1731 Loss:  7.466756e-06\n",
      "Training Step:  1732 Loss:  7.4115433e-06\n",
      "Training Step:  1733 Loss:  7.356921e-06\n",
      "Training Step:  1734 Loss:  7.3029523e-06\n",
      "Training Step:  1735 Loss:  7.2481976e-06\n",
      "Training Step:  1736 Loss:  7.1953136e-06\n",
      "Training Step:  1737 Loss:  7.1423515e-06\n",
      "Training Step:  1738 Loss:  7.0901397e-06\n",
      "Training Step:  1739 Loss:  7.0388064e-06\n",
      "Training Step:  1740 Loss:  6.9854877e-06\n",
      "Training Step:  1741 Loss:  6.933861e-06\n",
      "Training Step:  1742 Loss:  6.88405e-06\n",
      "Training Step:  1743 Loss:  6.8330746e-06\n",
      "Training Step:  1744 Loss:  6.782737e-06\n",
      "Training Step:  1745 Loss:  6.7319406e-06\n",
      "Training Step:  1746 Loss:  6.6821367e-06\n",
      "Training Step:  1747 Loss:  6.632322e-06\n",
      "Training Step:  1748 Loss:  6.5844406e-06\n",
      "Training Step:  1749 Loss:  6.535729e-06\n",
      "Training Step:  1750 Loss:  6.4869682e-06\n",
      "Training Step:  1751 Loss:  6.440082e-06\n",
      "Training Step:  1752 Loss:  6.3921448e-06\n",
      "Training Step:  1753 Loss:  6.3449415e-06\n",
      "Training Step:  1754 Loss:  6.2978597e-06\n",
      "Training Step:  1755 Loss:  6.2518193e-06\n",
      "Training Step:  1756 Loss:  6.2052422e-06\n",
      "Training Step:  1757 Loss:  6.160196e-06\n",
      "Training Step:  1758 Loss:  6.1144347e-06\n",
      "Training Step:  1759 Loss:  6.0700313e-06\n",
      "Training Step:  1760 Loss:  6.0241364e-06\n",
      "Training Step:  1761 Loss:  5.9806234e-06\n",
      "Training Step:  1762 Loss:  5.935105e-06\n",
      "Training Step:  1763 Loss:  5.8925525e-06\n",
      "Training Step:  1764 Loss:  5.84905e-06\n",
      "Training Step:  1765 Loss:  5.8049864e-06\n",
      "Training Step:  1766 Loss:  5.7629086e-06\n",
      "Training Step:  1767 Loss:  5.7200714e-06\n",
      "Training Step:  1768 Loss:  5.6779986e-06\n",
      "Training Step:  1769 Loss:  5.63484e-06\n",
      "Training Step:  1770 Loss:  5.5945848e-06\n",
      "Training Step:  1771 Loss:  5.552968e-06\n",
      "Training Step:  1772 Loss:  5.5121354e-06\n",
      "Training Step:  1773 Loss:  5.471522e-06\n",
      "Training Step:  1774 Loss:  5.431877e-06\n",
      "Training Step:  1775 Loss:  5.390608e-06\n",
      "Training Step:  1776 Loss:  5.3509175e-06\n",
      "Training Step:  1777 Loss:  5.311542e-06\n",
      "Training Step:  1778 Loss:  5.271878e-06\n",
      "Training Step:  1779 Loss:  5.232795e-06\n",
      "Training Step:  1780 Loss:  5.1942925e-06\n",
      "Training Step:  1781 Loss:  5.1562647e-06\n",
      "Training Step:  1782 Loss:  5.1192283e-06\n",
      "Training Step:  1783 Loss:  5.0808053e-06\n",
      "Training Step:  1784 Loss:  5.043031e-06\n",
      "Training Step:  1785 Loss:  5.0065137e-06\n",
      "Training Step:  1786 Loss:  4.967817e-06\n",
      "Training Step:  1787 Loss:  4.9316113e-06\n",
      "Training Step:  1788 Loss:  4.8949214e-06\n",
      "Training Step:  1789 Loss:  4.8595925e-06\n",
      "Training Step:  1790 Loss:  4.8241445e-06\n",
      "Training Step:  1791 Loss:  4.7875847e-06\n",
      "Training Step:  1792 Loss:  4.7523126e-06\n",
      "Training Step:  1793 Loss:  4.7171607e-06\n",
      "Training Step:  1794 Loss:  4.6821397e-06\n",
      "Training Step:  1795 Loss:  4.648755e-06\n",
      "Training Step:  1796 Loss:  4.6146524e-06\n",
      "Training Step:  1797 Loss:  4.5808288e-06\n",
      "Training Step:  1798 Loss:  4.5471324e-06\n",
      "Training Step:  1799 Loss:  4.512345e-06\n",
      "Training Step:  1800 Loss:  4.4800213e-06\n",
      "Training Step:  1801 Loss:  4.4469252e-06\n",
      "Training Step:  1802 Loss:  4.4140174e-06\n",
      "Training Step:  1803 Loss:  4.379969e-06\n",
      "Training Step:  1804 Loss:  4.3489035e-06\n",
      "Training Step:  1805 Loss:  4.317015e-06\n",
      "Training Step:  1806 Loss:  4.284645e-06\n",
      "Training Step:  1807 Loss:  4.2535103e-06\n",
      "Training Step:  1808 Loss:  4.2215797e-06\n",
      "Training Step:  1809 Loss:  4.189694e-06\n",
      "Training Step:  1810 Loss:  4.159743e-06\n",
      "Training Step:  1811 Loss:  4.1283874e-06\n",
      "Training Step:  1812 Loss:  4.0979407e-06\n",
      "Training Step:  1813 Loss:  4.068168e-06\n",
      "Training Step:  1814 Loss:  4.0380737e-06\n",
      "Training Step:  1815 Loss:  4.007802e-06\n",
      "Training Step:  1816 Loss:  3.978506e-06\n",
      "Training Step:  1817 Loss:  3.949093e-06\n",
      "Training Step:  1818 Loss:  3.920116e-06\n",
      "Training Step:  1819 Loss:  3.890779e-06\n",
      "Training Step:  1820 Loss:  3.862142e-06\n",
      "Training Step:  1821 Loss:  3.8340813e-06\n",
      "Training Step:  1822 Loss:  3.8060257e-06\n",
      "Training Step:  1823 Loss:  3.777507e-06\n",
      "Training Step:  1824 Loss:  3.7496595e-06\n",
      "Training Step:  1825 Loss:  3.7215977e-06\n",
      "Training Step:  1826 Loss:  3.6943247e-06\n",
      "Training Step:  1827 Loss:  3.6666095e-06\n",
      "Training Step:  1828 Loss:  3.63985e-06\n",
      "Training Step:  1829 Loss:  3.6129886e-06\n",
      "Training Step:  1830 Loss:  3.5862552e-06\n",
      "Training Step:  1831 Loss:  3.5601463e-06\n",
      "Training Step:  1832 Loss:  3.5329683e-06\n",
      "Training Step:  1833 Loss:  3.5074547e-06\n",
      "Training Step:  1834 Loss:  3.481454e-06\n",
      "Training Step:  1835 Loss:  3.4564496e-06\n",
      "Training Step:  1836 Loss:  3.4306393e-06\n",
      "Training Step:  1837 Loss:  3.4045315e-06\n",
      "Training Step:  1838 Loss:  3.3792228e-06\n",
      "Training Step:  1839 Loss:  3.3555343e-06\n",
      "Training Step:  1840 Loss:  3.3303695e-06\n",
      "Training Step:  1841 Loss:  3.30603e-06\n",
      "Training Step:  1842 Loss:  3.2807882e-06\n",
      "Training Step:  1843 Loss:  3.2570563e-06\n",
      "Training Step:  1844 Loss:  3.2331754e-06\n",
      "Training Step:  1845 Loss:  3.2093415e-06\n",
      "Training Step:  1846 Loss:  3.18579e-06\n",
      "Training Step:  1847 Loss:  3.1620268e-06\n",
      "Training Step:  1848 Loss:  3.1390193e-06\n",
      "Training Step:  1849 Loss:  3.1155143e-06\n",
      "Training Step:  1850 Loss:  3.0926765e-06\n",
      "Training Step:  1851 Loss:  3.07002e-06\n",
      "Training Step:  1852 Loss:  3.047138e-06\n",
      "Training Step:  1853 Loss:  3.0243962e-06\n",
      "Training Step:  1854 Loss:  3.001801e-06\n",
      "Training Step:  1855 Loss:  2.980252e-06\n",
      "Training Step:  1856 Loss:  2.9578466e-06\n",
      "Training Step:  1857 Loss:  2.9366338e-06\n",
      "Training Step:  1858 Loss:  2.9146013e-06\n",
      "Training Step:  1859 Loss:  2.8933603e-06\n",
      "Training Step:  1860 Loss:  2.8719169e-06\n",
      "Training Step:  1861 Loss:  2.8509553e-06\n",
      "Training Step:  1862 Loss:  2.8294326e-06\n",
      "Training Step:  1863 Loss:  2.8087454e-06\n",
      "Training Step:  1864 Loss:  2.7871392e-06\n",
      "Training Step:  1865 Loss:  2.7669253e-06\n",
      "Training Step:  1866 Loss:  2.7472947e-06\n",
      "Training Step:  1867 Loss:  2.7261535e-06\n",
      "Training Step:  1868 Loss:  2.707047e-06\n",
      "Training Step:  1869 Loss:  2.6867867e-06\n",
      "Training Step:  1870 Loss:  2.6664984e-06\n",
      "Training Step:  1871 Loss:  2.6467876e-06\n",
      "Training Step:  1872 Loss:  2.627851e-06\n",
      "Training Step:  1873 Loss:  2.6084485e-06\n",
      "Training Step:  1874 Loss:  2.5886532e-06\n",
      "Training Step:  1875 Loss:  2.5700856e-06\n",
      "Training Step:  1876 Loss:  2.550969e-06\n",
      "Training Step:  1877 Loss:  2.5321613e-06\n",
      "Training Step:  1878 Loss:  2.5132997e-06\n",
      "Training Step:  1879 Loss:  2.4953188e-06\n",
      "Training Step:  1880 Loss:  2.4764283e-06\n",
      "Training Step:  1881 Loss:  2.4576584e-06\n",
      "Training Step:  1882 Loss:  2.4393034e-06\n",
      "Training Step:  1883 Loss:  2.4221963e-06\n",
      "Training Step:  1884 Loss:  2.4041221e-06\n",
      "Training Step:  1885 Loss:  2.3868186e-06\n",
      "Training Step:  1886 Loss:  2.3684954e-06\n",
      "Training Step:  1887 Loss:  2.351543e-06\n",
      "Training Step:  1888 Loss:  2.333947e-06\n",
      "Training Step:  1889 Loss:  2.3164298e-06\n",
      "Training Step:  1890 Loss:  2.2993183e-06\n",
      "Training Step:  1891 Loss:  2.2823422e-06\n",
      "Training Step:  1892 Loss:  2.2655904e-06\n",
      "Training Step:  1893 Loss:  2.2485592e-06\n",
      "Training Step:  1894 Loss:  2.2323536e-06\n",
      "Training Step:  1895 Loss:  2.2161548e-06\n",
      "Training Step:  1896 Loss:  2.2003953e-06\n",
      "Training Step:  1897 Loss:  2.1828755e-06\n",
      "Training Step:  1898 Loss:  2.1671824e-06\n",
      "Training Step:  1899 Loss:  2.1512014e-06\n",
      "Training Step:  1900 Loss:  2.1353392e-06\n",
      "Training Step:  1901 Loss:  2.1195733e-06\n",
      "Training Step:  1902 Loss:  2.103623e-06\n",
      "Training Step:  1903 Loss:  2.0879288e-06\n",
      "Training Step:  1904 Loss:  2.0732448e-06\n",
      "Training Step:  1905 Loss:  2.0577527e-06\n",
      "Training Step:  1906 Loss:  2.042202e-06\n",
      "Training Step:  1907 Loss:  2.026982e-06\n",
      "Training Step:  1908 Loss:  2.0123625e-06\n",
      "Training Step:  1909 Loss:  1.997264e-06\n",
      "Training Step:  1910 Loss:  1.9829517e-06\n",
      "Training Step:  1911 Loss:  1.9679346e-06\n",
      "Training Step:  1912 Loss:  1.9542917e-06\n",
      "Training Step:  1913 Loss:  1.9390473e-06\n",
      "Training Step:  1914 Loss:  1.9244642e-06\n",
      "Training Step:  1915 Loss:  1.9104766e-06\n",
      "Training Step:  1916 Loss:  1.8966476e-06\n",
      "Training Step:  1917 Loss:  1.8823525e-06\n",
      "Training Step:  1918 Loss:  1.8694257e-06\n",
      "Training Step:  1919 Loss:  1.8550239e-06\n",
      "Training Step:  1920 Loss:  1.8409855e-06\n",
      "Training Step:  1921 Loss:  1.827663e-06\n",
      "Training Step:  1922 Loss:  1.8142836e-06\n",
      "Training Step:  1923 Loss:  1.8011533e-06\n",
      "Training Step:  1924 Loss:  1.7870038e-06\n",
      "Training Step:  1925 Loss:  1.7744252e-06\n",
      "Training Step:  1926 Loss:  1.7615067e-06\n",
      "Training Step:  1927 Loss:  1.7487281e-06\n",
      "Training Step:  1928 Loss:  1.7350155e-06\n",
      "Training Step:  1929 Loss:  1.722987e-06\n",
      "Training Step:  1930 Loss:  1.7096964e-06\n",
      "Training Step:  1931 Loss:  1.6968484e-06\n",
      "Training Step:  1932 Loss:  1.6843086e-06\n",
      "Training Step:  1933 Loss:  1.6718041e-06\n",
      "Training Step:  1934 Loss:  1.6593767e-06\n",
      "Training Step:  1935 Loss:  1.6473075e-06\n",
      "Training Step:  1936 Loss:  1.6352578e-06\n",
      "Training Step:  1937 Loss:  1.6232974e-06\n",
      "Training Step:  1938 Loss:  1.6113539e-06\n",
      "Training Step:  1939 Loss:  1.599514e-06\n",
      "Training Step:  1940 Loss:  1.5874201e-06\n",
      "Training Step:  1941 Loss:  1.5762782e-06\n",
      "Training Step:  1942 Loss:  1.5641226e-06\n",
      "Training Step:  1943 Loss:  1.5536148e-06\n",
      "Training Step:  1944 Loss:  1.5414761e-06\n",
      "Training Step:  1945 Loss:  1.5305399e-06\n",
      "Training Step:  1946 Loss:  1.5192578e-06\n",
      "Training Step:  1947 Loss:  1.5079077e-06\n",
      "Training Step:  1948 Loss:  1.4964832e-06\n",
      "Training Step:  1949 Loss:  1.4858866e-06\n",
      "Training Step:  1950 Loss:  1.4746026e-06\n",
      "Training Step:  1951 Loss:  1.4637118e-06\n",
      "Training Step:  1952 Loss:  1.4525133e-06\n",
      "Training Step:  1953 Loss:  1.442132e-06\n",
      "Training Step:  1954 Loss:  1.4308695e-06\n",
      "Training Step:  1955 Loss:  1.4208197e-06\n",
      "Training Step:  1956 Loss:  1.4101875e-06\n",
      "Training Step:  1957 Loss:  1.4001309e-06\n",
      "Training Step:  1958 Loss:  1.3895668e-06\n",
      "Training Step:  1959 Loss:  1.3798144e-06\n",
      "Training Step:  1960 Loss:  1.3690951e-06\n",
      "Training Step:  1961 Loss:  1.3589306e-06\n",
      "Training Step:  1962 Loss:  1.3494981e-06\n",
      "Training Step:  1963 Loss:  1.3392089e-06\n",
      "Training Step:  1964 Loss:  1.3292407e-06\n",
      "Training Step:  1965 Loss:  1.319547e-06\n",
      "Training Step:  1966 Loss:  1.3096528e-06\n",
      "Training Step:  1967 Loss:  1.300032e-06\n",
      "Training Step:  1968 Loss:  1.2905284e-06\n",
      "Training Step:  1969 Loss:  1.2809589e-06\n",
      "Training Step:  1970 Loss:  1.2713333e-06\n",
      "Training Step:  1971 Loss:  1.261935e-06\n",
      "Training Step:  1972 Loss:  1.253008e-06\n",
      "Training Step:  1973 Loss:  1.2437004e-06\n",
      "Training Step:  1974 Loss:  1.2338242e-06\n",
      "Training Step:  1975 Loss:  1.2256792e-06\n",
      "Training Step:  1976 Loss:  1.2165322e-06\n",
      "Training Step:  1977 Loss:  1.2073615e-06\n",
      "Training Step:  1978 Loss:  1.1976311e-06\n",
      "Training Step:  1979 Loss:  1.1895687e-06\n",
      "Training Step:  1980 Loss:  1.1807745e-06\n",
      "Training Step:  1981 Loss:  1.1724642e-06\n",
      "Training Step:  1982 Loss:  1.1632079e-06\n",
      "Training Step:  1983 Loss:  1.154833e-06\n",
      "Training Step:  1984 Loss:  1.1460545e-06\n",
      "Training Step:  1985 Loss:  1.1376628e-06\n",
      "Training Step:  1986 Loss:  1.1295833e-06\n",
      "Training Step:  1987 Loss:  1.120902e-06\n",
      "Training Step:  1988 Loss:  1.1126776e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  1989 Loss:  1.1044098e-06\n",
      "Training Step:  1990 Loss:  1.0962951e-06\n",
      "Training Step:  1991 Loss:  1.0883604e-06\n",
      "Training Step:  1992 Loss:  1.0801609e-06\n",
      "Training Step:  1993 Loss:  1.0719637e-06\n",
      "Training Step:  1994 Loss:  1.0639649e-06\n",
      "Training Step:  1995 Loss:  1.0563215e-06\n",
      "Training Step:  1996 Loss:  1.0486856e-06\n",
      "Training Step:  1997 Loss:  1.0416325e-06\n",
      "Training Step:  1998 Loss:  1.0330494e-06\n",
      "Training Step:  1999 Loss:  1.0255164e-06\n",
      "Training Step:  2000 Loss:  1.0178451e-06\n",
      "Training Step:  2001 Loss:  1.0104302e-06\n",
      "Training Step:  2002 Loss:  1.003269e-06\n",
      "Training Step:  2003 Loss:  9.956607e-07\n",
      "[array([-0.9998852], dtype=float32), array([0.9992778], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, name = 'x')\n",
    "y = tf.placeholder(tf.float32, name = 'y')\n",
    "w = tf.Variable([.5], tf.float32, name = 'w')\n",
    "b = tf.Variable([.0], tf.float32, name = 'b')\n",
    "linear_model = w * x + b\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "init = tf.global_variables_initializer()\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
    "train = optimizer.minimize(loss)\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(10000):\n",
    "    loss_val, train_val = sess.run([loss, train], {x: range(1, 10), y: range(0,-9,-1)})\n",
    "    print (\"Training Step: \", i, \"Loss: \", loss_val)\n",
    "    if loss_val < 1.0e-6: break \n",
    "print(sess.run([w, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
