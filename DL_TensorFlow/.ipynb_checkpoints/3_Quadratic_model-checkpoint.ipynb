{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  0 Loss:  6067.29\n",
      "Training Step:  1 Loss:  2874.9124\n",
      "Training Step:  2 Loss:  1364.3499\n",
      "Training Step:  3 Loss:  649.5838\n",
      "Training Step:  4 Loss:  311.37\n",
      "Training Step:  5 Loss:  151.33215\n",
      "Training Step:  6 Loss:  75.60304\n",
      "Training Step:  7 Loss:  39.766838\n",
      "Training Step:  8 Loss:  22.807062\n",
      "Training Step:  9 Loss:  14.779167\n",
      "Training Step:  10 Loss:  10.977639\n",
      "Training Step:  11 Loss:  9.175928\n",
      "Training Step:  12 Loss:  8.320493\n",
      "Training Step:  13 Loss:  7.9128113\n",
      "Training Step:  14 Loss:  7.717\n",
      "Training Step:  15 Loss:  7.6214457\n",
      "Training Step:  16 Loss:  7.5733285\n",
      "Training Step:  17 Loss:  7.5476627\n",
      "Training Step:  18 Loss:  7.532622\n",
      "Training Step:  19 Loss:  7.5226107\n",
      "Training Step:  20 Loss:  7.514982\n",
      "Training Step:  21 Loss:  7.5084815\n",
      "Training Step:  22 Loss:  7.5025196\n",
      "Training Step:  23 Loss:  7.4968157\n",
      "Training Step:  24 Loss:  7.4912324\n",
      "Training Step:  25 Loss:  7.485711\n",
      "Training Step:  26 Loss:  7.4802213\n",
      "Training Step:  27 Loss:  7.4747515\n",
      "Training Step:  28 Loss:  7.4692855\n",
      "Training Step:  29 Loss:  7.463832\n",
      "Training Step:  30 Loss:  7.458383\n",
      "Training Step:  31 Loss:  7.452937\n",
      "Training Step:  32 Loss:  7.4475\n",
      "Training Step:  33 Loss:  7.442065\n",
      "Training Step:  34 Loss:  7.4366355\n",
      "Training Step:  35 Loss:  7.4312096\n",
      "Training Step:  36 Loss:  7.4257865\n",
      "Training Step:  37 Loss:  7.42037\n",
      "Training Step:  38 Loss:  7.414958\n",
      "Training Step:  39 Loss:  7.4095483\n",
      "Training Step:  40 Loss:  7.404144\n",
      "Training Step:  41 Loss:  7.3987446\n",
      "Training Step:  42 Loss:  7.3933506\n",
      "Training Step:  43 Loss:  7.3879585\n",
      "Training Step:  44 Loss:  7.3825717\n",
      "Training Step:  45 Loss:  7.377188\n",
      "Training Step:  46 Loss:  7.371809\n",
      "Training Step:  47 Loss:  7.366438\n",
      "Training Step:  48 Loss:  7.3610687\n",
      "Training Step:  49 Loss:  7.3557057\n",
      "Training Step:  50 Loss:  7.350346\n",
      "Training Step:  51 Loss:  7.34499\n",
      "Training Step:  52 Loss:  7.3396406\n",
      "Training Step:  53 Loss:  7.33429\n",
      "Training Step:  54 Loss:  7.328949\n",
      "Training Step:  55 Loss:  7.3236103\n",
      "Training Step:  56 Loss:  7.3182755\n",
      "Training Step:  57 Loss:  7.3129454\n",
      "Training Step:  58 Loss:  7.307622\n",
      "Training Step:  59 Loss:  7.3023014\n",
      "Training Step:  60 Loss:  7.2969856\n",
      "Training Step:  61 Loss:  7.2916727\n",
      "Training Step:  62 Loss:  7.2863603\n",
      "Training Step:  63 Loss:  7.281059\n",
      "Training Step:  64 Loss:  7.275757\n",
      "Training Step:  65 Loss:  7.270462\n",
      "Training Step:  66 Loss:  7.2651725\n",
      "Training Step:  67 Loss:  7.2598877\n",
      "Training Step:  68 Loss:  7.2546043\n",
      "Training Step:  69 Loss:  7.2493286\n",
      "Training Step:  70 Loss:  7.2440515\n",
      "Training Step:  71 Loss:  7.238785\n",
      "Training Step:  72 Loss:  7.2335186\n",
      "Training Step:  73 Loss:  7.2282586\n",
      "Training Step:  74 Loss:  7.2230015\n",
      "Training Step:  75 Loss:  7.2177505\n",
      "Training Step:  76 Loss:  7.2125006\n",
      "Training Step:  77 Loss:  7.207257\n",
      "Training Step:  78 Loss:  7.2020183\n",
      "Training Step:  79 Loss:  7.1967835\n",
      "Training Step:  80 Loss:  7.191551\n",
      "Training Step:  81 Loss:  7.186325\n",
      "Training Step:  82 Loss:  7.181102\n",
      "Training Step:  83 Loss:  7.1758857\n",
      "Training Step:  84 Loss:  7.1706734\n",
      "Training Step:  85 Loss:  7.1654615\n",
      "Training Step:  86 Loss:  7.1602554\n",
      "Training Step:  87 Loss:  7.1550546\n",
      "Training Step:  88 Loss:  7.1498575\n",
      "Training Step:  89 Loss:  7.144665\n",
      "Training Step:  90 Loss:  7.139475\n",
      "Training Step:  91 Loss:  7.1342907\n",
      "Training Step:  92 Loss:  7.1291156\n",
      "Training Step:  93 Loss:  7.1239367\n",
      "Training Step:  94 Loss:  7.118765\n",
      "Training Step:  95 Loss:  7.113597\n",
      "Training Step:  96 Loss:  7.108433\n",
      "Training Step:  97 Loss:  7.1032734\n",
      "Training Step:  98 Loss:  7.0981183\n",
      "Training Step:  99 Loss:  7.092966\n",
      "Training Step:  100 Loss:  7.0878205\n",
      "Training Step:  101 Loss:  7.0826836\n",
      "Training Step:  102 Loss:  7.0775423\n",
      "Training Step:  103 Loss:  7.0724087\n",
      "Training Step:  104 Loss:  7.067278\n",
      "Training Step:  105 Loss:  7.062151\n",
      "Training Step:  106 Loss:  7.0570316\n",
      "Training Step:  107 Loss:  7.051915\n",
      "Training Step:  108 Loss:  7.0468006\n",
      "Training Step:  109 Loss:  7.0416913\n",
      "Training Step:  110 Loss:  7.036585\n",
      "Training Step:  111 Loss:  7.0314894\n",
      "Training Step:  112 Loss:  7.026389\n",
      "Training Step:  113 Loss:  7.021299\n",
      "Training Step:  114 Loss:  7.0162077\n",
      "Training Step:  115 Loss:  7.011127\n",
      "Training Step:  116 Loss:  7.0060463\n",
      "Training Step:  117 Loss:  7.000967\n",
      "Training Step:  118 Loss:  6.9958982\n",
      "Training Step:  119 Loss:  6.9908314\n",
      "Training Step:  120 Loss:  6.9857683\n",
      "Training Step:  121 Loss:  6.980708\n",
      "Training Step:  122 Loss:  6.9756536\n",
      "Training Step:  123 Loss:  6.970603\n",
      "Training Step:  124 Loss:  6.965555\n",
      "Training Step:  125 Loss:  6.960511\n",
      "Training Step:  126 Loss:  6.955472\n",
      "Training Step:  127 Loss:  6.9504385\n",
      "Training Step:  128 Loss:  6.9454103\n",
      "Training Step:  129 Loss:  6.9403825\n",
      "Training Step:  130 Loss:  6.935361\n",
      "Training Step:  131 Loss:  6.93034\n",
      "Training Step:  132 Loss:  6.9253273\n",
      "Training Step:  133 Loss:  6.920317\n",
      "Training Step:  134 Loss:  6.915311\n",
      "Training Step:  135 Loss:  6.910309\n",
      "Training Step:  136 Loss:  6.9053116\n",
      "Training Step:  137 Loss:  6.9003186\n",
      "Training Step:  138 Loss:  6.895326\n",
      "Training Step:  139 Loss:  6.8903418\n",
      "Training Step:  140 Loss:  6.88536\n",
      "Training Step:  141 Loss:  6.8803844\n",
      "Training Step:  142 Loss:  6.87541\n",
      "Training Step:  143 Loss:  6.8704405\n",
      "Training Step:  144 Loss:  6.8654723\n",
      "Training Step:  145 Loss:  6.8605123\n",
      "Training Step:  146 Loss:  6.8555584\n",
      "Training Step:  147 Loss:  6.850603\n",
      "Training Step:  148 Loss:  6.8456526\n",
      "Training Step:  149 Loss:  6.8407054\n",
      "Training Step:  150 Loss:  6.835766\n",
      "Training Step:  151 Loss:  6.8308287\n",
      "Training Step:  152 Loss:  6.8258944\n",
      "Training Step:  153 Loss:  6.8209643\n",
      "Training Step:  154 Loss:  6.816042\n",
      "Training Step:  155 Loss:  6.8111215\n",
      "Training Step:  156 Loss:  6.8062043\n",
      "Training Step:  157 Loss:  6.80129\n",
      "Training Step:  158 Loss:  6.796379\n",
      "Training Step:  159 Loss:  6.791475\n",
      "Training Step:  160 Loss:  6.786575\n",
      "Training Step:  161 Loss:  6.781676\n",
      "Training Step:  162 Loss:  6.7767816\n",
      "Training Step:  163 Loss:  6.771893\n",
      "Training Step:  164 Loss:  6.767009\n",
      "Training Step:  165 Loss:  6.762128\n",
      "Training Step:  166 Loss:  6.7572503\n",
      "Training Step:  167 Loss:  6.7523756\n",
      "Training Step:  168 Loss:  6.747508\n",
      "Training Step:  169 Loss:  6.7426405\n",
      "Training Step:  170 Loss:  6.7377806\n",
      "Training Step:  171 Loss:  6.732923\n",
      "Training Step:  172 Loss:  6.7280703\n",
      "Training Step:  173 Loss:  6.7232203\n",
      "Training Step:  174 Loss:  6.718376\n",
      "Training Step:  175 Loss:  6.7135324\n",
      "Training Step:  176 Loss:  6.7086935\n",
      "Training Step:  177 Loss:  6.7038574\n",
      "Training Step:  178 Loss:  6.6990294\n",
      "Training Step:  179 Loss:  6.694204\n",
      "Training Step:  180 Loss:  6.6893806\n",
      "Training Step:  181 Loss:  6.684563\n",
      "Training Step:  182 Loss:  6.679749\n",
      "Training Step:  183 Loss:  6.674939\n",
      "Training Step:  184 Loss:  6.6701303\n",
      "Training Step:  185 Loss:  6.66533\n",
      "Training Step:  186 Loss:  6.66053\n",
      "Training Step:  187 Loss:  6.655735\n",
      "Training Step:  188 Loss:  6.6509457\n",
      "Training Step:  189 Loss:  6.646159\n",
      "Training Step:  190 Loss:  6.6413755\n",
      "Training Step:  191 Loss:  6.6365967\n",
      "Training Step:  192 Loss:  6.6318197\n",
      "Training Step:  193 Loss:  6.6270523\n",
      "Training Step:  194 Loss:  6.622282\n",
      "Training Step:  195 Loss:  6.617521\n",
      "Training Step:  196 Loss:  6.6127596\n",
      "Training Step:  197 Loss:  6.6080046\n",
      "Training Step:  198 Loss:  6.603248\n",
      "Training Step:  199 Loss:  6.598501\n",
      "Training Step:  200 Loss:  6.5937576\n",
      "Training Step:  201 Loss:  6.5890183\n",
      "Training Step:  202 Loss:  6.58428\n",
      "Training Step:  203 Loss:  6.579547\n",
      "Training Step:  204 Loss:  6.574821\n",
      "Training Step:  205 Loss:  6.570093\n",
      "Training Step:  206 Loss:  6.5653744\n",
      "Training Step:  207 Loss:  6.5606565\n",
      "Training Step:  208 Loss:  6.555944\n",
      "Training Step:  209 Loss:  6.5512333\n",
      "Training Step:  210 Loss:  6.546527\n",
      "Training Step:  211 Loss:  6.541826\n",
      "Training Step:  212 Loss:  6.537127\n",
      "Training Step:  213 Loss:  6.5324316\n",
      "Training Step:  214 Loss:  6.5277433\n",
      "Training Step:  215 Loss:  6.5230536\n",
      "Training Step:  216 Loss:  6.5183697\n",
      "Training Step:  217 Loss:  6.5136943\n",
      "Training Step:  218 Loss:  6.509014\n",
      "Training Step:  219 Loss:  6.504346\n",
      "Training Step:  220 Loss:  6.499675\n",
      "Training Step:  221 Loss:  6.4950113\n",
      "Training Step:  222 Loss:  6.490355\n",
      "Training Step:  223 Loss:  6.4856977\n",
      "Training Step:  224 Loss:  6.4810467\n",
      "Training Step:  225 Loss:  6.4763966\n",
      "Training Step:  226 Loss:  6.4717507\n",
      "Training Step:  227 Loss:  6.4671097\n",
      "Training Step:  228 Loss:  6.4624715\n",
      "Training Step:  229 Loss:  6.457841\n",
      "Training Step:  230 Loss:  6.453209\n",
      "Training Step:  231 Loss:  6.4485836\n",
      "Training Step:  232 Loss:  6.443962\n",
      "Training Step:  233 Loss:  6.439341\n",
      "Training Step:  234 Loss:  6.4347267\n",
      "Training Step:  235 Loss:  6.4301157\n",
      "Training Step:  236 Loss:  6.425508\n",
      "Training Step:  237 Loss:  6.4209023\n",
      "Training Step:  238 Loss:  6.4163065\n",
      "Training Step:  239 Loss:  6.4117107\n",
      "Training Step:  240 Loss:  6.407115\n",
      "Training Step:  241 Loss:  6.402527\n",
      "Training Step:  242 Loss:  6.3979416\n",
      "Training Step:  243 Loss:  6.393359\n",
      "Training Step:  244 Loss:  6.3887825\n",
      "Training Step:  245 Loss:  6.384212\n",
      "Training Step:  246 Loss:  6.3796387\n",
      "Training Step:  247 Loss:  6.3750725\n",
      "Training Step:  248 Loss:  6.3705115\n",
      "Training Step:  249 Loss:  6.3659515\n",
      "Training Step:  250 Loss:  6.361396\n",
      "Training Step:  251 Loss:  6.356845\n",
      "Training Step:  252 Loss:  6.352297\n",
      "Training Step:  253 Loss:  6.347752\n",
      "Training Step:  254 Loss:  6.343214\n",
      "Training Step:  255 Loss:  6.3386755\n",
      "Training Step:  256 Loss:  6.3341446\n",
      "Training Step:  257 Loss:  6.3296127\n",
      "Training Step:  258 Loss:  6.3250895\n",
      "Training Step:  259 Loss:  6.320565\n",
      "Training Step:  260 Loss:  6.3160458\n",
      "Training Step:  261 Loss:  6.3115325\n",
      "Training Step:  262 Loss:  6.307021\n",
      "Training Step:  263 Loss:  6.302516\n",
      "Training Step:  264 Loss:  6.2980123\n",
      "Training Step:  265 Loss:  6.2935133\n",
      "Training Step:  266 Loss:  6.2890134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  267 Loss:  6.284523\n",
      "Training Step:  268 Loss:  6.2800326\n",
      "Training Step:  269 Loss:  6.2755475\n",
      "Training Step:  270 Loss:  6.271064\n",
      "Training Step:  271 Loss:  6.2665896\n",
      "Training Step:  272 Loss:  6.262113\n",
      "Training Step:  273 Loss:  6.257645\n",
      "Training Step:  274 Loss:  6.2531734\n",
      "Training Step:  275 Loss:  6.2487116\n",
      "Training Step:  276 Loss:  6.2442513\n",
      "Training Step:  277 Loss:  6.239795\n",
      "Training Step:  278 Loss:  6.235344\n",
      "Training Step:  279 Loss:  6.2308955\n",
      "Training Step:  280 Loss:  6.2264485\n",
      "Training Step:  281 Loss:  6.2220097\n",
      "Training Step:  282 Loss:  6.217567\n",
      "Training Step:  283 Loss:  6.213134\n",
      "Training Step:  284 Loss:  6.208703\n",
      "Training Step:  285 Loss:  6.2042737\n",
      "Training Step:  286 Loss:  6.19985\n",
      "Training Step:  287 Loss:  6.1954308\n",
      "Training Step:  288 Loss:  6.1910152\n",
      "Training Step:  289 Loss:  6.1866007\n",
      "Training Step:  290 Loss:  6.182193\n",
      "Training Step:  291 Loss:  6.1777864\n",
      "Training Step:  292 Loss:  6.173384\n",
      "Training Step:  293 Loss:  6.168985\n",
      "Training Step:  294 Loss:  6.164589\n",
      "Training Step:  295 Loss:  6.1602006\n",
      "Training Step:  296 Loss:  6.155811\n",
      "Training Step:  297 Loss:  6.1514273\n",
      "Training Step:  298 Loss:  6.147044\n",
      "Training Step:  299 Loss:  6.142669\n",
      "Training Step:  300 Loss:  6.138294\n",
      "Training Step:  301 Loss:  6.1339273\n",
      "Training Step:  302 Loss:  6.1295586\n",
      "Training Step:  303 Loss:  6.1251955\n",
      "Training Step:  304 Loss:  6.1208367\n",
      "Training Step:  305 Loss:  6.116483\n",
      "Training Step:  306 Loss:  6.1121254\n",
      "Training Step:  307 Loss:  6.1077766\n",
      "Training Step:  308 Loss:  6.103433\n",
      "Training Step:  309 Loss:  6.099093\n",
      "Training Step:  310 Loss:  6.0947537\n",
      "Training Step:  311 Loss:  6.0904193\n",
      "Training Step:  312 Loss:  6.0860863\n",
      "Training Step:  313 Loss:  6.081758\n",
      "Training Step:  314 Loss:  6.0774336\n",
      "Training Step:  315 Loss:  6.073115\n",
      "Training Step:  316 Loss:  6.0688\n",
      "Training Step:  317 Loss:  6.064483\n",
      "Training Step:  318 Loss:  6.060172\n",
      "Training Step:  319 Loss:  6.0558653\n",
      "Training Step:  320 Loss:  6.051565\n",
      "Training Step:  321 Loss:  6.047266\n",
      "Training Step:  322 Loss:  6.042969\n",
      "Training Step:  323 Loss:  6.0386767\n",
      "Training Step:  324 Loss:  6.034385\n",
      "Training Step:  325 Loss:  6.0301003\n",
      "Training Step:  326 Loss:  6.025819\n",
      "Training Step:  327 Loss:  6.0215383\n",
      "Training Step:  328 Loss:  6.017266\n",
      "Training Step:  329 Loss:  6.0129933\n",
      "Training Step:  330 Loss:  6.008726\n",
      "Training Step:  331 Loss:  6.0044613\n",
      "Training Step:  332 Loss:  6.0001984\n",
      "Training Step:  333 Loss:  5.995941\n",
      "Training Step:  334 Loss:  5.9916835\n",
      "Training Step:  335 Loss:  5.987437\n",
      "Training Step:  336 Loss:  5.983188\n",
      "Training Step:  337 Loss:  5.978944\n",
      "Training Step:  338 Loss:  5.9747043\n",
      "Training Step:  339 Loss:  5.9704657\n",
      "Training Step:  340 Loss:  5.9662313\n",
      "Training Step:  341 Loss:  5.9620028\n",
      "Training Step:  342 Loss:  5.957775\n",
      "Training Step:  343 Loss:  5.953551\n",
      "Training Step:  344 Loss:  5.949333\n",
      "Training Step:  345 Loss:  5.945116\n",
      "Training Step:  346 Loss:  5.940904\n",
      "Training Step:  347 Loss:  5.9366937\n",
      "Training Step:  348 Loss:  5.9324846\n",
      "Training Step:  349 Loss:  5.928283\n",
      "Training Step:  350 Loss:  5.924085\n",
      "Training Step:  351 Loss:  5.9198885\n",
      "Training Step:  352 Loss:  5.915694\n",
      "Training Step:  353 Loss:  5.9115076\n",
      "Training Step:  354 Loss:  5.9073215\n",
      "Training Step:  355 Loss:  5.90314\n",
      "Training Step:  356 Loss:  5.8989606\n",
      "Training Step:  357 Loss:  5.894783\n",
      "Training Step:  358 Loss:  5.8906093\n",
      "Training Step:  359 Loss:  5.886441\n",
      "Training Step:  360 Loss:  5.8822765\n",
      "Training Step:  361 Loss:  5.878115\n",
      "Training Step:  362 Loss:  5.8739576\n",
      "Training Step:  363 Loss:  5.8698034\n",
      "Training Step:  364 Loss:  5.8656497\n",
      "Training Step:  365 Loss:  5.8615017\n",
      "Training Step:  366 Loss:  5.857357\n",
      "Training Step:  367 Loss:  5.853213\n",
      "Training Step:  368 Loss:  5.8490753\n",
      "Training Step:  369 Loss:  5.8449388\n",
      "Training Step:  370 Loss:  5.8408113\n",
      "Training Step:  371 Loss:  5.8366823\n",
      "Training Step:  372 Loss:  5.8325586\n",
      "Training Step:  373 Loss:  5.828435\n",
      "Training Step:  374 Loss:  5.8243165\n",
      "Training Step:  375 Loss:  5.8202004\n",
      "Training Step:  376 Loss:  5.81609\n",
      "Training Step:  377 Loss:  5.81198\n",
      "Training Step:  378 Loss:  5.8078756\n",
      "Training Step:  379 Loss:  5.803775\n",
      "Training Step:  380 Loss:  5.7996774\n",
      "Training Step:  381 Loss:  5.795583\n",
      "Training Step:  382 Loss:  5.791491\n",
      "Training Step:  383 Loss:  5.787403\n",
      "Training Step:  384 Loss:  5.7833195\n",
      "Training Step:  385 Loss:  5.7792344\n",
      "Training Step:  386 Loss:  5.775158\n",
      "Training Step:  387 Loss:  5.771082\n",
      "Training Step:  388 Loss:  5.7670083\n",
      "Training Step:  389 Loss:  5.762943\n",
      "Training Step:  390 Loss:  5.7588773\n",
      "Training Step:  391 Loss:  5.7548137\n",
      "Training Step:  392 Loss:  5.7507563\n",
      "Training Step:  393 Loss:  5.7467012\n",
      "Training Step:  394 Loss:  5.7426505\n",
      "Training Step:  395 Loss:  5.738598\n",
      "Training Step:  396 Loss:  5.7345586\n",
      "Training Step:  397 Loss:  5.730516\n",
      "Training Step:  398 Loss:  5.726475\n",
      "Training Step:  399 Loss:  5.72244\n",
      "Training Step:  400 Loss:  5.7184086\n",
      "Training Step:  401 Loss:  5.7143784\n",
      "Training Step:  402 Loss:  5.7103515\n",
      "Training Step:  403 Loss:  5.7063313\n",
      "Training Step:  404 Loss:  5.702313\n",
      "Training Step:  405 Loss:  5.698296\n",
      "Training Step:  406 Loss:  5.6942844\n",
      "Training Step:  407 Loss:  5.690275\n",
      "Training Step:  408 Loss:  5.686268\n",
      "Training Step:  409 Loss:  5.682268\n",
      "Training Step:  410 Loss:  5.6782675\n",
      "Training Step:  411 Loss:  5.67427\n",
      "Training Step:  412 Loss:  5.670279\n",
      "Training Step:  413 Loss:  5.666287\n",
      "Training Step:  414 Loss:  5.662298\n",
      "Training Step:  415 Loss:  5.6583185\n",
      "Training Step:  416 Loss:  5.654339\n",
      "Training Step:  417 Loss:  5.650363\n",
      "Training Step:  418 Loss:  5.6463876\n",
      "Training Step:  419 Loss:  5.642419\n",
      "Training Step:  420 Loss:  5.6384506\n",
      "Training Step:  421 Loss:  5.6344852\n",
      "Training Step:  422 Loss:  5.6305265\n",
      "Training Step:  423 Loss:  5.626568\n",
      "Training Step:  424 Loss:  5.6226125\n",
      "Training Step:  425 Loss:  5.618664\n",
      "Training Step:  426 Loss:  5.6147156\n",
      "Training Step:  427 Loss:  5.6107717\n",
      "Training Step:  428 Loss:  5.606829\n",
      "Training Step:  429 Loss:  5.6028895\n",
      "Training Step:  430 Loss:  5.5989575\n",
      "Training Step:  431 Loss:  5.5950227\n",
      "Training Step:  432 Loss:  5.5910954\n",
      "Training Step:  433 Loss:  5.587171\n",
      "Training Step:  434 Loss:  5.5832477\n",
      "Training Step:  435 Loss:  5.5793295\n",
      "Training Step:  436 Loss:  5.5754128\n",
      "Training Step:  437 Loss:  5.5715027\n",
      "Training Step:  438 Loss:  5.567592\n",
      "Training Step:  439 Loss:  5.5636888\n",
      "Training Step:  440 Loss:  5.5597816\n",
      "Training Step:  441 Loss:  5.5558825\n",
      "Training Step:  442 Loss:  5.5519867\n",
      "Training Step:  443 Loss:  5.54809\n",
      "Training Step:  444 Loss:  5.5442004\n",
      "Training Step:  445 Loss:  5.5403137\n",
      "Training Step:  446 Loss:  5.5364323\n",
      "Training Step:  447 Loss:  5.53255\n",
      "Training Step:  448 Loss:  5.5286713\n",
      "Training Step:  449 Loss:  5.5247955\n",
      "Training Step:  450 Loss:  5.5209255\n",
      "Training Step:  451 Loss:  5.517055\n",
      "Training Step:  452 Loss:  5.513193\n",
      "Training Step:  453 Loss:  5.50933\n",
      "Training Step:  454 Loss:  5.5054703\n",
      "Training Step:  455 Loss:  5.5016136\n",
      "Training Step:  456 Loss:  5.497763\n",
      "Training Step:  457 Loss:  5.493915\n",
      "Training Step:  458 Loss:  5.490067\n",
      "Training Step:  459 Loss:  5.4862223\n",
      "Training Step:  460 Loss:  5.4823833\n",
      "Training Step:  461 Loss:  5.4785447\n",
      "Training Step:  462 Loss:  5.474713\n",
      "Training Step:  463 Loss:  5.4708805\n",
      "Training Step:  464 Loss:  5.4670525\n",
      "Training Step:  465 Loss:  5.463228\n",
      "Training Step:  466 Loss:  5.459405\n",
      "Training Step:  467 Loss:  5.4555893\n",
      "Training Step:  468 Loss:  5.451775\n",
      "Training Step:  469 Loss:  5.4479604\n",
      "Training Step:  470 Loss:  5.444151\n",
      "Training Step:  471 Loss:  5.4403477\n",
      "Training Step:  472 Loss:  5.4365454\n",
      "Training Step:  473 Loss:  5.4327445\n",
      "Training Step:  474 Loss:  5.4289494\n",
      "Training Step:  475 Loss:  5.4251523\n",
      "Training Step:  476 Loss:  5.4213624\n",
      "Training Step:  477 Loss:  5.417574\n",
      "Training Step:  478 Loss:  5.4137897\n",
      "Training Step:  479 Loss:  5.4100084\n",
      "Training Step:  480 Loss:  5.406232\n",
      "Training Step:  481 Loss:  5.4024563\n",
      "Training Step:  482 Loss:  5.398685\n",
      "Training Step:  483 Loss:  5.3949146\n",
      "Training Step:  484 Loss:  5.3911476\n",
      "Training Step:  485 Loss:  5.3873854\n",
      "Training Step:  486 Loss:  5.3836284\n",
      "Training Step:  487 Loss:  5.379871\n",
      "Training Step:  488 Loss:  5.376115\n",
      "Training Step:  489 Loss:  5.3723626\n",
      "Training Step:  490 Loss:  5.3686156\n",
      "Training Step:  491 Loss:  5.364871\n",
      "Training Step:  492 Loss:  5.3611298\n",
      "Training Step:  493 Loss:  5.3573914\n",
      "Training Step:  494 Loss:  5.353657\n",
      "Training Step:  495 Loss:  5.3499246\n",
      "Training Step:  496 Loss:  5.346195\n",
      "Training Step:  497 Loss:  5.3424683\n",
      "Training Step:  498 Loss:  5.338745\n",
      "Training Step:  499 Loss:  5.3350267\n",
      "Training Step:  500 Loss:  5.3313055\n",
      "Training Step:  501 Loss:  5.327593\n",
      "Training Step:  502 Loss:  5.3238816\n",
      "Training Step:  503 Loss:  5.3201733\n",
      "Training Step:  504 Loss:  5.3164682\n",
      "Training Step:  505 Loss:  5.312769\n",
      "Training Step:  506 Loss:  5.3090677\n",
      "Training Step:  507 Loss:  5.30537\n",
      "Training Step:  508 Loss:  5.301676\n",
      "Training Step:  509 Loss:  5.297988\n",
      "Training Step:  510 Loss:  5.2943\n",
      "Training Step:  511 Loss:  5.2906165\n",
      "Training Step:  512 Loss:  5.286937\n",
      "Training Step:  513 Loss:  5.283254\n",
      "Training Step:  514 Loss:  5.279582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  515 Loss:  5.2759104\n",
      "Training Step:  516 Loss:  5.272239\n",
      "Training Step:  517 Loss:  5.2685733\n",
      "Training Step:  518 Loss:  5.264912\n",
      "Training Step:  519 Loss:  5.2612505\n",
      "Training Step:  520 Loss:  5.257593\n",
      "Training Step:  521 Loss:  5.253939\n",
      "Training Step:  522 Loss:  5.250288\n",
      "Training Step:  523 Loss:  5.246637\n",
      "Training Step:  524 Loss:  5.2429934\n",
      "Training Step:  525 Loss:  5.2393503\n",
      "Training Step:  526 Loss:  5.235708\n",
      "Training Step:  527 Loss:  5.232075\n",
      "Training Step:  528 Loss:  5.2284393\n",
      "Training Step:  529 Loss:  5.2248106\n",
      "Training Step:  530 Loss:  5.2211833\n",
      "Training Step:  531 Loss:  5.217556\n",
      "Training Step:  532 Loss:  5.213937\n",
      "Training Step:  533 Loss:  5.2103143\n",
      "Training Step:  534 Loss:  5.2067\n",
      "Training Step:  535 Loss:  5.203088\n",
      "Training Step:  536 Loss:  5.199475\n",
      "Training Step:  537 Loss:  5.1958685\n",
      "Training Step:  538 Loss:  5.1922655\n",
      "Training Step:  539 Loss:  5.188662\n",
      "Training Step:  540 Loss:  5.185066\n",
      "Training Step:  541 Loss:  5.1814723\n",
      "Training Step:  542 Loss:  5.1778774\n",
      "Training Step:  543 Loss:  5.1742883\n",
      "Training Step:  544 Loss:  5.1707\n",
      "Training Step:  545 Loss:  5.167116\n",
      "Training Step:  546 Loss:  5.1635365\n",
      "Training Step:  547 Loss:  5.1599584\n",
      "Training Step:  548 Loss:  5.156382\n",
      "Training Step:  549 Loss:  5.152811\n",
      "Training Step:  550 Loss:  5.14924\n",
      "Training Step:  551 Loss:  5.1456747\n",
      "Training Step:  552 Loss:  5.1421113\n",
      "Training Step:  553 Loss:  5.138553\n",
      "Training Step:  554 Loss:  5.1349945\n",
      "Training Step:  555 Loss:  5.1314397\n",
      "Training Step:  556 Loss:  5.1278877\n",
      "Training Step:  557 Loss:  5.124339\n",
      "Training Step:  558 Loss:  5.1207914\n",
      "Training Step:  559 Loss:  5.117248\n",
      "Training Step:  560 Loss:  5.1137075\n",
      "Training Step:  561 Loss:  5.1101704\n",
      "Training Step:  562 Loss:  5.1066384\n",
      "Training Step:  563 Loss:  5.1031046\n",
      "Training Step:  564 Loss:  5.099573\n",
      "Training Step:  565 Loss:  5.0960493\n",
      "Training Step:  566 Loss:  5.092525\n",
      "Training Step:  567 Loss:  5.089007\n",
      "Training Step:  568 Loss:  5.0854893\n",
      "Training Step:  569 Loss:  5.081973\n",
      "Training Step:  570 Loss:  5.0784636\n",
      "Training Step:  571 Loss:  5.074952\n",
      "Training Step:  572 Loss:  5.0714464\n",
      "Training Step:  573 Loss:  5.067943\n",
      "Training Step:  574 Loss:  5.0644407\n",
      "Training Step:  575 Loss:  5.060945\n",
      "Training Step:  576 Loss:  5.0574517\n",
      "Training Step:  577 Loss:  5.0539584\n",
      "Training Step:  578 Loss:  5.0504727\n",
      "Training Step:  579 Loss:  5.0469847\n",
      "Training Step:  580 Loss:  5.0435004\n",
      "Training Step:  581 Loss:  5.0400205\n",
      "Training Step:  582 Loss:  5.036543\n",
      "Training Step:  583 Loss:  5.033069\n",
      "Training Step:  584 Loss:  5.0295973\n",
      "Training Step:  585 Loss:  5.0261264\n",
      "Training Step:  586 Loss:  5.0226593\n",
      "Training Step:  587 Loss:  5.0191975\n",
      "Training Step:  588 Loss:  5.0157366\n",
      "Training Step:  589 Loss:  5.0122786\n",
      "Training Step:  590 Loss:  5.008823\n",
      "Training Step:  591 Loss:  5.0053725\n",
      "Training Step:  592 Loss:  5.0019207\n",
      "Training Step:  593 Loss:  4.998476\n",
      "Training Step:  594 Loss:  4.995032\n",
      "Training Step:  595 Loss:  4.9915876\n",
      "Training Step:  596 Loss:  4.988152\n",
      "Training Step:  597 Loss:  4.984718\n",
      "Training Step:  598 Loss:  4.981281\n",
      "Training Step:  599 Loss:  4.9778514\n",
      "Training Step:  600 Loss:  4.974427\n",
      "Training Step:  601 Loss:  4.9710007\n",
      "Training Step:  602 Loss:  4.9675803\n",
      "Training Step:  603 Loss:  4.96416\n",
      "Training Step:  604 Loss:  4.960745\n",
      "Training Step:  605 Loss:  4.9573317\n",
      "Training Step:  606 Loss:  4.95392\n",
      "Training Step:  607 Loss:  4.9505157\n",
      "Training Step:  608 Loss:  4.947109\n",
      "Training Step:  609 Loss:  4.9437046\n",
      "Training Step:  610 Loss:  4.940306\n",
      "Training Step:  611 Loss:  4.936912\n",
      "Training Step:  612 Loss:  4.9335184\n",
      "Training Step:  613 Loss:  4.930125\n",
      "Training Step:  614 Loss:  4.926737\n",
      "Training Step:  615 Loss:  4.923349\n",
      "Training Step:  616 Loss:  4.9199667\n",
      "Training Step:  617 Loss:  4.9165845\n",
      "Training Step:  618 Loss:  4.9132094\n",
      "Training Step:  619 Loss:  4.9098334\n",
      "Training Step:  620 Loss:  4.9064627\n",
      "Training Step:  621 Loss:  4.903092\n",
      "Training Step:  622 Loss:  4.899728\n",
      "Training Step:  623 Loss:  4.8963647\n",
      "Training Step:  624 Loss:  4.8930025\n",
      "Training Step:  625 Loss:  4.8896456\n",
      "Training Step:  626 Loss:  4.8862886\n",
      "Training Step:  627 Loss:  4.882934\n",
      "Training Step:  628 Loss:  4.8795867\n",
      "Training Step:  629 Loss:  4.876237\n",
      "Training Step:  630 Loss:  4.872893\n",
      "Training Step:  631 Loss:  4.8695536\n",
      "Training Step:  632 Loss:  4.8662124\n",
      "Training Step:  633 Loss:  4.8628755\n",
      "Training Step:  634 Loss:  4.85954\n",
      "Training Step:  635 Loss:  4.85621\n",
      "Training Step:  636 Loss:  4.8528824\n",
      "Training Step:  637 Loss:  4.849559\n",
      "Training Step:  638 Loss:  4.846234\n",
      "Training Step:  639 Loss:  4.842912\n",
      "Training Step:  640 Loss:  4.839596\n",
      "Training Step:  641 Loss:  4.836282\n",
      "Training Step:  642 Loss:  4.832969\n",
      "Training Step:  643 Loss:  4.8296585\n",
      "Training Step:  644 Loss:  4.8263516\n",
      "Training Step:  645 Loss:  4.8230505\n",
      "Training Step:  646 Loss:  4.819746\n",
      "Training Step:  647 Loss:  4.816448\n",
      "Training Step:  648 Loss:  4.8131514\n",
      "Training Step:  649 Loss:  4.8098564\n",
      "Training Step:  650 Loss:  4.8065658\n",
      "Training Step:  651 Loss:  4.80328\n",
      "Training Step:  652 Loss:  4.7999945\n",
      "Training Step:  653 Loss:  4.7967105\n",
      "Training Step:  654 Loss:  4.7934327\n",
      "Training Step:  655 Loss:  4.790152\n",
      "Training Step:  656 Loss:  4.78688\n",
      "Training Step:  657 Loss:  4.783608\n",
      "Training Step:  658 Loss:  4.780335\n",
      "Training Step:  659 Loss:  4.7770715\n",
      "Training Step:  660 Loss:  4.7738085\n",
      "Training Step:  661 Loss:  4.770547\n",
      "Training Step:  662 Loss:  4.7672834\n",
      "Training Step:  663 Loss:  4.764029\n",
      "Training Step:  664 Loss:  4.760776\n",
      "Training Step:  665 Loss:  4.757527\n",
      "Training Step:  666 Loss:  4.754278\n",
      "Training Step:  667 Loss:  4.75103\n",
      "Training Step:  668 Loss:  4.7477894\n",
      "Training Step:  669 Loss:  4.7445474\n",
      "Training Step:  670 Loss:  4.741311\n",
      "Training Step:  671 Loss:  4.738076\n",
      "Training Step:  672 Loss:  4.7348423\n",
      "Training Step:  673 Loss:  4.731612\n",
      "Training Step:  674 Loss:  4.7283864\n",
      "Training Step:  675 Loss:  4.725161\n",
      "Training Step:  676 Loss:  4.7219377\n",
      "Training Step:  677 Loss:  4.7187195\n",
      "Training Step:  678 Loss:  4.715505\n",
      "Training Step:  679 Loss:  4.7122912\n",
      "Training Step:  680 Loss:  4.709079\n",
      "Training Step:  681 Loss:  4.705868\n",
      "Training Step:  682 Loss:  4.702661\n",
      "Training Step:  683 Loss:  4.699457\n",
      "Training Step:  684 Loss:  4.6962576\n",
      "Training Step:  685 Loss:  4.6930594\n",
      "Training Step:  686 Loss:  4.6898627\n",
      "Training Step:  687 Loss:  4.68667\n",
      "Training Step:  688 Loss:  4.6834803\n",
      "Training Step:  689 Loss:  4.680289\n",
      "Training Step:  690 Loss:  4.6771054\n",
      "Training Step:  691 Loss:  4.6739225\n",
      "Training Step:  692 Loss:  4.6707416\n",
      "Training Step:  693 Loss:  4.667565\n",
      "Training Step:  694 Loss:  4.6643887\n",
      "Training Step:  695 Loss:  4.6612177\n",
      "Training Step:  696 Loss:  4.658045\n",
      "Training Step:  697 Loss:  4.6548805\n",
      "Training Step:  698 Loss:  4.6517134\n",
      "Training Step:  699 Loss:  4.648553\n",
      "Training Step:  700 Loss:  4.6453943\n",
      "Training Step:  701 Loss:  4.6422358\n",
      "Training Step:  702 Loss:  4.639082\n",
      "Training Step:  703 Loss:  4.6359267\n",
      "Training Step:  704 Loss:  4.6327777\n",
      "Training Step:  705 Loss:  4.629631\n",
      "Training Step:  706 Loss:  4.626487\n",
      "Training Step:  707 Loss:  4.623343\n",
      "Training Step:  708 Loss:  4.6202044\n",
      "Training Step:  709 Loss:  4.6170673\n",
      "Training Step:  710 Loss:  4.6139355\n",
      "Training Step:  711 Loss:  4.6108027\n",
      "Training Step:  712 Loss:  4.6076703\n",
      "Training Step:  713 Loss:  4.6045465\n",
      "Training Step:  714 Loss:  4.601424\n",
      "Training Step:  715 Loss:  4.5983033\n",
      "Training Step:  716 Loss:  4.595183\n",
      "Training Step:  717 Loss:  4.592065\n",
      "Training Step:  718 Loss:  4.588952\n",
      "Training Step:  719 Loss:  4.585841\n",
      "Training Step:  720 Loss:  4.5827312\n",
      "Training Step:  721 Loss:  4.579625\n",
      "Training Step:  722 Loss:  4.576522\n",
      "Training Step:  723 Loss:  4.5734196\n",
      "Training Step:  724 Loss:  4.5703216\n",
      "Training Step:  725 Loss:  4.567226\n",
      "Training Step:  726 Loss:  4.5641294\n",
      "Training Step:  727 Loss:  4.56104\n",
      "Training Step:  728 Loss:  4.557952\n",
      "Training Step:  729 Loss:  4.554863\n",
      "Training Step:  730 Loss:  4.55178\n",
      "Training Step:  731 Loss:  4.5487\n",
      "Training Step:  732 Loss:  4.5456214\n",
      "Training Step:  733 Loss:  4.542545\n",
      "Training Step:  734 Loss:  4.5394716\n",
      "Training Step:  735 Loss:  4.536399\n",
      "Training Step:  736 Loss:  4.53333\n",
      "Training Step:  737 Loss:  4.5302634\n",
      "Training Step:  738 Loss:  4.527201\n",
      "Training Step:  739 Loss:  4.5241385\n",
      "Training Step:  740 Loss:  4.5210805\n",
      "Training Step:  741 Loss:  4.518025\n",
      "Training Step:  742 Loss:  4.51497\n",
      "Training Step:  743 Loss:  4.5119195\n",
      "Training Step:  744 Loss:  4.5088696\n",
      "Training Step:  745 Loss:  4.505823\n",
      "Training Step:  746 Loss:  4.502782\n",
      "Training Step:  747 Loss:  4.4997387\n",
      "Training Step:  748 Loss:  4.4966993\n",
      "Training Step:  749 Loss:  4.4936643\n",
      "Training Step:  750 Loss:  4.490628\n",
      "Training Step:  751 Loss:  4.487598\n",
      "Training Step:  752 Loss:  4.4845676\n",
      "Training Step:  753 Loss:  4.481543\n",
      "Training Step:  754 Loss:  4.4785175\n",
      "Training Step:  755 Loss:  4.4754944\n",
      "Training Step:  756 Loss:  4.472477\n",
      "Training Step:  757 Loss:  4.4694595\n",
      "Training Step:  758 Loss:  4.466445\n",
      "Training Step:  759 Loss:  4.463433\n",
      "Training Step:  760 Loss:  4.4604216\n",
      "Training Step:  761 Loss:  4.4574156\n",
      "Training Step:  762 Loss:  4.4544115\n",
      "Training Step:  763 Loss:  4.4514074\n",
      "Training Step:  764 Loss:  4.4484096\n",
      "Training Step:  765 Loss:  4.4454145\n",
      "Training Step:  766 Loss:  4.442418\n",
      "Training Step:  767 Loss:  4.439426\n",
      "Training Step:  768 Loss:  4.436434\n",
      "Training Step:  769 Loss:  4.4334493\n",
      "Training Step:  770 Loss:  4.4304614\n",
      "Training Step:  771 Loss:  4.427478\n",
      "Training Step:  772 Loss:  4.4245\n",
      "Training Step:  773 Loss:  4.4215207\n",
      "Training Step:  774 Loss:  4.4185443\n",
      "Training Step:  775 Loss:  4.4155717\n",
      "Training Step:  776 Loss:  4.412603\n",
      "Training Step:  777 Loss:  4.4096336\n",
      "Training Step:  778 Loss:  4.406666\n",
      "Training Step:  779 Loss:  4.403704\n",
      "Training Step:  780 Loss:  4.4007425\n",
      "Training Step:  781 Loss:  4.3977857\n",
      "Training Step:  782 Loss:  4.3948264\n",
      "Training Step:  783 Loss:  4.3918743\n",
      "Training Step:  784 Loss:  4.3889227\n",
      "Training Step:  785 Loss:  4.385974\n",
      "Training Step:  786 Loss:  4.3830266\n",
      "Training Step:  787 Loss:  4.380084\n",
      "Training Step:  788 Loss:  4.3771396\n",
      "Training Step:  789 Loss:  4.3742037\n",
      "Training Step:  790 Loss:  4.371264\n",
      "Training Step:  791 Loss:  4.3683276\n",
      "Training Step:  792 Loss:  4.3653975\n",
      "Training Step:  793 Loss:  4.3624654\n",
      "Training Step:  794 Loss:  4.35954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  795 Loss:  4.356614\n",
      "Training Step:  796 Loss:  4.353692\n",
      "Training Step:  797 Loss:  4.3507733\n",
      "Training Step:  798 Loss:  4.347852\n",
      "Training Step:  799 Loss:  4.3449354\n",
      "Training Step:  800 Loss:  4.3420224\n",
      "Training Step:  801 Loss:  4.3391128\n",
      "Training Step:  802 Loss:  4.3362045\n",
      "Training Step:  803 Loss:  4.333298\n",
      "Training Step:  804 Loss:  4.330391\n",
      "Training Step:  805 Loss:  4.3274913\n",
      "Training Step:  806 Loss:  4.3245945\n",
      "Training Step:  807 Loss:  4.3216977\n",
      "Training Step:  808 Loss:  4.318801\n",
      "Training Step:  809 Loss:  4.315911\n",
      "Training Step:  810 Loss:  4.313019\n",
      "Training Step:  811 Loss:  4.3101325\n",
      "Training Step:  812 Loss:  4.3072467\n",
      "Training Step:  813 Loss:  4.304365\n",
      "Training Step:  814 Loss:  4.301482\n",
      "Training Step:  815 Loss:  4.2986045\n",
      "Training Step:  816 Loss:  4.29573\n",
      "Training Step:  817 Loss:  4.292857\n",
      "Training Step:  818 Loss:  4.2899857\n",
      "Training Step:  819 Loss:  4.287118\n",
      "Training Step:  820 Loss:  4.284251\n",
      "Training Step:  821 Loss:  4.2813864\n",
      "Training Step:  822 Loss:  4.2785244\n",
      "Training Step:  823 Loss:  4.275663\n",
      "Training Step:  824 Loss:  4.272805\n",
      "Training Step:  825 Loss:  4.2699513\n",
      "Training Step:  826 Loss:  4.2670984\n",
      "Training Step:  827 Loss:  4.2642493\n",
      "Training Step:  828 Loss:  4.2614026\n",
      "Training Step:  829 Loss:  4.258556\n",
      "Training Step:  830 Loss:  4.255713\n",
      "Training Step:  831 Loss:  4.2528725\n",
      "Training Step:  832 Loss:  4.2500315\n",
      "Training Step:  833 Loss:  4.2471967\n",
      "Training Step:  834 Loss:  4.2443633\n",
      "Training Step:  835 Loss:  4.241532\n",
      "Training Step:  836 Loss:  4.238704\n",
      "Training Step:  837 Loss:  4.2358756\n",
      "Training Step:  838 Loss:  4.2330513\n",
      "Training Step:  839 Loss:  4.2302265\n",
      "Training Step:  840 Loss:  4.2274065\n",
      "Training Step:  841 Loss:  4.2245893\n",
      "Training Step:  842 Loss:  4.2217736\n",
      "Training Step:  843 Loss:  4.2189584\n",
      "Training Step:  844 Loss:  4.216148\n",
      "Training Step:  845 Loss:  4.2133403\n",
      "Training Step:  846 Loss:  4.210533\n",
      "Training Step:  847 Loss:  4.207729\n",
      "Training Step:  848 Loss:  4.2049284\n",
      "Training Step:  849 Loss:  4.202127\n",
      "Training Step:  850 Loss:  4.1993284\n",
      "Training Step:  851 Loss:  4.196535\n",
      "Training Step:  852 Loss:  4.1937423\n",
      "Training Step:  853 Loss:  4.1909494\n",
      "Training Step:  854 Loss:  4.1881614\n",
      "Training Step:  855 Loss:  4.1853766\n",
      "Training Step:  856 Loss:  4.182591\n",
      "Training Step:  857 Loss:  4.1798096\n",
      "Training Step:  858 Loss:  4.1770325\n",
      "Training Step:  859 Loss:  4.1742535\n",
      "Training Step:  860 Loss:  4.1714797\n",
      "Training Step:  861 Loss:  4.168707\n",
      "Training Step:  862 Loss:  4.1659365\n",
      "Training Step:  863 Loss:  4.1631656\n",
      "Training Step:  864 Loss:  4.1604023\n",
      "Training Step:  865 Loss:  4.1576395\n",
      "Training Step:  866 Loss:  4.1548767\n",
      "Training Step:  867 Loss:  4.152118\n",
      "Training Step:  868 Loss:  4.1493626\n",
      "Training Step:  869 Loss:  4.1466055\n",
      "Training Step:  870 Loss:  4.143854\n",
      "Training Step:  871 Loss:  4.1411014\n",
      "Training Step:  872 Loss:  4.138355\n",
      "Training Step:  873 Loss:  4.1356096\n",
      "Training Step:  874 Loss:  4.132865\n",
      "Training Step:  875 Loss:  4.130125\n",
      "Training Step:  876 Loss:  4.127387\n",
      "Training Step:  877 Loss:  4.124649\n",
      "Training Step:  878 Loss:  4.121914\n",
      "Training Step:  879 Loss:  4.119182\n",
      "Training Step:  880 Loss:  4.1164517\n",
      "Training Step:  881 Loss:  4.1137247\n",
      "Training Step:  882 Loss:  4.110996\n",
      "Training Step:  883 Loss:  4.108274\n",
      "Training Step:  884 Loss:  4.1055527\n",
      "Training Step:  885 Loss:  4.102833\n",
      "Training Step:  886 Loss:  4.1001177\n",
      "Training Step:  887 Loss:  4.097403\n",
      "Training Step:  888 Loss:  4.094689\n",
      "Training Step:  889 Loss:  4.091979\n",
      "Training Step:  890 Loss:  4.089271\n",
      "Training Step:  891 Loss:  4.0865645\n",
      "Training Step:  892 Loss:  4.08386\n",
      "Training Step:  893 Loss:  4.081158\n",
      "Training Step:  894 Loss:  4.0784583\n",
      "Training Step:  895 Loss:  4.075762\n",
      "Training Step:  896 Loss:  4.0730677\n",
      "Training Step:  897 Loss:  4.0703745\n",
      "Training Step:  898 Loss:  4.0676837\n",
      "Training Step:  899 Loss:  4.0649962\n",
      "Training Step:  900 Loss:  4.0623093\n",
      "Training Step:  901 Loss:  4.0596266\n",
      "Training Step:  902 Loss:  4.0569425\n",
      "Training Step:  903 Loss:  4.054265\n",
      "Training Step:  904 Loss:  4.051588\n",
      "Training Step:  905 Loss:  4.0489097\n",
      "Training Step:  906 Loss:  4.046238\n",
      "Training Step:  907 Loss:  4.0435653\n",
      "Training Step:  908 Loss:  4.0408964\n",
      "Training Step:  909 Loss:  4.0382304\n",
      "Training Step:  910 Loss:  4.0355673\n",
      "Training Step:  911 Loss:  4.032903\n",
      "Training Step:  912 Loss:  4.030245\n",
      "Training Step:  913 Loss:  4.027584\n",
      "Training Step:  914 Loss:  4.0249295\n",
      "Training Step:  915 Loss:  4.0222783\n",
      "Training Step:  916 Loss:  4.0196257\n",
      "Training Step:  917 Loss:  4.016974\n",
      "Training Step:  918 Loss:  4.0143275\n",
      "Training Step:  919 Loss:  4.011684\n",
      "Training Step:  920 Loss:  4.009041\n",
      "Training Step:  921 Loss:  4.0063987\n",
      "Training Step:  922 Loss:  4.0037603\n",
      "Training Step:  923 Loss:  4.0011215\n",
      "Training Step:  924 Loss:  3.998489\n",
      "Training Step:  925 Loss:  3.9958546\n",
      "Training Step:  926 Loss:  3.9932263\n",
      "Training Step:  927 Loss:  3.990597\n",
      "Training Step:  928 Loss:  3.9879723\n",
      "Training Step:  929 Loss:  3.9853473\n",
      "Training Step:  930 Loss:  3.982727\n",
      "Training Step:  931 Loss:  3.9801083\n",
      "Training Step:  932 Loss:  3.9774888\n",
      "Training Step:  933 Loss:  3.974875\n",
      "Training Step:  934 Loss:  3.9722607\n",
      "Training Step:  935 Loss:  3.9696498\n",
      "Training Step:  936 Loss:  3.9670434\n",
      "Training Step:  937 Loss:  3.964434\n",
      "Training Step:  938 Loss:  3.9618316\n",
      "Training Step:  939 Loss:  3.9592273\n",
      "Training Step:  940 Loss:  3.9566257\n",
      "Training Step:  941 Loss:  3.9540262\n",
      "Training Step:  942 Loss:  3.9514318\n",
      "Training Step:  943 Loss:  3.9488387\n",
      "Training Step:  944 Loss:  3.9462452\n",
      "Training Step:  945 Loss:  3.9436564\n",
      "Training Step:  946 Loss:  3.9410672\n",
      "Training Step:  947 Loss:  3.9384823\n",
      "Training Step:  948 Loss:  3.9358969\n",
      "Training Step:  949 Loss:  3.933314\n",
      "Training Step:  950 Loss:  3.930737\n",
      "Training Step:  951 Loss:  3.9281585\n",
      "Training Step:  952 Loss:  3.9255805\n",
      "Training Step:  953 Loss:  3.9230113\n",
      "Training Step:  954 Loss:  3.9204383\n",
      "Training Step:  955 Loss:  3.917868\n",
      "Training Step:  956 Loss:  3.9153006\n",
      "Training Step:  957 Loss:  3.912738\n",
      "Training Step:  958 Loss:  3.9101741\n",
      "Training Step:  959 Loss:  3.907613\n",
      "Training Step:  960 Loss:  3.9050546\n",
      "Training Step:  961 Loss:  3.9025002\n",
      "Training Step:  962 Loss:  3.8999443\n",
      "Training Step:  963 Loss:  3.8973925\n",
      "Training Step:  964 Loss:  3.8948402\n",
      "Training Step:  965 Loss:  3.892293\n",
      "Training Step:  966 Loss:  3.8897464\n",
      "Training Step:  967 Loss:  3.8872044\n",
      "Training Step:  968 Loss:  3.88466\n",
      "Training Step:  969 Loss:  3.8821223\n",
      "Training Step:  970 Loss:  3.8795798\n",
      "Training Step:  971 Loss:  3.8770478\n",
      "Training Step:  972 Loss:  3.8745124\n",
      "Training Step:  973 Loss:  3.8719819\n",
      "Training Step:  974 Loss:  3.8694499\n",
      "Training Step:  975 Loss:  3.8669243\n",
      "Training Step:  976 Loss:  3.864397\n",
      "Training Step:  977 Loss:  3.8618717\n",
      "Training Step:  978 Loss:  3.8593524\n",
      "Training Step:  979 Loss:  3.8568332\n",
      "Training Step:  980 Loss:  3.854316\n",
      "Training Step:  981 Loss:  3.851801\n",
      "Training Step:  982 Loss:  3.849288\n",
      "Training Step:  983 Loss:  3.846775\n",
      "Training Step:  984 Loss:  3.844266\n",
      "Training Step:  985 Loss:  3.8417616\n",
      "Training Step:  986 Loss:  3.8392546\n",
      "Training Step:  987 Loss:  3.8367503\n",
      "Training Step:  988 Loss:  3.83425\n",
      "Training Step:  989 Loss:  3.8317487\n",
      "Training Step:  990 Loss:  3.829253\n",
      "Training Step:  991 Loss:  3.826761\n",
      "Training Step:  992 Loss:  3.8242655\n",
      "Training Step:  993 Loss:  3.8217747\n",
      "Training Step:  994 Loss:  3.819284\n",
      "Training Step:  995 Loss:  3.8167977\n",
      "Training Step:  996 Loss:  3.8143137\n",
      "Training Step:  997 Loss:  3.811831\n",
      "Training Step:  998 Loss:  3.8093514\n",
      "Training Step:  999 Loss:  3.80687\n",
      "Training Step:  1000 Loss:  3.8043935\n",
      "Training Step:  1001 Loss:  3.801921\n",
      "Training Step:  1002 Loss:  3.7994459\n",
      "Training Step:  1003 Loss:  3.7969754\n",
      "Training Step:  1004 Loss:  3.7945087\n",
      "Training Step:  1005 Loss:  3.7920375\n",
      "Training Step:  1006 Loss:  3.7895732\n",
      "Training Step:  1007 Loss:  3.7871096\n",
      "Training Step:  1008 Loss:  3.7846494\n",
      "Training Step:  1009 Loss:  3.7821922\n",
      "Training Step:  1010 Loss:  3.779735\n",
      "Training Step:  1011 Loss:  3.7772799\n",
      "Training Step:  1012 Loss:  3.774827\n",
      "Training Step:  1013 Loss:  3.772376\n",
      "Training Step:  1014 Loss:  3.7699249\n",
      "Training Step:  1015 Loss:  3.7674794\n",
      "Training Step:  1016 Loss:  3.7650378\n",
      "Training Step:  1017 Loss:  3.7625933\n",
      "Training Step:  1018 Loss:  3.7601511\n",
      "Training Step:  1019 Loss:  3.7577114\n",
      "Training Step:  1020 Loss:  3.7552752\n",
      "Training Step:  1021 Loss:  3.7528405\n",
      "Training Step:  1022 Loss:  3.7504075\n",
      "Training Step:  1023 Loss:  3.7479753\n",
      "Training Step:  1024 Loss:  3.745544\n",
      "Training Step:  1025 Loss:  3.743118\n",
      "Training Step:  1026 Loss:  3.740694\n",
      "Training Step:  1027 Loss:  3.7382686\n",
      "Training Step:  1028 Loss:  3.7358484\n",
      "Training Step:  1029 Loss:  3.73343\n",
      "Training Step:  1030 Loss:  3.7310114\n",
      "Training Step:  1031 Loss:  3.7285957\n",
      "Training Step:  1032 Loss:  3.7261848\n",
      "Training Step:  1033 Loss:  3.7237716\n",
      "Training Step:  1034 Loss:  3.721362\n",
      "Training Step:  1035 Loss:  3.718954\n",
      "Training Step:  1036 Loss:  3.7165487\n",
      "Training Step:  1037 Loss:  3.7141452\n",
      "Training Step:  1038 Loss:  3.7117436\n",
      "Training Step:  1039 Loss:  3.709343\n",
      "Training Step:  1040 Loss:  3.7069457\n",
      "Training Step:  1041 Loss:  3.7045503\n",
      "Training Step:  1042 Loss:  3.7021556\n",
      "Training Step:  1043 Loss:  3.6997645\n",
      "Training Step:  1044 Loss:  3.6973739\n",
      "Training Step:  1045 Loss:  3.694986\n",
      "Training Step:  1046 Loss:  3.6925988\n",
      "Training Step:  1047 Loss:  3.690215\n",
      "Training Step:  1048 Loss:  3.687833\n",
      "Training Step:  1049 Loss:  3.685452\n",
      "Training Step:  1050 Loss:  3.683074\n",
      "Training Step:  1051 Loss:  3.680695\n",
      "Training Step:  1052 Loss:  3.6783226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  1053 Loss:  3.6759489\n",
      "Training Step:  1054 Loss:  3.6735783\n",
      "Training Step:  1055 Loss:  3.6712089\n",
      "Training Step:  1056 Loss:  3.668842\n",
      "Training Step:  1057 Loss:  3.6664782\n",
      "Training Step:  1058 Loss:  3.6641147\n",
      "Training Step:  1059 Loss:  3.6617544\n",
      "Training Step:  1060 Loss:  3.6593924\n",
      "Training Step:  1061 Loss:  3.6570358\n",
      "Training Step:  1062 Loss:  3.6546803\n",
      "Training Step:  1063 Loss:  3.652327\n",
      "Training Step:  1064 Loss:  3.649977\n",
      "Training Step:  1065 Loss:  3.6476264\n",
      "Training Step:  1066 Loss:  3.6452794\n",
      "Training Step:  1067 Loss:  3.6429324\n",
      "Training Step:  1068 Loss:  3.6405897\n",
      "Training Step:  1069 Loss:  3.6382446\n",
      "Training Step:  1070 Loss:  3.6359065\n",
      "Training Step:  1071 Loss:  3.6335673\n",
      "Training Step:  1072 Loss:  3.6312306\n",
      "Training Step:  1073 Loss:  3.6288955\n",
      "Training Step:  1074 Loss:  3.6265638\n",
      "Training Step:  1075 Loss:  3.6242342\n",
      "Training Step:  1076 Loss:  3.6219044\n",
      "Training Step:  1077 Loss:  3.6195767\n",
      "Training Step:  1078 Loss:  3.617252\n",
      "Training Step:  1079 Loss:  3.614928\n",
      "Training Step:  1080 Loss:  3.6126075\n",
      "Training Step:  1081 Loss:  3.6102874\n",
      "Training Step:  1082 Loss:  3.6079698\n",
      "Training Step:  1083 Loss:  3.6056542\n",
      "Training Step:  1084 Loss:  3.6033409\n",
      "Training Step:  1085 Loss:  3.6010284\n",
      "Training Step:  1086 Loss:  3.5987182\n",
      "Training Step:  1087 Loss:  3.5964088\n",
      "Training Step:  1088 Loss:  3.5941043\n",
      "Training Step:  1089 Loss:  3.5917993\n",
      "Training Step:  1090 Loss:  3.5894952\n",
      "Training Step:  1091 Loss:  3.5871947\n",
      "Training Step:  1092 Loss:  3.584898\n",
      "Training Step:  1093 Loss:  3.5825977\n",
      "Training Step:  1094 Loss:  3.5803041\n",
      "Training Step:  1095 Loss:  3.5780098\n",
      "Training Step:  1096 Loss:  3.5757208\n",
      "Training Step:  1097 Loss:  3.573431\n",
      "Training Step:  1098 Loss:  3.5711422\n",
      "Training Step:  1099 Loss:  3.5688558\n",
      "Training Step:  1100 Loss:  3.5665722\n",
      "Training Step:  1101 Loss:  3.56429\n",
      "Training Step:  1102 Loss:  3.5620112\n",
      "Training Step:  1103 Loss:  3.5597322\n",
      "Training Step:  1104 Loss:  3.5574532\n",
      "Training Step:  1105 Loss:  3.5551825\n",
      "Training Step:  1106 Loss:  3.5529068\n",
      "Training Step:  1107 Loss:  3.5506375\n",
      "Training Step:  1108 Loss:  3.5483665\n",
      "Training Step:  1109 Loss:  3.5460992\n",
      "Training Step:  1110 Loss:  3.5438344\n",
      "Training Step:  1111 Loss:  3.5415702\n",
      "Training Step:  1112 Loss:  3.5393093\n",
      "Training Step:  1113 Loss:  3.5370474\n",
      "Training Step:  1114 Loss:  3.5347898\n",
      "Training Step:  1115 Loss:  3.5325327\n",
      "Training Step:  1116 Loss:  3.5302792\n",
      "Training Step:  1117 Loss:  3.5280256\n",
      "Training Step:  1118 Loss:  3.5257752\n",
      "Training Step:  1119 Loss:  3.5235252\n",
      "Training Step:  1120 Loss:  3.5212798\n",
      "Training Step:  1121 Loss:  3.5190344\n",
      "Training Step:  1122 Loss:  3.5167909\n",
      "Training Step:  1123 Loss:  3.5145488\n",
      "Training Step:  1124 Loss:  3.5123081\n",
      "Training Step:  1125 Loss:  3.510067\n",
      "Training Step:  1126 Loss:  3.5078347\n",
      "Training Step:  1127 Loss:  3.5055985\n",
      "Training Step:  1128 Loss:  3.5033662\n",
      "Training Step:  1129 Loss:  3.5011349\n",
      "Training Step:  1130 Loss:  3.4989066\n",
      "Training Step:  1131 Loss:  3.4966767\n",
      "Training Step:  1132 Loss:  3.4944518\n",
      "Training Step:  1133 Loss:  3.492231\n",
      "Training Step:  1134 Loss:  3.4900043\n",
      "Training Step:  1135 Loss:  3.4877875\n",
      "Training Step:  1136 Loss:  3.485568\n",
      "Training Step:  1137 Loss:  3.4833531\n",
      "Training Step:  1138 Loss:  3.4811382\n",
      "Training Step:  1139 Loss:  3.478926\n",
      "Training Step:  1140 Loss:  3.4767127\n",
      "Training Step:  1141 Loss:  3.4745035\n",
      "Training Step:  1142 Loss:  3.472296\n",
      "Training Step:  1143 Loss:  3.4700906\n",
      "Training Step:  1144 Loss:  3.467886\n",
      "Training Step:  1145 Loss:  3.4656847\n",
      "Training Step:  1146 Loss:  3.4634833\n",
      "Training Step:  1147 Loss:  3.461286\n",
      "Training Step:  1148 Loss:  3.4590874\n",
      "Training Step:  1149 Loss:  3.4568942\n",
      "Training Step:  1150 Loss:  3.4547002\n",
      "Training Step:  1151 Loss:  3.4525082\n",
      "Training Step:  1152 Loss:  3.4503171\n",
      "Training Step:  1153 Loss:  3.4481306\n",
      "Training Step:  1154 Loss:  3.4459443\n",
      "Training Step:  1155 Loss:  3.4437602\n",
      "Training Step:  1156 Loss:  3.4415789\n",
      "Training Step:  1157 Loss:  3.4393966\n",
      "Training Step:  1158 Loss:  3.4372177\n",
      "Training Step:  1159 Loss:  3.4350395\n",
      "Training Step:  1160 Loss:  3.4328625\n",
      "Training Step:  1161 Loss:  3.4306903\n",
      "Training Step:  1162 Loss:  3.4285183\n",
      "Training Step:  1163 Loss:  3.426347\n",
      "Training Step:  1164 Loss:  3.4241772\n",
      "Training Step:  1165 Loss:  3.422013\n",
      "Training Step:  1166 Loss:  3.419847\n",
      "Training Step:  1167 Loss:  3.417684\n",
      "Training Step:  1168 Loss:  3.415521\n",
      "Training Step:  1169 Loss:  3.413362\n",
      "Training Step:  1170 Loss:  3.4112034\n",
      "Training Step:  1171 Loss:  3.4090476\n",
      "Training Step:  1172 Loss:  3.4068937\n",
      "Training Step:  1173 Loss:  3.4047384\n",
      "Training Step:  1174 Loss:  3.4025877\n",
      "Training Step:  1175 Loss:  3.4004407\n",
      "Training Step:  1176 Loss:  3.3982935\n",
      "Training Step:  1177 Loss:  3.3961484\n",
      "Training Step:  1178 Loss:  3.394002\n",
      "Training Step:  1179 Loss:  3.391861\n",
      "Training Step:  1180 Loss:  3.38972\n",
      "Training Step:  1181 Loss:  3.3875813\n",
      "Training Step:  1182 Loss:  3.3854425\n",
      "Training Step:  1183 Loss:  3.383307\n",
      "Training Step:  1184 Loss:  3.3811731\n",
      "Training Step:  1185 Loss:  3.3790421\n",
      "Training Step:  1186 Loss:  3.3769104\n",
      "Training Step:  1187 Loss:  3.374784\n",
      "Training Step:  1188 Loss:  3.3726559\n",
      "Training Step:  1189 Loss:  3.3705332\n",
      "Training Step:  1190 Loss:  3.3684082\n",
      "Training Step:  1191 Loss:  3.3662868\n",
      "Training Step:  1192 Loss:  3.3641653\n",
      "Training Step:  1193 Loss:  3.362048\n",
      "Training Step:  1194 Loss:  3.3599324\n",
      "Training Step:  1195 Loss:  3.3578162\n",
      "Training Step:  1196 Loss:  3.3557034\n",
      "Training Step:  1197 Loss:  3.3535929\n",
      "Training Step:  1198 Loss:  3.3514829\n",
      "Training Step:  1199 Loss:  3.349375\n",
      "Training Step:  1200 Loss:  3.3472686\n",
      "Training Step:  1201 Loss:  3.3451653\n",
      "Training Step:  1202 Loss:  3.343061\n",
      "Training Step:  1203 Loss:  3.340961\n",
      "Training Step:  1204 Loss:  3.3388622\n",
      "Training Step:  1205 Loss:  3.3367622\n",
      "Training Step:  1206 Loss:  3.3346677\n",
      "Training Step:  1207 Loss:  3.3325727\n",
      "Training Step:  1208 Loss:  3.3304808\n",
      "Training Step:  1209 Loss:  3.3283887\n",
      "Training Step:  1210 Loss:  3.326299\n",
      "Training Step:  1211 Loss:  3.3242126\n",
      "Training Step:  1212 Loss:  3.3221257\n",
      "Training Step:  1213 Loss:  3.3200438\n",
      "Training Step:  1214 Loss:  3.317959\n",
      "Training Step:  1215 Loss:  3.315878\n",
      "Training Step:  1216 Loss:  3.3137999\n",
      "Training Step:  1217 Loss:  3.3117208\n",
      "Training Step:  1218 Loss:  3.3096473\n",
      "Training Step:  1219 Loss:  3.3075726\n",
      "Training Step:  1220 Loss:  3.305499\n",
      "Training Step:  1221 Loss:  3.3034282\n",
      "Training Step:  1222 Loss:  3.3013592\n",
      "Training Step:  1223 Loss:  3.2992938\n",
      "Training Step:  1224 Loss:  3.2972264\n",
      "Training Step:  1225 Loss:  3.2951624\n",
      "Training Step:  1226 Loss:  3.293102\n",
      "Training Step:  1227 Loss:  3.29104\n",
      "Training Step:  1228 Loss:  3.2889814\n",
      "Training Step:  1229 Loss:  3.2869244\n",
      "Training Step:  1230 Loss:  3.2848663\n",
      "Training Step:  1231 Loss:  3.282814\n",
      "Training Step:  1232 Loss:  3.2807608\n",
      "Training Step:  1233 Loss:  3.2787113\n",
      "Training Step:  1234 Loss:  3.276661\n",
      "Training Step:  1235 Loss:  3.2746148\n",
      "Training Step:  1236 Loss:  3.2725692\n",
      "Training Step:  1237 Loss:  3.2705243\n",
      "Training Step:  1238 Loss:  3.2684817\n",
      "Training Step:  1239 Loss:  3.2664413\n",
      "Training Step:  1240 Loss:  3.2644033\n",
      "Training Step:  1241 Loss:  3.262365\n",
      "Training Step:  1242 Loss:  3.2603283\n",
      "Training Step:  1243 Loss:  3.2582948\n",
      "Training Step:  1244 Loss:  3.2562637\n",
      "Training Step:  1245 Loss:  3.2542298\n",
      "Training Step:  1246 Loss:  3.2522013\n",
      "Training Step:  1247 Loss:  3.2501745\n",
      "Training Step:  1248 Loss:  3.2481494\n",
      "Training Step:  1249 Loss:  3.2461252\n",
      "Training Step:  1250 Loss:  3.2441013\n",
      "Training Step:  1251 Loss:  3.2420819\n",
      "Training Step:  1252 Loss:  3.2400627\n",
      "Training Step:  1253 Loss:  3.2380443\n",
      "Training Step:  1254 Loss:  3.2360277\n",
      "Training Step:  1255 Loss:  3.2340138\n",
      "Training Step:  1256 Loss:  3.2320018\n",
      "Training Step:  1257 Loss:  3.22999\n",
      "Training Step:  1258 Loss:  3.227981\n",
      "Training Step:  1259 Loss:  3.225973\n",
      "Training Step:  1260 Loss:  3.223966\n",
      "Training Step:  1261 Loss:  3.2219615\n",
      "Training Step:  1262 Loss:  3.219957\n",
      "Training Step:  1263 Loss:  3.2179554\n",
      "Training Step:  1264 Loss:  3.2159586\n",
      "Training Step:  1265 Loss:  3.2139585\n",
      "Training Step:  1266 Loss:  3.2119608\n",
      "Training Step:  1267 Loss:  3.2099671\n",
      "Training Step:  1268 Loss:  3.2079728\n",
      "Training Step:  1269 Loss:  3.2059798\n",
      "Training Step:  1270 Loss:  3.2039924\n",
      "Training Step:  1271 Loss:  3.202003\n",
      "Training Step:  1272 Loss:  3.200016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  1273 Loss:  3.198031\n",
      "Training Step:  1274 Loss:  3.1960468\n",
      "Training Step:  1275 Loss:  3.1940625\n",
      "Training Step:  1276 Loss:  3.1920824\n",
      "Training Step:  1277 Loss:  3.1901045\n",
      "Training Step:  1278 Loss:  3.188129\n",
      "Training Step:  1279 Loss:  3.1861537\n",
      "Training Step:  1280 Loss:  3.1841795\n",
      "Training Step:  1281 Loss:  3.1822062\n",
      "Training Step:  1282 Loss:  3.180233\n",
      "Training Step:  1283 Loss:  3.178263\n",
      "Training Step:  1284 Loss:  3.1762967\n",
      "Training Step:  1285 Loss:  3.174329\n",
      "Training Step:  1286 Loss:  3.1723664\n",
      "Training Step:  1287 Loss:  3.1704044\n",
      "Training Step:  1288 Loss:  3.1684432\n",
      "Training Step:  1289 Loss:  3.1664813\n",
      "Training Step:  1290 Loss:  3.1645255\n",
      "Training Step:  1291 Loss:  3.1625662\n",
      "Training Step:  1292 Loss:  3.160613\n",
      "Training Step:  1293 Loss:  3.1586576\n",
      "Training Step:  1294 Loss:  3.1567075\n",
      "Training Step:  1295 Loss:  3.154757\n",
      "Training Step:  1296 Loss:  3.1528087\n",
      "Training Step:  1297 Loss:  3.1508603\n",
      "Training Step:  1298 Loss:  3.148915\n",
      "Training Step:  1299 Loss:  3.146972\n",
      "Training Step:  1300 Loss:  3.1450305\n",
      "Training Step:  1301 Loss:  3.1430888\n",
      "Training Step:  1302 Loss:  3.1411493\n",
      "Training Step:  1303 Loss:  3.1392121\n",
      "Training Step:  1304 Loss:  3.1372752\n",
      "Training Step:  1305 Loss:  3.1353412\n",
      "Training Step:  1306 Loss:  3.133408\n",
      "Training Step:  1307 Loss:  3.1314778\n",
      "Training Step:  1308 Loss:  3.1295455\n",
      "Training Step:  1309 Loss:  3.1276188\n",
      "Training Step:  1310 Loss:  3.125692\n",
      "Training Step:  1311 Loss:  3.1237664\n",
      "Training Step:  1312 Loss:  3.1218443\n",
      "Training Step:  1313 Loss:  3.1199193\n",
      "Training Step:  1314 Loss:  3.1180005\n",
      "Training Step:  1315 Loss:  3.116082\n",
      "Training Step:  1316 Loss:  3.114163\n",
      "Training Step:  1317 Loss:  3.1122475\n",
      "Training Step:  1318 Loss:  3.110333\n",
      "Training Step:  1319 Loss:  3.1084208\n",
      "Training Step:  1320 Loss:  3.1065097\n",
      "Training Step:  1321 Loss:  3.1045995\n",
      "Training Step:  1322 Loss:  3.102691\n",
      "Training Step:  1323 Loss:  3.1007862\n",
      "Training Step:  1324 Loss:  3.0988803\n",
      "Training Step:  1325 Loss:  3.0969775\n",
      "Training Step:  1326 Loss:  3.0950773\n",
      "Training Step:  1327 Loss:  3.0931747\n",
      "Training Step:  1328 Loss:  3.091276\n",
      "Training Step:  1329 Loss:  3.0893788\n",
      "Training Step:  1330 Loss:  3.0874808\n",
      "Training Step:  1331 Loss:  3.0855875\n",
      "Training Step:  1332 Loss:  3.0836954\n",
      "Training Step:  1333 Loss:  3.0818062\n",
      "Training Step:  1334 Loss:  3.0799153\n",
      "Training Step:  1335 Loss:  3.0780268\n",
      "Training Step:  1336 Loss:  3.0761409\n",
      "Training Step:  1337 Loss:  3.0742545\n",
      "Training Step:  1338 Loss:  3.072372\n",
      "Training Step:  1339 Loss:  3.0704894\n",
      "Training Step:  1340 Loss:  3.0686085\n",
      "Training Step:  1341 Loss:  3.066731\n",
      "Training Step:  1342 Loss:  3.0648527\n",
      "Training Step:  1343 Loss:  3.0629766\n",
      "Training Step:  1344 Loss:  3.0611022\n",
      "Training Step:  1345 Loss:  3.0592294\n",
      "Training Step:  1346 Loss:  3.057359\n",
      "Training Step:  1347 Loss:  3.0554879\n",
      "Training Step:  1348 Loss:  3.0536184\n",
      "Training Step:  1349 Loss:  3.051753\n",
      "Training Step:  1350 Loss:  3.0498872\n",
      "Training Step:  1351 Loss:  3.0480227\n",
      "Training Step:  1352 Loss:  3.0461626\n",
      "Training Step:  1353 Loss:  3.044301\n",
      "Training Step:  1354 Loss:  3.042441\n",
      "Training Step:  1355 Loss:  3.0405836\n",
      "Training Step:  1356 Loss:  3.038728\n",
      "Training Step:  1357 Loss:  3.0368726\n",
      "Training Step:  1358 Loss:  3.0350184\n",
      "Training Step:  1359 Loss:  3.033167\n",
      "Training Step:  1360 Loss:  3.0313172\n",
      "Training Step:  1361 Loss:  3.02947\n",
      "Training Step:  1362 Loss:  3.0276217\n",
      "Training Step:  1363 Loss:  3.025775\n",
      "Training Step:  1364 Loss:  3.02393\n",
      "Training Step:  1365 Loss:  3.0220876\n",
      "Training Step:  1366 Loss:  3.0202456\n",
      "Training Step:  1367 Loss:  3.0184073\n",
      "Training Step:  1368 Loss:  3.0165675\n",
      "Training Step:  1369 Loss:  3.0147305\n",
      "Training Step:  1370 Loss:  3.0128975\n",
      "Training Step:  1371 Loss:  3.0110607\n",
      "Training Step:  1372 Loss:  3.0092294\n",
      "Training Step:  1373 Loss:  3.007398\n",
      "Training Step:  1374 Loss:  3.005569\n",
      "Training Step:  1375 Loss:  3.0037413\n",
      "Training Step:  1376 Loss:  3.001915\n",
      "Training Step:  1377 Loss:  3.0000892\n",
      "Training Step:  1378 Loss:  2.9982648\n",
      "Training Step:  1379 Loss:  2.9964442\n",
      "Training Step:  1380 Loss:  2.9946234\n",
      "Training Step:  1381 Loss:  2.9928048\n",
      "Training Step:  1382 Loss:  2.9909863\n",
      "Training Step:  1383 Loss:  2.9891684\n",
      "Training Step:  1384 Loss:  2.9873543\n",
      "Training Step:  1385 Loss:  2.9855409\n",
      "Training Step:  1386 Loss:  2.9837298\n",
      "Training Step:  1387 Loss:  2.9819217\n",
      "Training Step:  1388 Loss:  2.9801097\n",
      "Training Step:  1389 Loss:  2.9783032\n",
      "Training Step:  1390 Loss:  2.9764974\n",
      "Training Step:  1391 Loss:  2.974693\n",
      "Training Step:  1392 Loss:  2.9728906\n",
      "Training Step:  1393 Loss:  2.9710884\n",
      "Training Step:  1394 Loss:  2.9692888\n",
      "Training Step:  1395 Loss:  2.9674911\n",
      "Training Step:  1396 Loss:  2.9656918\n",
      "Training Step:  1397 Loss:  2.9638958\n",
      "Training Step:  1398 Loss:  2.9621024\n",
      "Training Step:  1399 Loss:  2.9603107\n",
      "Training Step:  1400 Loss:  2.958518\n",
      "Training Step:  1401 Loss:  2.9567282\n",
      "Training Step:  1402 Loss:  2.9549417\n",
      "Training Step:  1403 Loss:  2.9531527\n",
      "Training Step:  1404 Loss:  2.951369\n",
      "Training Step:  1405 Loss:  2.949583\n",
      "Training Step:  1406 Loss:  2.9478023\n",
      "Training Step:  1407 Loss:  2.9460194\n",
      "Training Step:  1408 Loss:  2.94424\n",
      "Training Step:  1409 Loss:  2.9424617\n",
      "Training Step:  1410 Loss:  2.940685\n",
      "Training Step:  1411 Loss:  2.938908\n",
      "Training Step:  1412 Loss:  2.9371352\n",
      "Training Step:  1413 Loss:  2.9353628\n",
      "Training Step:  1414 Loss:  2.933591\n",
      "Training Step:  1415 Loss:  2.931821\n",
      "Training Step:  1416 Loss:  2.9300528\n",
      "Training Step:  1417 Loss:  2.928286\n",
      "Training Step:  1418 Loss:  2.9265184\n",
      "Training Step:  1419 Loss:  2.9247558\n",
      "Training Step:  1420 Loss:  2.9229932\n",
      "Training Step:  1421 Loss:  2.9212332\n",
      "Training Step:  1422 Loss:  2.9194713\n",
      "Training Step:  1423 Loss:  2.9177139\n",
      "Training Step:  1424 Loss:  2.9159575\n",
      "Training Step:  1425 Loss:  2.9142025\n",
      "Training Step:  1426 Loss:  2.912447\n",
      "Training Step:  1427 Loss:  2.9106941\n",
      "Training Step:  1428 Loss:  2.908942\n",
      "Training Step:  1429 Loss:  2.9071941\n",
      "Training Step:  1430 Loss:  2.9054446\n",
      "Training Step:  1431 Loss:  2.903699\n",
      "Training Step:  1432 Loss:  2.9019537\n",
      "Training Step:  1433 Loss:  2.900209\n",
      "Training Step:  1434 Loss:  2.898466\n",
      "Training Step:  1435 Loss:  2.8967247\n",
      "Training Step:  1436 Loss:  2.8949842\n",
      "Training Step:  1437 Loss:  2.8932445\n",
      "Training Step:  1438 Loss:  2.8915086\n",
      "Training Step:  1439 Loss:  2.8897727\n",
      "Training Step:  1440 Loss:  2.8880386\n",
      "Training Step:  1441 Loss:  2.8863065\n",
      "Training Step:  1442 Loss:  2.8845744\n",
      "Training Step:  1443 Loss:  2.882844\n",
      "Training Step:  1444 Loss:  2.8811154\n",
      "Training Step:  1445 Loss:  2.8793867\n",
      "Training Step:  1446 Loss:  2.8776608\n",
      "Training Step:  1447 Loss:  2.8759372\n",
      "Training Step:  1448 Loss:  2.874214\n",
      "Training Step:  1449 Loss:  2.8724933\n",
      "Training Step:  1450 Loss:  2.870772\n",
      "Training Step:  1451 Loss:  2.8690543\n",
      "Training Step:  1452 Loss:  2.867335\n",
      "Training Step:  1453 Loss:  2.8656187\n",
      "Training Step:  1454 Loss:  2.863903\n",
      "Training Step:  1455 Loss:  2.8621917\n",
      "Training Step:  1456 Loss:  2.8604784\n",
      "Training Step:  1457 Loss:  2.858768\n",
      "Training Step:  1458 Loss:  2.8570588\n",
      "Training Step:  1459 Loss:  2.8553517\n",
      "Training Step:  1460 Loss:  2.8536444\n",
      "Training Step:  1461 Loss:  2.8519404\n",
      "Training Step:  1462 Loss:  2.8502374\n",
      "Training Step:  1463 Loss:  2.8485327\n",
      "Training Step:  1464 Loss:  2.8468316\n",
      "Training Step:  1465 Loss:  2.845133\n",
      "Training Step:  1466 Loss:  2.8434362\n",
      "Training Step:  1467 Loss:  2.8417375\n",
      "Training Step:  1468 Loss:  2.8400428\n",
      "Training Step:  1469 Loss:  2.8383498\n",
      "Training Step:  1470 Loss:  2.8366542\n",
      "Training Step:  1471 Loss:  2.834963\n",
      "Training Step:  1472 Loss:  2.8332741\n",
      "Training Step:  1473 Loss:  2.831585\n",
      "Training Step:  1474 Loss:  2.8298965\n",
      "Training Step:  1475 Loss:  2.8282118\n",
      "Training Step:  1476 Loss:  2.8265254\n",
      "Training Step:  1477 Loss:  2.824845\n",
      "Training Step:  1478 Loss:  2.8231604\n",
      "Training Step:  1479 Loss:  2.8214827\n",
      "Training Step:  1480 Loss:  2.819802\n",
      "Training Step:  1481 Loss:  2.818124\n",
      "Training Step:  1482 Loss:  2.8164485\n",
      "Training Step:  1483 Loss:  2.814772\n",
      "Training Step:  1484 Loss:  2.8131013\n",
      "Training Step:  1485 Loss:  2.8114264\n",
      "Training Step:  1486 Loss:  2.8097553\n",
      "Training Step:  1487 Loss:  2.8080869\n",
      "Training Step:  1488 Loss:  2.8064187\n",
      "Training Step:  1489 Loss:  2.8047504\n",
      "Training Step:  1490 Loss:  2.8030841\n",
      "Training Step:  1491 Loss:  2.8014183\n",
      "Training Step:  1492 Loss:  2.7997587\n",
      "Training Step:  1493 Loss:  2.7980976\n",
      "Training Step:  1494 Loss:  2.7964358\n",
      "Training Step:  1495 Loss:  2.7947779\n",
      "Training Step:  1496 Loss:  2.7931194\n",
      "Training Step:  1497 Loss:  2.7914636\n",
      "Training Step:  1498 Loss:  2.7898088\n",
      "Training Step:  1499 Loss:  2.7881541\n",
      "Training Step:  1500 Loss:  2.7865014\n",
      "Training Step:  1501 Loss:  2.7848516\n",
      "Training Step:  1502 Loss:  2.7832012\n",
      "Training Step:  1503 Loss:  2.781556\n",
      "Training Step:  1504 Loss:  2.779907\n",
      "Training Step:  1505 Loss:  2.7782626\n",
      "Training Step:  1506 Loss:  2.7766166\n",
      "Training Step:  1507 Loss:  2.7749743\n",
      "Training Step:  1508 Loss:  2.7733345\n",
      "Training Step:  1509 Loss:  2.7716942\n",
      "Training Step:  1510 Loss:  2.7700524\n",
      "Training Step:  1511 Loss:  2.7684155\n",
      "Training Step:  1512 Loss:  2.76678\n",
      "Training Step:  1513 Loss:  2.765146\n",
      "Training Step:  1514 Loss:  2.7635105\n",
      "Training Step:  1515 Loss:  2.7618794\n",
      "Training Step:  1516 Loss:  2.7602496\n",
      "Training Step:  1517 Loss:  2.758617\n",
      "Training Step:  1518 Loss:  2.756991\n",
      "Training Step:  1519 Loss:  2.7553654\n",
      "Training Step:  1520 Loss:  2.7537386\n",
      "Training Step:  1521 Loss:  2.7521124\n",
      "Training Step:  1522 Loss:  2.7504907\n",
      "Training Step:  1523 Loss:  2.7488675\n",
      "Training Step:  1524 Loss:  2.747248\n",
      "Training Step:  1525 Loss:  2.7456298\n",
      "Training Step:  1526 Loss:  2.744009\n",
      "Training Step:  1527 Loss:  2.742395\n",
      "Training Step:  1528 Loss:  2.7407784\n",
      "Training Step:  1529 Loss:  2.7391653\n",
      "Training Step:  1530 Loss:  2.7375536\n",
      "Training Step:  1531 Loss:  2.7359421\n",
      "Training Step:  1532 Loss:  2.734332\n",
      "Training Step:  1533 Loss:  2.7327228\n",
      "Training Step:  1534 Loss:  2.731115\n",
      "Training Step:  1535 Loss:  2.7295094\n",
      "Training Step:  1536 Loss:  2.7279048\n",
      "Training Step:  1537 Loss:  2.7263017\n",
      "Training Step:  1538 Loss:  2.7246988\n",
      "Training Step:  1539 Loss:  2.7230976\n",
      "Training Step:  1540 Loss:  2.721498\n",
      "Training Step:  1541 Loss:  2.7199004\n",
      "Training Step:  1542 Loss:  2.718303\n",
      "Training Step:  1543 Loss:  2.7167063\n",
      "Training Step:  1544 Loss:  2.7151113\n",
      "Training Step:  1545 Loss:  2.7135186\n",
      "Training Step:  1546 Loss:  2.7119267\n",
      "Training Step:  1547 Loss:  2.7103367\n",
      "Training Step:  1548 Loss:  2.708747\n",
      "Training Step:  1549 Loss:  2.7071598\n",
      "Training Step:  1550 Loss:  2.7055717\n",
      "Training Step:  1551 Loss:  2.7039857\n",
      "Training Step:  1552 Loss:  2.702404\n",
      "Training Step:  1553 Loss:  2.7008207\n",
      "Training Step:  1554 Loss:  2.6992378\n",
      "Training Step:  1555 Loss:  2.6976576\n",
      "Training Step:  1556 Loss:  2.6960776\n",
      "Training Step:  1557 Loss:  2.6945002\n",
      "Training Step:  1558 Loss:  2.6929245\n",
      "Training Step:  1559 Loss:  2.6913483\n",
      "Training Step:  1560 Loss:  2.6897743\n",
      "Training Step:  1561 Loss:  2.6882029\n",
      "Training Step:  1562 Loss:  2.6866326\n",
      "Training Step:  1563 Loss:  2.68506\n",
      "Training Step:  1564 Loss:  2.6834924\n",
      "Training Step:  1565 Loss:  2.681923\n",
      "Training Step:  1566 Loss:  2.6803584\n",
      "Training Step:  1567 Loss:  2.6787932\n",
      "Training Step:  1568 Loss:  2.677228\n",
      "Training Step:  1569 Loss:  2.6756656\n",
      "Training Step:  1570 Loss:  2.674105\n",
      "Training Step:  1571 Loss:  2.6725454\n",
      "Training Step:  1572 Loss:  2.6709855\n",
      "Training Step:  1573 Loss:  2.6694288\n",
      "Training Step:  1574 Loss:  2.6678715\n",
      "Training Step:  1575 Loss:  2.666317\n",
      "Training Step:  1576 Loss:  2.6647635\n",
      "Training Step:  1577 Loss:  2.6632104\n",
      "Training Step:  1578 Loss:  2.6616588\n",
      "Training Step:  1579 Loss:  2.6601098\n",
      "Training Step:  1580 Loss:  2.6585596\n",
      "Training Step:  1581 Loss:  2.657014\n",
      "Training Step:  1582 Loss:  2.6554668\n",
      "Training Step:  1583 Loss:  2.6539223\n",
      "Training Step:  1584 Loss:  2.6523762\n",
      "Training Step:  1585 Loss:  2.6508367\n",
      "Training Step:  1586 Loss:  2.6492944\n",
      "Training Step:  1587 Loss:  2.6477542\n",
      "Training Step:  1588 Loss:  2.6462154\n",
      "Training Step:  1589 Loss:  2.6446786\n",
      "Training Step:  1590 Loss:  2.6431408\n",
      "Training Step:  1591 Loss:  2.6416056\n",
      "Training Step:  1592 Loss:  2.6400738\n",
      "Training Step:  1593 Loss:  2.6385407\n",
      "Training Step:  1594 Loss:  2.6370106\n",
      "Training Step:  1595 Loss:  2.63548\n",
      "Training Step:  1596 Loss:  2.633951\n",
      "Training Step:  1597 Loss:  2.6324239\n",
      "Training Step:  1598 Loss:  2.6308956\n",
      "Training Step:  1599 Loss:  2.6293702\n",
      "Training Step:  1600 Loss:  2.6278467\n",
      "Training Step:  1601 Loss:  2.6263235\n",
      "Training Step:  1602 Loss:  2.624804\n",
      "Training Step:  1603 Loss:  2.623283\n",
      "Training Step:  1604 Loss:  2.6217632\n",
      "Training Step:  1605 Loss:  2.6202443\n",
      "Training Step:  1606 Loss:  2.6187289\n",
      "Training Step:  1607 Loss:  2.617214\n",
      "Training Step:  1608 Loss:  2.6157002\n",
      "Training Step:  1609 Loss:  2.614185\n",
      "Training Step:  1610 Loss:  2.6126754\n",
      "Training Step:  1611 Loss:  2.611164\n",
      "Training Step:  1612 Loss:  2.6096537\n",
      "Training Step:  1613 Loss:  2.6081462\n",
      "Training Step:  1614 Loss:  2.6066422\n",
      "Training Step:  1615 Loss:  2.605134\n",
      "Training Step:  1616 Loss:  2.6036305\n",
      "Training Step:  1617 Loss:  2.6021283\n",
      "Training Step:  1618 Loss:  2.6006258\n",
      "Training Step:  1619 Loss:  2.5991232\n",
      "Training Step:  1620 Loss:  2.5976248\n",
      "Training Step:  1621 Loss:  2.5961268\n",
      "Training Step:  1622 Loss:  2.59463\n",
      "Training Step:  1623 Loss:  2.5931334\n",
      "Training Step:  1624 Loss:  2.5916405\n",
      "Training Step:  1625 Loss:  2.5901463\n",
      "Training Step:  1626 Loss:  2.5886521\n",
      "Training Step:  1627 Loss:  2.587164\n",
      "Training Step:  1628 Loss:  2.5856724\n",
      "Training Step:  1629 Loss:  2.5841827\n",
      "Training Step:  1630 Loss:  2.5826967\n",
      "Training Step:  1631 Loss:  2.5812097\n",
      "Training Step:  1632 Loss:  2.5797248\n",
      "Training Step:  1633 Loss:  2.57824\n",
      "Training Step:  1634 Loss:  2.5767574\n",
      "Training Step:  1635 Loss:  2.5752773\n",
      "Training Step:  1636 Loss:  2.5737963\n",
      "Training Step:  1637 Loss:  2.5723176\n",
      "Training Step:  1638 Loss:  2.5708394\n",
      "Training Step:  1639 Loss:  2.5693617\n",
      "Training Step:  1640 Loss:  2.5678878\n",
      "Training Step:  1641 Loss:  2.5664132\n",
      "Training Step:  1642 Loss:  2.564938\n",
      "Training Step:  1643 Loss:  2.5634675\n",
      "Training Step:  1644 Loss:  2.5619965\n",
      "Training Step:  1645 Loss:  2.5605268\n",
      "Training Step:  1646 Loss:  2.5590591\n",
      "Training Step:  1647 Loss:  2.5575922\n",
      "Training Step:  1648 Loss:  2.5561256\n",
      "Training Step:  1649 Loss:  2.5546613\n",
      "Training Step:  1650 Loss:  2.5531971\n",
      "Training Step:  1651 Loss:  2.5517359\n",
      "Training Step:  1652 Loss:  2.5502744\n",
      "Training Step:  1653 Loss:  2.5488138\n",
      "Training Step:  1654 Loss:  2.547356\n",
      "Training Step:  1655 Loss:  2.5458965\n",
      "Training Step:  1656 Loss:  2.5444403\n",
      "Training Step:  1657 Loss:  2.5429854\n",
      "Training Step:  1658 Loss:  2.5415301\n",
      "Training Step:  1659 Loss:  2.5400789\n",
      "Training Step:  1660 Loss:  2.5386243\n",
      "Training Step:  1661 Loss:  2.5371757\n",
      "Training Step:  1662 Loss:  2.5357246\n",
      "Training Step:  1663 Loss:  2.534276\n",
      "Training Step:  1664 Loss:  2.5328295\n",
      "Training Step:  1665 Loss:  2.531384\n",
      "Training Step:  1666 Loss:  2.5299404\n",
      "Training Step:  1667 Loss:  2.5284944\n",
      "Training Step:  1668 Loss:  2.5270514\n",
      "Training Step:  1669 Loss:  2.5256102\n",
      "Training Step:  1670 Loss:  2.5241709\n",
      "Training Step:  1671 Loss:  2.5227308\n",
      "Training Step:  1672 Loss:  2.5212946\n",
      "Training Step:  1673 Loss:  2.5198572\n",
      "Training Step:  1674 Loss:  2.5184212\n",
      "Training Step:  1675 Loss:  2.5169864\n",
      "Training Step:  1676 Loss:  2.5155537\n",
      "Training Step:  1677 Loss:  2.5141215\n",
      "Training Step:  1678 Loss:  2.5126905\n",
      "Training Step:  1679 Loss:  2.51126\n",
      "Training Step:  1680 Loss:  2.5098305\n",
      "Training Step:  1681 Loss:  2.508406\n",
      "Training Step:  1682 Loss:  2.5069776\n",
      "Training Step:  1683 Loss:  2.5055535\n",
      "Training Step:  1684 Loss:  2.504129\n",
      "Training Step:  1685 Loss:  2.502706\n",
      "Training Step:  1686 Loss:  2.5012856\n",
      "Training Step:  1687 Loss:  2.4998636\n",
      "Training Step:  1688 Loss:  2.498445\n",
      "Training Step:  1689 Loss:  2.4970253\n",
      "Training Step:  1690 Loss:  2.4956093\n",
      "Training Step:  1691 Loss:  2.494193\n",
      "Training Step:  1692 Loss:  2.4927783\n",
      "Training Step:  1693 Loss:  2.4913654\n",
      "Training Step:  1694 Loss:  2.489952\n",
      "Training Step:  1695 Loss:  2.4885404\n",
      "Training Step:  1696 Loss:  2.4871316\n",
      "Training Step:  1697 Loss:  2.4857213\n",
      "Training Step:  1698 Loss:  2.4843147\n",
      "Training Step:  1699 Loss:  2.4829068\n",
      "Training Step:  1700 Loss:  2.4815016\n",
      "Training Step:  1701 Loss:  2.4800942\n",
      "Training Step:  1702 Loss:  2.4786925\n",
      "Training Step:  1703 Loss:  2.4772906\n",
      "Training Step:  1704 Loss:  2.4758892\n",
      "Training Step:  1705 Loss:  2.474489\n",
      "Training Step:  1706 Loss:  2.47309\n",
      "Training Step:  1707 Loss:  2.4716923\n",
      "Training Step:  1708 Loss:  2.4702961\n",
      "Training Step:  1709 Loss:  2.4688983\n",
      "Training Step:  1710 Loss:  2.467506\n",
      "Training Step:  1711 Loss:  2.4661126\n",
      "Training Step:  1712 Loss:  2.4647198\n",
      "Training Step:  1713 Loss:  2.4633284\n",
      "Training Step:  1714 Loss:  2.4619408\n",
      "Training Step:  1715 Loss:  2.4605508\n",
      "Training Step:  1716 Loss:  2.4591641\n",
      "Training Step:  1717 Loss:  2.4577768\n",
      "Training Step:  1718 Loss:  2.4563918\n",
      "Training Step:  1719 Loss:  2.4550064\n",
      "Training Step:  1720 Loss:  2.453625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  1721 Loss:  2.452242\n",
      "Training Step:  1722 Loss:  2.4508593\n",
      "Training Step:  1723 Loss:  2.4494812\n",
      "Training Step:  1724 Loss:  2.448102\n",
      "Training Step:  1725 Loss:  2.446725\n",
      "Training Step:  1726 Loss:  2.4453478\n",
      "Training Step:  1727 Loss:  2.4439716\n",
      "Training Step:  1728 Loss:  2.442599\n",
      "Training Step:  1729 Loss:  2.441225\n",
      "Training Step:  1730 Loss:  2.4398525\n",
      "Training Step:  1731 Loss:  2.43848\n",
      "Training Step:  1732 Loss:  2.4371119\n",
      "Training Step:  1733 Loss:  2.4357429\n",
      "Training Step:  1734 Loss:  2.4343753\n",
      "Training Step:  1735 Loss:  2.4330087\n",
      "Training Step:  1736 Loss:  2.431643\n",
      "Training Step:  1737 Loss:  2.4302785\n",
      "Training Step:  1738 Loss:  2.428916\n",
      "Training Step:  1739 Loss:  2.4275537\n",
      "Training Step:  1740 Loss:  2.4261928\n",
      "Training Step:  1741 Loss:  2.4248333\n",
      "Training Step:  1742 Loss:  2.423473\n",
      "Training Step:  1743 Loss:  2.4221153\n",
      "Training Step:  1744 Loss:  2.420759\n",
      "Training Step:  1745 Loss:  2.419403\n",
      "Training Step:  1746 Loss:  2.418049\n",
      "Training Step:  1747 Loss:  2.4166946\n",
      "Training Step:  1748 Loss:  2.415344\n",
      "Training Step:  1749 Loss:  2.4139915\n",
      "Training Step:  1750 Loss:  2.4126408\n",
      "Training Step:  1751 Loss:  2.4112918\n",
      "Training Step:  1752 Loss:  2.409943\n",
      "Training Step:  1753 Loss:  2.4085972\n",
      "Training Step:  1754 Loss:  2.4072523\n",
      "Training Step:  1755 Loss:  2.4059064\n",
      "Training Step:  1756 Loss:  2.404563\n",
      "Training Step:  1757 Loss:  2.4032197\n",
      "Training Step:  1758 Loss:  2.40188\n",
      "Training Step:  1759 Loss:  2.400538\n",
      "Training Step:  1760 Loss:  2.3991995\n",
      "Training Step:  1761 Loss:  2.3978617\n",
      "Training Step:  1762 Loss:  2.3965228\n",
      "Training Step:  1763 Loss:  2.395188\n",
      "Training Step:  1764 Loss:  2.3938546\n",
      "Training Step:  1765 Loss:  2.3925195\n",
      "Training Step:  1766 Loss:  2.391187\n",
      "Training Step:  1767 Loss:  2.389854\n",
      "Training Step:  1768 Loss:  2.3885243\n",
      "Training Step:  1769 Loss:  2.387194\n",
      "Training Step:  1770 Loss:  2.385867\n",
      "Training Step:  1771 Loss:  2.3845391\n",
      "Training Step:  1772 Loss:  2.3832138\n",
      "Training Step:  1773 Loss:  2.381887\n",
      "Training Step:  1774 Loss:  2.3805654\n",
      "Training Step:  1775 Loss:  2.379239\n",
      "Training Step:  1776 Loss:  2.3779182\n",
      "Training Step:  1777 Loss:  2.3765986\n",
      "Training Step:  1778 Loss:  2.3752766\n",
      "Training Step:  1779 Loss:  2.3739574\n",
      "Training Step:  1780 Loss:  2.3726401\n",
      "Training Step:  1781 Loss:  2.371324\n",
      "Training Step:  1782 Loss:  2.3700058\n",
      "Training Step:  1783 Loss:  2.3686914\n",
      "Training Step:  1784 Loss:  2.3673797\n",
      "Training Step:  1785 Loss:  2.3660667\n",
      "Training Step:  1786 Loss:  2.3647542\n",
      "Training Step:  1787 Loss:  2.3634434\n",
      "Training Step:  1788 Loss:  2.3621354\n",
      "Training Step:  1789 Loss:  2.3608255\n",
      "Training Step:  1790 Loss:  2.3595204\n",
      "Training Step:  1791 Loss:  2.3582158\n",
      "Training Step:  1792 Loss:  2.356909\n",
      "Training Step:  1793 Loss:  2.3556046\n",
      "Training Step:  1794 Loss:  2.3543017\n",
      "Training Step:  1795 Loss:  2.3529997\n",
      "Training Step:  1796 Loss:  2.3516977\n",
      "Training Step:  1797 Loss:  2.3503985\n",
      "Training Step:  1798 Loss:  2.3491015\n",
      "Training Step:  1799 Loss:  2.3478036\n",
      "Training Step:  1800 Loss:  2.3465052\n",
      "Training Step:  1801 Loss:  2.3452094\n",
      "Training Step:  1802 Loss:  2.3439145\n",
      "Training Step:  1803 Loss:  2.3426216\n",
      "Training Step:  1804 Loss:  2.341329\n",
      "Training Step:  1805 Loss:  2.3400364\n",
      "Training Step:  1806 Loss:  2.338747\n",
      "Training Step:  1807 Loss:  2.3374557\n",
      "Training Step:  1808 Loss:  2.3361688\n",
      "Training Step:  1809 Loss:  2.3348813\n",
      "Training Step:  1810 Loss:  2.3335946\n",
      "Training Step:  1811 Loss:  2.3323095\n",
      "Training Step:  1812 Loss:  2.3310237\n",
      "Training Step:  1813 Loss:  2.3297434\n",
      "Training Step:  1814 Loss:  2.3284597\n",
      "Training Step:  1815 Loss:  2.3271778\n",
      "Training Step:  1816 Loss:  2.325897\n",
      "Training Step:  1817 Loss:  2.3246195\n",
      "Training Step:  1818 Loss:  2.323341\n",
      "Training Step:  1819 Loss:  2.322064\n",
      "Training Step:  1820 Loss:  2.3207881\n",
      "Training Step:  1821 Loss:  2.3195114\n",
      "Training Step:  1822 Loss:  2.3182383\n",
      "Training Step:  1823 Loss:  2.3169656\n",
      "Training Step:  1824 Loss:  2.3156955\n",
      "Training Step:  1825 Loss:  2.3144226\n",
      "Training Step:  1826 Loss:  2.3131516\n",
      "Training Step:  1827 Loss:  2.3118844\n",
      "Training Step:  1828 Loss:  2.3106148\n",
      "Training Step:  1829 Loss:  2.3093488\n",
      "Training Step:  1830 Loss:  2.308082\n",
      "Training Step:  1831 Loss:  2.3068185\n",
      "Training Step:  1832 Loss:  2.3055553\n",
      "Training Step:  1833 Loss:  2.3042927\n",
      "Training Step:  1834 Loss:  2.3030305\n",
      "Training Step:  1835 Loss:  2.301769\n",
      "Training Step:  1836 Loss:  2.3005116\n",
      "Training Step:  1837 Loss:  2.299253\n",
      "Training Step:  1838 Loss:  2.2979932\n",
      "Training Step:  1839 Loss:  2.2967384\n",
      "Training Step:  1840 Loss:  2.295481\n",
      "Training Step:  1841 Loss:  2.2942266\n",
      "Training Step:  1842 Loss:  2.292973\n",
      "Training Step:  1843 Loss:  2.2917223\n",
      "Training Step:  1844 Loss:  2.2904701\n",
      "Training Step:  1845 Loss:  2.2892199\n",
      "Training Step:  1846 Loss:  2.2879694\n",
      "Training Step:  1847 Loss:  2.2867208\n",
      "Training Step:  1848 Loss:  2.285473\n",
      "Training Step:  1849 Loss:  2.2842271\n",
      "Training Step:  1850 Loss:  2.2829823\n",
      "Training Step:  1851 Loss:  2.2817361\n",
      "Training Step:  1852 Loss:  2.280493\n",
      "Training Step:  1853 Loss:  2.2792494\n",
      "Training Step:  1854 Loss:  2.278008\n",
      "Training Step:  1855 Loss:  2.2767684\n",
      "Training Step:  1856 Loss:  2.2755294\n",
      "Training Step:  1857 Loss:  2.27429\n",
      "Training Step:  1858 Loss:  2.2730522\n",
      "Training Step:  1859 Loss:  2.2718172\n",
      "Training Step:  1860 Loss:  2.270579\n",
      "Training Step:  1861 Loss:  2.269347\n",
      "Training Step:  1862 Loss:  2.2681122\n",
      "Training Step:  1863 Loss:  2.2668796\n",
      "Training Step:  1864 Loss:  2.2656476\n",
      "Training Step:  1865 Loss:  2.264416\n",
      "Training Step:  1866 Loss:  2.263189\n",
      "Training Step:  1867 Loss:  2.2619586\n",
      "Training Step:  1868 Loss:  2.260733\n",
      "Training Step:  1869 Loss:  2.259505\n",
      "Training Step:  1870 Loss:  2.2582805\n",
      "Training Step:  1871 Loss:  2.2570565\n",
      "Training Step:  1872 Loss:  2.2558317\n",
      "Training Step:  1873 Loss:  2.2546098\n",
      "Training Step:  1874 Loss:  2.253387\n",
      "Training Step:  1875 Loss:  2.2521672\n",
      "Training Step:  1876 Loss:  2.2509456\n",
      "Training Step:  1877 Loss:  2.2497287\n",
      "Training Step:  1878 Loss:  2.2485104\n",
      "Training Step:  1879 Loss:  2.2472937\n",
      "Training Step:  1880 Loss:  2.246077\n",
      "Training Step:  1881 Loss:  2.244863\n",
      "Training Step:  1882 Loss:  2.2436497\n",
      "Training Step:  1883 Loss:  2.2424355\n",
      "Training Step:  1884 Loss:  2.2412243\n",
      "Training Step:  1885 Loss:  2.2400136\n",
      "Training Step:  1886 Loss:  2.238804\n",
      "Training Step:  1887 Loss:  2.2375946\n",
      "Training Step:  1888 Loss:  2.2363853\n",
      "Training Step:  1889 Loss:  2.23518\n",
      "Training Step:  1890 Loss:  2.2339725\n",
      "Training Step:  1891 Loss:  2.2327685\n",
      "Training Step:  1892 Loss:  2.231564\n",
      "Training Step:  1893 Loss:  2.23036\n",
      "Training Step:  1894 Loss:  2.2291582\n",
      "Training Step:  1895 Loss:  2.2279572\n",
      "Training Step:  1896 Loss:  2.226757\n",
      "Training Step:  1897 Loss:  2.2255588\n",
      "Training Step:  1898 Loss:  2.2243595\n",
      "Training Step:  1899 Loss:  2.2231617\n",
      "Training Step:  1900 Loss:  2.2219665\n",
      "Training Step:  1901 Loss:  2.2207708\n",
      "Training Step:  1902 Loss:  2.2195764\n",
      "Training Step:  1903 Loss:  2.2183838\n",
      "Training Step:  1904 Loss:  2.2171912\n",
      "Training Step:  1905 Loss:  2.215998\n",
      "Training Step:  1906 Loss:  2.2148094\n",
      "Training Step:  1907 Loss:  2.2136183\n",
      "Training Step:  1908 Loss:  2.2124293\n",
      "Training Step:  1909 Loss:  2.2112432\n",
      "Training Step:  1910 Loss:  2.2100542\n",
      "Training Step:  1911 Loss:  2.2088685\n",
      "Training Step:  1912 Loss:  2.2076855\n",
      "Training Step:  1913 Loss:  2.2064998\n",
      "Training Step:  1914 Loss:  2.2053175\n",
      "Training Step:  1915 Loss:  2.2041361\n",
      "Training Step:  1916 Loss:  2.2029533\n",
      "Training Step:  1917 Loss:  2.2017746\n",
      "Training Step:  1918 Loss:  2.200595\n",
      "Training Step:  1919 Loss:  2.1994166\n",
      "Training Step:  1920 Loss:  2.19824\n",
      "Training Step:  1921 Loss:  2.1970634\n",
      "Training Step:  1922 Loss:  2.1958876\n",
      "Training Step:  1923 Loss:  2.194713\n",
      "Training Step:  1924 Loss:  2.1935396\n",
      "Training Step:  1925 Loss:  2.1923685\n",
      "Training Step:  1926 Loss:  2.1911976\n",
      "Training Step:  1927 Loss:  2.1900253\n",
      "Training Step:  1928 Loss:  2.1888576\n",
      "Training Step:  1929 Loss:  2.1876874\n",
      "Training Step:  1930 Loss:  2.18652\n",
      "Training Step:  1931 Loss:  2.1853533\n",
      "Training Step:  1932 Loss:  2.1841884\n",
      "Training Step:  1933 Loss:  2.1830215\n",
      "Training Step:  1934 Loss:  2.1818576\n",
      "Training Step:  1935 Loss:  2.180696\n",
      "Training Step:  1936 Loss:  2.1795323\n",
      "Training Step:  1937 Loss:  2.1783714\n",
      "Training Step:  1938 Loss:  2.1772103\n",
      "Training Step:  1939 Loss:  2.17605\n",
      "Training Step:  1940 Loss:  2.1748936\n",
      "Training Step:  1941 Loss:  2.173736\n",
      "Training Step:  1942 Loss:  2.1725795\n",
      "Training Step:  1943 Loss:  2.171423\n",
      "Training Step:  1944 Loss:  2.170271\n",
      "Training Step:  1945 Loss:  2.169116\n",
      "Training Step:  1946 Loss:  2.167962\n",
      "Training Step:  1947 Loss:  2.166811\n",
      "Training Step:  1948 Loss:  2.165659\n",
      "Training Step:  1949 Loss:  2.1645098\n",
      "Training Step:  1950 Loss:  2.1633618\n",
      "Training Step:  1951 Loss:  2.162211\n",
      "Training Step:  1952 Loss:  2.1610646\n",
      "Training Step:  1953 Loss:  2.1599176\n",
      "Training Step:  1954 Loss:  2.158773\n",
      "Training Step:  1955 Loss:  2.1576283\n",
      "Training Step:  1956 Loss:  2.156485\n",
      "Training Step:  1957 Loss:  2.1553416\n",
      "Training Step:  1958 Loss:  2.1542003\n",
      "Training Step:  1959 Loss:  2.15306\n",
      "Training Step:  1960 Loss:  2.1519191\n",
      "Training Step:  1961 Loss:  2.1507812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  1962 Loss:  2.1496425\n",
      "Training Step:  1963 Loss:  2.1485057\n",
      "Training Step:  1964 Loss:  2.1473703\n",
      "Training Step:  1965 Loss:  2.146233\n",
      "Training Step:  1966 Loss:  2.1450996\n",
      "Training Step:  1967 Loss:  2.1439652\n",
      "Training Step:  1968 Loss:  2.1428351\n",
      "Training Step:  1969 Loss:  2.1417022\n",
      "Training Step:  1970 Loss:  2.1405702\n",
      "Training Step:  1971 Loss:  2.139441\n",
      "Training Step:  1972 Loss:  2.1383116\n",
      "Training Step:  1973 Loss:  2.1371846\n",
      "Training Step:  1974 Loss:  2.136056\n",
      "Training Step:  1975 Loss:  2.1349306\n",
      "Training Step:  1976 Loss:  2.133805\n",
      "Training Step:  1977 Loss:  2.1326807\n",
      "Training Step:  1978 Loss:  2.131558\n",
      "Training Step:  1979 Loss:  2.130435\n",
      "Training Step:  1980 Loss:  2.1293123\n",
      "Training Step:  1981 Loss:  2.1281927\n",
      "Training Step:  1982 Loss:  2.1270714\n",
      "Training Step:  1983 Loss:  2.125953\n",
      "Training Step:  1984 Loss:  2.1248348\n",
      "Training Step:  1985 Loss:  2.1237187\n",
      "Training Step:  1986 Loss:  2.1226008\n",
      "Training Step:  1987 Loss:  2.121487\n",
      "Training Step:  1988 Loss:  2.120371\n",
      "Training Step:  1989 Loss:  2.1192575\n",
      "Training Step:  1990 Loss:  2.1181448\n",
      "Training Step:  1991 Loss:  2.117033\n",
      "Training Step:  1992 Loss:  2.115923\n",
      "Training Step:  1993 Loss:  2.114812\n",
      "Training Step:  1994 Loss:  2.1137037\n",
      "Training Step:  1995 Loss:  2.1125946\n",
      "Training Step:  1996 Loss:  2.1114874\n",
      "Training Step:  1997 Loss:  2.1103811\n",
      "Training Step:  1998 Loss:  2.1092749\n",
      "Training Step:  1999 Loss:  2.108171\n",
      "Training Step:  2000 Loss:  2.1070664\n",
      "Training Step:  2001 Loss:  2.1059635\n",
      "Training Step:  2002 Loss:  2.1048625\n",
      "Training Step:  2003 Loss:  2.1037612\n",
      "Training Step:  2004 Loss:  2.1026607\n",
      "Training Step:  2005 Loss:  2.1015599\n",
      "Training Step:  2006 Loss:  2.100462\n",
      "Training Step:  2007 Loss:  2.0993657\n",
      "Training Step:  2008 Loss:  2.0982668\n",
      "Training Step:  2009 Loss:  2.0971732\n",
      "Training Step:  2010 Loss:  2.0960786\n",
      "Training Step:  2011 Loss:  2.0949845\n",
      "Training Step:  2012 Loss:  2.093892\n",
      "Training Step:  2013 Loss:  2.0927982\n",
      "Training Step:  2014 Loss:  2.0917065\n",
      "Training Step:  2015 Loss:  2.0906153\n",
      "Training Step:  2016 Loss:  2.089527\n",
      "Training Step:  2017 Loss:  2.0884376\n",
      "Training Step:  2018 Loss:  2.0873485\n",
      "Training Step:  2019 Loss:  2.0862622\n",
      "Training Step:  2020 Loss:  2.085176\n",
      "Training Step:  2021 Loss:  2.0840921\n",
      "Training Step:  2022 Loss:  2.0830073\n",
      "Training Step:  2023 Loss:  2.0819232\n",
      "Training Step:  2024 Loss:  2.080841\n",
      "Training Step:  2025 Loss:  2.0797575\n",
      "Training Step:  2026 Loss:  2.0786765\n",
      "Training Step:  2027 Loss:  2.0775971\n",
      "Training Step:  2028 Loss:  2.0765183\n",
      "Training Step:  2029 Loss:  2.075439\n",
      "Training Step:  2030 Loss:  2.0743613\n",
      "Training Step:  2031 Loss:  2.0732865\n",
      "Training Step:  2032 Loss:  2.072211\n",
      "Training Step:  2033 Loss:  2.0711353\n",
      "Training Step:  2034 Loss:  2.0700605\n",
      "Training Step:  2035 Loss:  2.0689871\n",
      "Training Step:  2036 Loss:  2.0679154\n",
      "Training Step:  2037 Loss:  2.0668454\n",
      "Training Step:  2038 Loss:  2.065774\n",
      "Training Step:  2039 Loss:  2.0647035\n",
      "Training Step:  2040 Loss:  2.063634\n",
      "Training Step:  2041 Loss:  2.0625672\n",
      "Training Step:  2042 Loss:  2.0614996\n",
      "Training Step:  2043 Loss:  2.0604324\n",
      "Training Step:  2044 Loss:  2.0593672\n",
      "Training Step:  2045 Loss:  2.058303\n",
      "Training Step:  2046 Loss:  2.0572405\n",
      "Training Step:  2047 Loss:  2.0561779\n",
      "Training Step:  2048 Loss:  2.055115\n",
      "Training Step:  2049 Loss:  2.0540535\n",
      "Training Step:  2050 Loss:  2.052993\n",
      "Training Step:  2051 Loss:  2.0519338\n",
      "Training Step:  2052 Loss:  2.050875\n",
      "Training Step:  2053 Loss:  2.0498185\n",
      "Training Step:  2054 Loss:  2.0487604\n",
      "Training Step:  2055 Loss:  2.047705\n",
      "Training Step:  2056 Loss:  2.04665\n",
      "Training Step:  2057 Loss:  2.0455964\n",
      "Training Step:  2058 Loss:  2.0445418\n",
      "Training Step:  2059 Loss:  2.0434875\n",
      "Training Step:  2060 Loss:  2.0424364\n",
      "Training Step:  2061 Loss:  2.0413876\n",
      "Training Step:  2062 Loss:  2.0403357\n",
      "Training Step:  2063 Loss:  2.0392852\n",
      "Training Step:  2064 Loss:  2.0382385\n",
      "Training Step:  2065 Loss:  2.037191\n",
      "Training Step:  2066 Loss:  2.036145\n",
      "Training Step:  2067 Loss:  2.0350974\n",
      "Training Step:  2068 Loss:  2.0340526\n",
      "Training Step:  2069 Loss:  2.033008\n",
      "Training Step:  2070 Loss:  2.0319643\n",
      "Training Step:  2071 Loss:  2.030923\n",
      "Training Step:  2072 Loss:  2.029881\n",
      "Training Step:  2073 Loss:  2.0288403\n",
      "Training Step:  2074 Loss:  2.0277996\n",
      "Training Step:  2075 Loss:  2.0267613\n",
      "Training Step:  2076 Loss:  2.025722\n",
      "Training Step:  2077 Loss:  2.0246854\n",
      "Training Step:  2078 Loss:  2.0236492\n",
      "Training Step:  2079 Loss:  2.0226126\n",
      "Training Step:  2080 Loss:  2.0215783\n",
      "Training Step:  2081 Loss:  2.0205448\n",
      "Training Step:  2082 Loss:  2.0195103\n",
      "Training Step:  2083 Loss:  2.0184774\n",
      "Training Step:  2084 Loss:  2.017445\n",
      "Training Step:  2085 Loss:  2.0164146\n",
      "Training Step:  2086 Loss:  2.015384\n",
      "Training Step:  2087 Loss:  2.014356\n",
      "Training Step:  2088 Loss:  2.0133283\n",
      "Training Step:  2089 Loss:  2.0123\n",
      "Training Step:  2090 Loss:  2.0112731\n",
      "Training Step:  2091 Loss:  2.010246\n",
      "Training Step:  2092 Loss:  2.0092232\n",
      "Training Step:  2093 Loss:  2.0081985\n",
      "Training Step:  2094 Loss:  2.0071743\n",
      "Training Step:  2095 Loss:  2.0061495\n",
      "Training Step:  2096 Loss:  2.00513\n",
      "Training Step:  2097 Loss:  2.0041103\n",
      "Training Step:  2098 Loss:  2.0030897\n",
      "Training Step:  2099 Loss:  2.00207\n",
      "Training Step:  2100 Loss:  2.0010514\n",
      "Training Step:  2101 Loss:  2.0000346\n",
      "Training Step:  2102 Loss:  1.999017\n",
      "Training Step:  2103 Loss:  1.9980006\n",
      "Training Step:  2104 Loss:  1.9969853\n",
      "Training Step:  2105 Loss:  1.9959698\n",
      "Training Step:  2106 Loss:  1.994957\n",
      "Training Step:  2107 Loss:  1.9939446\n",
      "Training Step:  2108 Loss:  1.9929334\n",
      "Training Step:  2109 Loss:  1.9919205\n",
      "Training Step:  2110 Loss:  1.9909112\n",
      "Training Step:  2111 Loss:  1.9899015\n",
      "Training Step:  2112 Loss:  1.9888927\n",
      "Training Step:  2113 Loss:  1.9878846\n",
      "Training Step:  2114 Loss:  1.9868758\n",
      "Training Step:  2115 Loss:  1.9858702\n",
      "Training Step:  2116 Loss:  1.984866\n",
      "Training Step:  2117 Loss:  1.9838598\n",
      "Training Step:  2118 Loss:  1.9828551\n",
      "Training Step:  2119 Loss:  1.9818532\n",
      "Training Step:  2120 Loss:  1.9808509\n",
      "Training Step:  2121 Loss:  1.9798491\n",
      "Training Step:  2122 Loss:  1.9788493\n",
      "Training Step:  2123 Loss:  1.9778497\n",
      "Training Step:  2124 Loss:  1.97685\n",
      "Training Step:  2125 Loss:  1.9758509\n",
      "Training Step:  2126 Loss:  1.9748539\n",
      "Training Step:  2127 Loss:  1.9738588\n",
      "Training Step:  2128 Loss:  1.9728634\n",
      "Training Step:  2129 Loss:  1.9718666\n",
      "Training Step:  2130 Loss:  1.9708731\n",
      "Training Step:  2131 Loss:  1.9698795\n",
      "Training Step:  2132 Loss:  1.9688872\n",
      "Training Step:  2133 Loss:  1.9678953\n",
      "Training Step:  2134 Loss:  1.9669033\n",
      "Training Step:  2135 Loss:  1.9659114\n",
      "Training Step:  2136 Loss:  1.964923\n",
      "Training Step:  2137 Loss:  1.9639349\n",
      "Training Step:  2138 Loss:  1.9629474\n",
      "Training Step:  2139 Loss:  1.9619595\n",
      "Training Step:  2140 Loss:  1.9609748\n",
      "Training Step:  2141 Loss:  1.9599898\n",
      "Training Step:  2142 Loss:  1.9590025\n",
      "Training Step:  2143 Loss:  1.9580197\n",
      "Training Step:  2144 Loss:  1.9570367\n",
      "Training Step:  2145 Loss:  1.9560547\n",
      "Training Step:  2146 Loss:  1.9550726\n",
      "Training Step:  2147 Loss:  1.9540915\n",
      "Training Step:  2148 Loss:  1.953112\n",
      "Training Step:  2149 Loss:  1.9521337\n",
      "Training Step:  2150 Loss:  1.9511544\n",
      "Training Step:  2151 Loss:  1.9501766\n",
      "Training Step:  2152 Loss:  1.9492004\n",
      "Training Step:  2153 Loss:  1.9482223\n",
      "Training Step:  2154 Loss:  1.9472489\n",
      "Training Step:  2155 Loss:  1.9462749\n",
      "Training Step:  2156 Loss:  1.9453013\n",
      "Training Step:  2157 Loss:  1.9443262\n",
      "Training Step:  2158 Loss:  1.9433542\n",
      "Training Step:  2159 Loss:  1.9423844\n",
      "Training Step:  2160 Loss:  1.9414142\n",
      "Training Step:  2161 Loss:  1.9404447\n",
      "Training Step:  2162 Loss:  1.9394752\n",
      "Training Step:  2163 Loss:  1.9385059\n",
      "Training Step:  2164 Loss:  1.9375393\n",
      "Training Step:  2165 Loss:  1.9365709\n",
      "Training Step:  2166 Loss:  1.935606\n",
      "Training Step:  2167 Loss:  1.9346409\n",
      "Training Step:  2168 Loss:  1.9336766\n",
      "Training Step:  2169 Loss:  1.9327128\n",
      "Training Step:  2170 Loss:  1.9317508\n",
      "Training Step:  2171 Loss:  1.9307897\n",
      "Training Step:  2172 Loss:  1.9298278\n",
      "Training Step:  2173 Loss:  1.9288651\n",
      "Training Step:  2174 Loss:  1.9279059\n",
      "Training Step:  2175 Loss:  1.9269483\n",
      "Training Step:  2176 Loss:  1.9259888\n",
      "Training Step:  2177 Loss:  1.9250321\n",
      "Training Step:  2178 Loss:  1.9240744\n",
      "Training Step:  2179 Loss:  1.9231212\n",
      "Training Step:  2180 Loss:  1.9221637\n",
      "Training Step:  2181 Loss:  1.92121\n",
      "Training Step:  2182 Loss:  1.9202557\n",
      "Training Step:  2183 Loss:  1.9193041\n",
      "Training Step:  2184 Loss:  1.9183509\n",
      "Training Step:  2185 Loss:  1.9173985\n",
      "Training Step:  2186 Loss:  1.9164507\n",
      "Training Step:  2187 Loss:  1.9155012\n",
      "Training Step:  2188 Loss:  1.9145521\n",
      "Training Step:  2189 Loss:  1.9136033\n",
      "Training Step:  2190 Loss:  1.9126544\n",
      "Training Step:  2191 Loss:  1.9117085\n",
      "Training Step:  2192 Loss:  1.9107628\n",
      "Training Step:  2193 Loss:  1.9098182\n",
      "Training Step:  2194 Loss:  1.9088736\n",
      "Training Step:  2195 Loss:  1.9079297\n",
      "Training Step:  2196 Loss:  1.906985\n",
      "Training Step:  2197 Loss:  1.9060435\n",
      "Training Step:  2198 Loss:  1.9051019\n",
      "Training Step:  2199 Loss:  1.9041613\n",
      "Training Step:  2200 Loss:  1.9032216\n",
      "Training Step:  2201 Loss:  1.9022822\n",
      "Training Step:  2202 Loss:  1.9013447\n",
      "Training Step:  2203 Loss:  1.9004064\n",
      "Training Step:  2204 Loss:  1.8994693\n",
      "Training Step:  2205 Loss:  1.8985325\n",
      "Training Step:  2206 Loss:  1.8975992\n",
      "Training Step:  2207 Loss:  1.8966633\n",
      "Training Step:  2208 Loss:  1.8957283\n",
      "Training Step:  2209 Loss:  1.8947955\n",
      "Training Step:  2210 Loss:  1.8938642\n",
      "Training Step:  2211 Loss:  1.8929303\n",
      "Training Step:  2212 Loss:  1.8920012\n",
      "Training Step:  2213 Loss:  1.8910714\n",
      "Training Step:  2214 Loss:  1.8901408\n",
      "Training Step:  2215 Loss:  1.8892121\n",
      "Training Step:  2216 Loss:  1.8882844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  2217 Loss:  1.8873572\n",
      "Training Step:  2218 Loss:  1.8864328\n",
      "Training Step:  2219 Loss:  1.8855062\n",
      "Training Step:  2220 Loss:  1.8845824\n",
      "Training Step:  2221 Loss:  1.8836567\n",
      "Training Step:  2222 Loss:  1.8827336\n",
      "Training Step:  2223 Loss:  1.8818109\n",
      "Training Step:  2224 Loss:  1.8808897\n",
      "Training Step:  2225 Loss:  1.8799667\n",
      "Training Step:  2226 Loss:  1.8790462\n",
      "Training Step:  2227 Loss:  1.8781259\n",
      "Training Step:  2228 Loss:  1.8772075\n",
      "Training Step:  2229 Loss:  1.8762907\n",
      "Training Step:  2230 Loss:  1.8753717\n",
      "Training Step:  2231 Loss:  1.8744549\n",
      "Training Step:  2232 Loss:  1.8735393\n",
      "Training Step:  2233 Loss:  1.8726249\n",
      "Training Step:  2234 Loss:  1.871708\n",
      "Training Step:  2235 Loss:  1.8707951\n",
      "Training Step:  2236 Loss:  1.8698823\n",
      "Training Step:  2237 Loss:  1.8689705\n",
      "Training Step:  2238 Loss:  1.8680568\n",
      "Training Step:  2239 Loss:  1.8671476\n",
      "Training Step:  2240 Loss:  1.866237\n",
      "Training Step:  2241 Loss:  1.8653275\n",
      "Training Step:  2242 Loss:  1.8644189\n",
      "Training Step:  2243 Loss:  1.8635094\n",
      "Training Step:  2244 Loss:  1.8626039\n",
      "Training Step:  2245 Loss:  1.8616972\n",
      "Training Step:  2246 Loss:  1.8607903\n",
      "Training Step:  2247 Loss:  1.8598858\n",
      "Training Step:  2248 Loss:  1.8589809\n",
      "Training Step:  2249 Loss:  1.8580773\n",
      "Training Step:  2250 Loss:  1.8571748\n",
      "Training Step:  2251 Loss:  1.8562733\n",
      "Training Step:  2252 Loss:  1.8553711\n",
      "Training Step:  2253 Loss:  1.8544701\n",
      "Training Step:  2254 Loss:  1.8535707\n",
      "Training Step:  2255 Loss:  1.8526708\n",
      "Training Step:  2256 Loss:  1.8517724\n",
      "Training Step:  2257 Loss:  1.8508754\n",
      "Training Step:  2258 Loss:  1.8499792\n",
      "Training Step:  2259 Loss:  1.8490813\n",
      "Training Step:  2260 Loss:  1.8481852\n",
      "Training Step:  2261 Loss:  1.8472914\n",
      "Training Step:  2262 Loss:  1.8463953\n",
      "Training Step:  2263 Loss:  1.8455013\n",
      "Training Step:  2264 Loss:  1.8446099\n",
      "Training Step:  2265 Loss:  1.8437178\n",
      "Training Step:  2266 Loss:  1.8428252\n",
      "Training Step:  2267 Loss:  1.8419346\n",
      "Training Step:  2268 Loss:  1.8410447\n",
      "Training Step:  2269 Loss:  1.840156\n",
      "Training Step:  2270 Loss:  1.8392673\n",
      "Training Step:  2271 Loss:  1.8383778\n",
      "Training Step:  2272 Loss:  1.8374908\n",
      "Training Step:  2273 Loss:  1.8366046\n",
      "Training Step:  2274 Loss:  1.8357205\n",
      "Training Step:  2275 Loss:  1.8348346\n",
      "Training Step:  2276 Loss:  1.8339515\n",
      "Training Step:  2277 Loss:  1.8330654\n",
      "Training Step:  2278 Loss:  1.8321844\n",
      "Training Step:  2279 Loss:  1.8313019\n",
      "Training Step:  2280 Loss:  1.8304204\n",
      "Training Step:  2281 Loss:  1.8295391\n",
      "Training Step:  2282 Loss:  1.8286598\n",
      "Training Step:  2283 Loss:  1.8277795\n",
      "Training Step:  2284 Loss:  1.8269002\n",
      "Training Step:  2285 Loss:  1.8260227\n",
      "Training Step:  2286 Loss:  1.825145\n",
      "Training Step:  2287 Loss:  1.8242705\n",
      "Training Step:  2288 Loss:  1.8233926\n",
      "Training Step:  2289 Loss:  1.8225186\n",
      "Training Step:  2290 Loss:  1.8216441\n",
      "Training Step:  2291 Loss:  1.8207709\n",
      "Training Step:  2292 Loss:  1.8198973\n",
      "Training Step:  2293 Loss:  1.8190246\n",
      "Training Step:  2294 Loss:  1.8181534\n",
      "Training Step:  2295 Loss:  1.8172829\n",
      "Training Step:  2296 Loss:  1.8164111\n",
      "Training Step:  2297 Loss:  1.8155433\n",
      "Training Step:  2298 Loss:  1.8146735\n",
      "Training Step:  2299 Loss:  1.8138062\n",
      "Training Step:  2300 Loss:  1.812937\n",
      "Training Step:  2301 Loss:  1.8120719\n",
      "Training Step:  2302 Loss:  1.8112054\n",
      "Training Step:  2303 Loss:  1.8103403\n",
      "Training Step:  2304 Loss:  1.8094742\n",
      "Training Step:  2305 Loss:  1.8086116\n",
      "Training Step:  2306 Loss:  1.8077478\n",
      "Training Step:  2307 Loss:  1.8068848\n",
      "Training Step:  2308 Loss:  1.806024\n",
      "Training Step:  2309 Loss:  1.8051621\n",
      "Training Step:  2310 Loss:  1.8043019\n",
      "Training Step:  2311 Loss:  1.8034425\n",
      "Training Step:  2312 Loss:  1.8025819\n",
      "Training Step:  2313 Loss:  1.8017246\n",
      "Training Step:  2314 Loss:  1.800868\n",
      "Training Step:  2315 Loss:  1.8000101\n",
      "Training Step:  2316 Loss:  1.799154\n",
      "Training Step:  2317 Loss:  1.7982975\n",
      "Training Step:  2318 Loss:  1.797442\n",
      "Training Step:  2319 Loss:  1.7965881\n",
      "Training Step:  2320 Loss:  1.7957336\n",
      "Training Step:  2321 Loss:  1.7948806\n",
      "Training Step:  2322 Loss:  1.7940286\n",
      "Training Step:  2323 Loss:  1.7931764\n",
      "Training Step:  2324 Loss:  1.7923257\n",
      "Training Step:  2325 Loss:  1.7914767\n",
      "Training Step:  2326 Loss:  1.7906262\n",
      "Training Step:  2327 Loss:  1.7897785\n",
      "Training Step:  2328 Loss:  1.7889295\n",
      "Training Step:  2329 Loss:  1.7880805\n",
      "Training Step:  2330 Loss:  1.7872337\n",
      "Training Step:  2331 Loss:  1.7863888\n",
      "Training Step:  2332 Loss:  1.7855444\n",
      "Training Step:  2333 Loss:  1.784697\n",
      "Training Step:  2334 Loss:  1.7838546\n",
      "Training Step:  2335 Loss:  1.7830112\n",
      "Training Step:  2336 Loss:  1.78217\n",
      "Training Step:  2337 Loss:  1.781325\n",
      "Training Step:  2338 Loss:  1.7804841\n",
      "Training Step:  2339 Loss:  1.7796443\n",
      "Training Step:  2340 Loss:  1.778803\n",
      "Training Step:  2341 Loss:  1.7779633\n",
      "Training Step:  2342 Loss:  1.7771249\n",
      "Training Step:  2343 Loss:  1.7762883\n",
      "Training Step:  2344 Loss:  1.7754499\n",
      "Training Step:  2345 Loss:  1.7746133\n",
      "Training Step:  2346 Loss:  1.7737775\n",
      "Training Step:  2347 Loss:  1.77294\n",
      "Training Step:  2348 Loss:  1.7721076\n",
      "Training Step:  2349 Loss:  1.7712717\n",
      "Training Step:  2350 Loss:  1.7704389\n",
      "Training Step:  2351 Loss:  1.7696047\n",
      "Training Step:  2352 Loss:  1.7687744\n",
      "Training Step:  2353 Loss:  1.7679437\n",
      "Training Step:  2354 Loss:  1.767113\n",
      "Training Step:  2355 Loss:  1.7662822\n",
      "Training Step:  2356 Loss:  1.7654537\n",
      "Training Step:  2357 Loss:  1.764624\n",
      "Training Step:  2358 Loss:  1.7637961\n",
      "Training Step:  2359 Loss:  1.7629693\n",
      "Training Step:  2360 Loss:  1.7621424\n",
      "Training Step:  2361 Loss:  1.761315\n",
      "Training Step:  2362 Loss:  1.7604916\n",
      "Training Step:  2363 Loss:  1.7596661\n",
      "Training Step:  2364 Loss:  1.7588407\n",
      "Training Step:  2365 Loss:  1.7580181\n",
      "Training Step:  2366 Loss:  1.7571956\n",
      "Training Step:  2367 Loss:  1.7563739\n",
      "Training Step:  2368 Loss:  1.7555526\n",
      "Training Step:  2369 Loss:  1.7547314\n",
      "Training Step:  2370 Loss:  1.7539113\n",
      "Training Step:  2371 Loss:  1.7530916\n",
      "Training Step:  2372 Loss:  1.7522725\n",
      "Training Step:  2373 Loss:  1.7514541\n",
      "Training Step:  2374 Loss:  1.7506363\n",
      "Training Step:  2375 Loss:  1.7498201\n",
      "Training Step:  2376 Loss:  1.7490034\n",
      "Training Step:  2377 Loss:  1.7481886\n",
      "Training Step:  2378 Loss:  1.7473755\n",
      "Training Step:  2379 Loss:  1.7465593\n",
      "Training Step:  2380 Loss:  1.7457459\n",
      "Training Step:  2381 Loss:  1.7449329\n",
      "Training Step:  2382 Loss:  1.7441205\n",
      "Training Step:  2383 Loss:  1.7433087\n",
      "Training Step:  2384 Loss:  1.7424974\n",
      "Training Step:  2385 Loss:  1.741688\n",
      "Training Step:  2386 Loss:  1.7408782\n",
      "Training Step:  2387 Loss:  1.7400689\n",
      "Training Step:  2388 Loss:  1.7392594\n",
      "Training Step:  2389 Loss:  1.7384522\n",
      "Training Step:  2390 Loss:  1.737644\n",
      "Training Step:  2391 Loss:  1.7368398\n",
      "Training Step:  2392 Loss:  1.7360333\n",
      "Training Step:  2393 Loss:  1.7352275\n",
      "Training Step:  2394 Loss:  1.7344233\n",
      "Training Step:  2395 Loss:  1.733619\n",
      "Training Step:  2396 Loss:  1.7328148\n",
      "Training Step:  2397 Loss:  1.7320133\n",
      "Training Step:  2398 Loss:  1.7312119\n",
      "Training Step:  2399 Loss:  1.7304105\n",
      "Training Step:  2400 Loss:  1.7296085\n",
      "Training Step:  2401 Loss:  1.7288095\n",
      "Training Step:  2402 Loss:  1.7280107\n",
      "Training Step:  2403 Loss:  1.727211\n",
      "Training Step:  2404 Loss:  1.7264136\n",
      "Training Step:  2405 Loss:  1.7256162\n",
      "Training Step:  2406 Loss:  1.7248178\n",
      "Training Step:  2407 Loss:  1.7240218\n",
      "Training Step:  2408 Loss:  1.7232264\n",
      "Training Step:  2409 Loss:  1.7224301\n",
      "Training Step:  2410 Loss:  1.7216377\n",
      "Training Step:  2411 Loss:  1.7208433\n",
      "Training Step:  2412 Loss:  1.7200514\n",
      "Training Step:  2413 Loss:  1.719259\n",
      "Training Step:  2414 Loss:  1.718468\n",
      "Training Step:  2415 Loss:  1.7176772\n",
      "Training Step:  2416 Loss:  1.7168853\n",
      "Training Step:  2417 Loss:  1.7160969\n",
      "Training Step:  2418 Loss:  1.7153064\n",
      "Training Step:  2419 Loss:  1.714516\n",
      "Training Step:  2420 Loss:  1.7137291\n",
      "Training Step:  2421 Loss:  1.7129405\n",
      "Training Step:  2422 Loss:  1.7121559\n",
      "Training Step:  2423 Loss:  1.7113682\n",
      "Training Step:  2424 Loss:  1.7105848\n",
      "Training Step:  2425 Loss:  1.7097993\n",
      "Training Step:  2426 Loss:  1.7090138\n",
      "Training Step:  2427 Loss:  1.7082319\n",
      "Training Step:  2428 Loss:  1.7074485\n",
      "Training Step:  2429 Loss:  1.7066658\n",
      "Training Step:  2430 Loss:  1.7058848\n",
      "Training Step:  2431 Loss:  1.7051028\n",
      "Training Step:  2432 Loss:  1.7043229\n",
      "Training Step:  2433 Loss:  1.7035425\n",
      "Training Step:  2434 Loss:  1.7027638\n",
      "Training Step:  2435 Loss:  1.701985\n",
      "Training Step:  2436 Loss:  1.7012066\n",
      "Training Step:  2437 Loss:  1.7004299\n",
      "Training Step:  2438 Loss:  1.6996522\n",
      "Training Step:  2439 Loss:  1.6988766\n",
      "Training Step:  2440 Loss:  1.6981014\n",
      "Training Step:  2441 Loss:  1.6973271\n",
      "Training Step:  2442 Loss:  1.6965525\n",
      "Training Step:  2443 Loss:  1.695779\n",
      "Training Step:  2444 Loss:  1.6950053\n",
      "Training Step:  2445 Loss:  1.694233\n",
      "Training Step:  2446 Loss:  1.6934611\n",
      "Training Step:  2447 Loss:  1.6926911\n",
      "Training Step:  2448 Loss:  1.6919198\n",
      "Training Step:  2449 Loss:  1.6911496\n",
      "Training Step:  2450 Loss:  1.6903799\n",
      "Training Step:  2451 Loss:  1.6896116\n",
      "Training Step:  2452 Loss:  1.6888433\n",
      "Training Step:  2453 Loss:  1.6880755\n",
      "Training Step:  2454 Loss:  1.6873099\n",
      "Training Step:  2455 Loss:  1.6865425\n",
      "Training Step:  2456 Loss:  1.6857777\n",
      "Training Step:  2457 Loss:  1.6850133\n",
      "Training Step:  2458 Loss:  1.6842496\n",
      "Training Step:  2459 Loss:  1.6834855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  2460 Loss:  1.6827219\n",
      "Training Step:  2461 Loss:  1.6819581\n",
      "Training Step:  2462 Loss:  1.6811967\n",
      "Training Step:  2463 Loss:  1.6804367\n",
      "Training Step:  2464 Loss:  1.679676\n",
      "Training Step:  2465 Loss:  1.6789145\n",
      "Training Step:  2466 Loss:  1.6781535\n",
      "Training Step:  2467 Loss:  1.6773963\n",
      "Training Step:  2468 Loss:  1.6766374\n",
      "Training Step:  2469 Loss:  1.6758801\n",
      "Training Step:  2470 Loss:  1.6751238\n",
      "Training Step:  2471 Loss:  1.6743668\n",
      "Training Step:  2472 Loss:  1.6736122\n",
      "Training Step:  2473 Loss:  1.672856\n",
      "Training Step:  2474 Loss:  1.6721026\n",
      "Training Step:  2475 Loss:  1.6713473\n",
      "Training Step:  2476 Loss:  1.6705928\n",
      "Training Step:  2477 Loss:  1.6698418\n",
      "Training Step:  2478 Loss:  1.6690874\n",
      "Training Step:  2479 Loss:  1.6683367\n",
      "Training Step:  2480 Loss:  1.6675855\n",
      "Training Step:  2481 Loss:  1.6668347\n",
      "Training Step:  2482 Loss:  1.6660862\n",
      "Training Step:  2483 Loss:  1.6653365\n",
      "Training Step:  2484 Loss:  1.6645879\n",
      "Training Step:  2485 Loss:  1.6638408\n",
      "Training Step:  2486 Loss:  1.6630931\n",
      "Training Step:  2487 Loss:  1.662345\n",
      "Training Step:  2488 Loss:  1.661598\n",
      "Training Step:  2489 Loss:  1.6608546\n",
      "Training Step:  2490 Loss:  1.6601074\n",
      "Training Step:  2491 Loss:  1.6593633\n",
      "Training Step:  2492 Loss:  1.6586196\n",
      "Training Step:  2493 Loss:  1.6578755\n",
      "Training Step:  2494 Loss:  1.6571345\n",
      "Training Step:  2495 Loss:  1.6563916\n",
      "Training Step:  2496 Loss:  1.6556487\n",
      "Training Step:  2497 Loss:  1.6549094\n",
      "Training Step:  2498 Loss:  1.6541694\n",
      "Training Step:  2499 Loss:  1.6534281\n",
      "Training Step:  2500 Loss:  1.6526899\n",
      "Training Step:  2501 Loss:  1.6519517\n",
      "Training Step:  2502 Loss:  1.6512134\n",
      "Training Step:  2503 Loss:  1.6504756\n",
      "Training Step:  2504 Loss:  1.64974\n",
      "Training Step:  2505 Loss:  1.6490039\n",
      "Training Step:  2506 Loss:  1.6482679\n",
      "Training Step:  2507 Loss:  1.6475328\n",
      "Training Step:  2508 Loss:  1.6467983\n",
      "Training Step:  2509 Loss:  1.6460645\n",
      "Training Step:  2510 Loss:  1.6453321\n",
      "Training Step:  2511 Loss:  1.6445984\n",
      "Training Step:  2512 Loss:  1.6438661\n",
      "Training Step:  2513 Loss:  1.6431339\n",
      "Training Step:  2514 Loss:  1.6424037\n",
      "Training Step:  2515 Loss:  1.6416726\n",
      "Training Step:  2516 Loss:  1.6409441\n",
      "Training Step:  2517 Loss:  1.6402144\n",
      "Training Step:  2518 Loss:  1.6394861\n",
      "Training Step:  2519 Loss:  1.6387575\n",
      "Training Step:  2520 Loss:  1.6380309\n",
      "Training Step:  2521 Loss:  1.637304\n",
      "Training Step:  2522 Loss:  1.6365775\n",
      "Training Step:  2523 Loss:  1.6358528\n",
      "Training Step:  2524 Loss:  1.6351259\n",
      "Training Step:  2525 Loss:  1.6344008\n",
      "Training Step:  2526 Loss:  1.6336787\n",
      "Training Step:  2527 Loss:  1.6329548\n",
      "Training Step:  2528 Loss:  1.6322327\n",
      "Training Step:  2529 Loss:  1.6315094\n",
      "Training Step:  2530 Loss:  1.6307884\n",
      "Training Step:  2531 Loss:  1.6300669\n",
      "Training Step:  2532 Loss:  1.6293461\n",
      "Training Step:  2533 Loss:  1.6286265\n",
      "Training Step:  2534 Loss:  1.6279061\n",
      "Training Step:  2535 Loss:  1.6271873\n",
      "Training Step:  2536 Loss:  1.6264693\n",
      "Training Step:  2537 Loss:  1.6257517\n",
      "Training Step:  2538 Loss:  1.625035\n",
      "Training Step:  2539 Loss:  1.6243184\n",
      "Training Step:  2540 Loss:  1.623603\n",
      "Training Step:  2541 Loss:  1.6228886\n",
      "Training Step:  2542 Loss:  1.622172\n",
      "Training Step:  2543 Loss:  1.621459\n",
      "Training Step:  2544 Loss:  1.6207466\n",
      "Training Step:  2545 Loss:  1.6200325\n",
      "Training Step:  2546 Loss:  1.6193199\n",
      "Training Step:  2547 Loss:  1.618607\n",
      "Training Step:  2548 Loss:  1.6178966\n",
      "Training Step:  2549 Loss:  1.617187\n",
      "Training Step:  2550 Loss:  1.6164756\n",
      "Training Step:  2551 Loss:  1.6157663\n",
      "Training Step:  2552 Loss:  1.6150562\n",
      "Training Step:  2553 Loss:  1.6143479\n",
      "Training Step:  2554 Loss:  1.61364\n",
      "Training Step:  2555 Loss:  1.6129335\n",
      "Training Step:  2556 Loss:  1.6122252\n",
      "Training Step:  2557 Loss:  1.61152\n",
      "Training Step:  2558 Loss:  1.6108139\n",
      "Training Step:  2559 Loss:  1.6101085\n",
      "Training Step:  2560 Loss:  1.6094037\n",
      "Training Step:  2561 Loss:  1.6086991\n",
      "Training Step:  2562 Loss:  1.6079955\n",
      "Training Step:  2563 Loss:  1.6072931\n",
      "Training Step:  2564 Loss:  1.6065915\n",
      "Training Step:  2565 Loss:  1.6058897\n",
      "Training Step:  2566 Loss:  1.605187\n",
      "Training Step:  2567 Loss:  1.6044902\n",
      "Training Step:  2568 Loss:  1.6037886\n",
      "Training Step:  2569 Loss:  1.6030884\n",
      "Training Step:  2570 Loss:  1.6023904\n",
      "Training Step:  2571 Loss:  1.6016924\n",
      "Training Step:  2572 Loss:  1.6009935\n",
      "Training Step:  2573 Loss:  1.6002963\n",
      "Training Step:  2574 Loss:  1.5995996\n",
      "Training Step:  2575 Loss:  1.5989041\n",
      "Training Step:  2576 Loss:  1.598207\n",
      "Training Step:  2577 Loss:  1.5975122\n",
      "Training Step:  2578 Loss:  1.5968183\n",
      "Training Step:  2579 Loss:  1.5961232\n",
      "Training Step:  2580 Loss:  1.5954323\n",
      "Training Step:  2581 Loss:  1.594738\n",
      "Training Step:  2582 Loss:  1.5940449\n",
      "Training Step:  2583 Loss:  1.5933528\n",
      "Training Step:  2584 Loss:  1.5926635\n",
      "Training Step:  2585 Loss:  1.5919715\n",
      "Training Step:  2586 Loss:  1.5912817\n",
      "Training Step:  2587 Loss:  1.5905908\n",
      "Training Step:  2588 Loss:  1.5899022\n",
      "Training Step:  2589 Loss:  1.589215\n",
      "Training Step:  2590 Loss:  1.588527\n",
      "Training Step:  2591 Loss:  1.5878392\n",
      "Training Step:  2592 Loss:  1.587151\n",
      "Training Step:  2593 Loss:  1.5864649\n",
      "Training Step:  2594 Loss:  1.5857805\n",
      "Training Step:  2595 Loss:  1.5850949\n",
      "Training Step:  2596 Loss:  1.5844101\n",
      "Training Step:  2597 Loss:  1.5837264\n",
      "Training Step:  2598 Loss:  1.5830429\n",
      "Training Step:  2599 Loss:  1.5823604\n",
      "Training Step:  2600 Loss:  1.5816779\n",
      "Training Step:  2601 Loss:  1.5809941\n",
      "Training Step:  2602 Loss:  1.5803136\n",
      "Training Step:  2603 Loss:  1.5796329\n",
      "Training Step:  2604 Loss:  1.5789523\n",
      "Training Step:  2605 Loss:  1.5782733\n",
      "Training Step:  2606 Loss:  1.5775945\n",
      "Training Step:  2607 Loss:  1.576915\n",
      "Training Step:  2608 Loss:  1.5762376\n",
      "Training Step:  2609 Loss:  1.5755591\n",
      "Training Step:  2610 Loss:  1.574883\n",
      "Training Step:  2611 Loss:  1.5742065\n",
      "Training Step:  2612 Loss:  1.5735306\n",
      "Training Step:  2613 Loss:  1.5728562\n",
      "Training Step:  2614 Loss:  1.5721799\n",
      "Training Step:  2615 Loss:  1.5715073\n",
      "Training Step:  2616 Loss:  1.5708339\n",
      "Training Step:  2617 Loss:  1.5701592\n",
      "Training Step:  2618 Loss:  1.5694879\n",
      "Training Step:  2619 Loss:  1.5688162\n",
      "Training Step:  2620 Loss:  1.5681437\n",
      "Training Step:  2621 Loss:  1.5674729\n",
      "Training Step:  2622 Loss:  1.5668019\n",
      "Training Step:  2623 Loss:  1.566132\n",
      "Training Step:  2624 Loss:  1.5654624\n",
      "Training Step:  2625 Loss:  1.5647935\n",
      "Training Step:  2626 Loss:  1.5641248\n",
      "Training Step:  2627 Loss:  1.5634565\n",
      "Training Step:  2628 Loss:  1.5627896\n",
      "Training Step:  2629 Loss:  1.5621231\n",
      "Training Step:  2630 Loss:  1.5614567\n",
      "Training Step:  2631 Loss:  1.5607923\n",
      "Training Step:  2632 Loss:  1.5601261\n",
      "Training Step:  2633 Loss:  1.5594621\n",
      "Training Step:  2634 Loss:  1.5587978\n",
      "Training Step:  2635 Loss:  1.5581335\n",
      "Training Step:  2636 Loss:  1.5574713\n",
      "Training Step:  2637 Loss:  1.5568091\n",
      "Training Step:  2638 Loss:  1.5561458\n",
      "Training Step:  2639 Loss:  1.5554843\n",
      "Training Step:  2640 Loss:  1.5548232\n",
      "Training Step:  2641 Loss:  1.5541621\n",
      "Training Step:  2642 Loss:  1.5535029\n",
      "Training Step:  2643 Loss:  1.5528439\n",
      "Training Step:  2644 Loss:  1.5521852\n",
      "Training Step:  2645 Loss:  1.5515271\n",
      "Training Step:  2646 Loss:  1.550869\n",
      "Training Step:  2647 Loss:  1.5502117\n",
      "Training Step:  2648 Loss:  1.5495536\n",
      "Training Step:  2649 Loss:  1.5488983\n",
      "Training Step:  2650 Loss:  1.548244\n",
      "Training Step:  2651 Loss:  1.5475873\n",
      "Training Step:  2652 Loss:  1.5469323\n",
      "Training Step:  2653 Loss:  1.5462797\n",
      "Training Step:  2654 Loss:  1.5456257\n",
      "Training Step:  2655 Loss:  1.5449721\n",
      "Training Step:  2656 Loss:  1.5443183\n",
      "Training Step:  2657 Loss:  1.5436679\n",
      "Training Step:  2658 Loss:  1.5430161\n",
      "Training Step:  2659 Loss:  1.5423644\n",
      "Training Step:  2660 Loss:  1.5417141\n",
      "Training Step:  2661 Loss:  1.5410645\n",
      "Training Step:  2662 Loss:  1.5404153\n",
      "Training Step:  2663 Loss:  1.5397664\n",
      "Training Step:  2664 Loss:  1.5391173\n",
      "Training Step:  2665 Loss:  1.5384699\n",
      "Training Step:  2666 Loss:  1.5378225\n",
      "Training Step:  2667 Loss:  1.5371755\n",
      "Training Step:  2668 Loss:  1.53653\n",
      "Training Step:  2669 Loss:  1.5358841\n",
      "Training Step:  2670 Loss:  1.5352389\n",
      "Training Step:  2671 Loss:  1.5345924\n",
      "Training Step:  2672 Loss:  1.5339484\n",
      "Training Step:  2673 Loss:  1.5333037\n",
      "Training Step:  2674 Loss:  1.5326618\n",
      "Training Step:  2675 Loss:  1.5320189\n",
      "Training Step:  2676 Loss:  1.5313764\n",
      "Training Step:  2677 Loss:  1.530735\n",
      "Training Step:  2678 Loss:  1.5300926\n",
      "Training Step:  2679 Loss:  1.5294532\n",
      "Training Step:  2680 Loss:  1.5288126\n",
      "Training Step:  2681 Loss:  1.5281726\n",
      "Training Step:  2682 Loss:  1.5275352\n",
      "Training Step:  2683 Loss:  1.526895\n",
      "Training Step:  2684 Loss:  1.5262569\n",
      "Training Step:  2685 Loss:  1.52562\n",
      "Training Step:  2686 Loss:  1.5249814\n",
      "Training Step:  2687 Loss:  1.5243458\n",
      "Training Step:  2688 Loss:  1.5237098\n",
      "Training Step:  2689 Loss:  1.5230724\n",
      "Training Step:  2690 Loss:  1.5224378\n",
      "Training Step:  2691 Loss:  1.521804\n",
      "Training Step:  2692 Loss:  1.5211684\n",
      "Training Step:  2693 Loss:  1.5205345\n",
      "Training Step:  2694 Loss:  1.5199032\n",
      "Training Step:  2695 Loss:  1.5192685\n",
      "Training Step:  2696 Loss:  1.5186365\n",
      "Training Step:  2697 Loss:  1.5180051\n",
      "Training Step:  2698 Loss:  1.5173734\n",
      "Training Step:  2699 Loss:  1.5167422\n",
      "Training Step:  2700 Loss:  1.516114\n",
      "Training Step:  2701 Loss:  1.5154834\n",
      "Training Step:  2702 Loss:  1.5148537\n",
      "Training Step:  2703 Loss:  1.5142252\n",
      "Training Step:  2704 Loss:  1.5135974\n",
      "Training Step:  2705 Loss:  1.5129697\n",
      "Training Step:  2706 Loss:  1.512343\n",
      "Training Step:  2707 Loss:  1.5117159\n",
      "Training Step:  2708 Loss:  1.5110902\n",
      "Training Step:  2709 Loss:  1.5104637\n",
      "Training Step:  2710 Loss:  1.5098379\n",
      "Training Step:  2711 Loss:  1.5092139\n",
      "Training Step:  2712 Loss:  1.5085893\n",
      "Training Step:  2713 Loss:  1.5079659\n",
      "Training Step:  2714 Loss:  1.507343\n",
      "Training Step:  2715 Loss:  1.506721\n",
      "Training Step:  2716 Loss:  1.5060976\n",
      "Training Step:  2717 Loss:  1.5054767\n",
      "Training Step:  2718 Loss:  1.5048559\n",
      "Training Step:  2719 Loss:  1.5042336\n",
      "Training Step:  2720 Loss:  1.5036159\n",
      "Training Step:  2721 Loss:  1.5029958\n",
      "Training Step:  2722 Loss:  1.5023768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  2723 Loss:  1.5017576\n",
      "Training Step:  2724 Loss:  1.5011394\n",
      "Training Step:  2725 Loss:  1.5005224\n",
      "Training Step:  2726 Loss:  1.4999042\n",
      "Training Step:  2727 Loss:  1.4992865\n",
      "Training Step:  2728 Loss:  1.498671\n",
      "Training Step:  2729 Loss:  1.4980553\n",
      "Training Step:  2730 Loss:  1.4974385\n",
      "Training Step:  2731 Loss:  1.4968257\n",
      "Training Step:  2732 Loss:  1.4962103\n",
      "Training Step:  2733 Loss:  1.4955974\n",
      "Training Step:  2734 Loss:  1.4949837\n",
      "Training Step:  2735 Loss:  1.4943708\n",
      "Training Step:  2736 Loss:  1.4937578\n",
      "Training Step:  2737 Loss:  1.4931463\n",
      "Training Step:  2738 Loss:  1.4925342\n",
      "Training Step:  2739 Loss:  1.4919233\n",
      "Training Step:  2740 Loss:  1.4913136\n",
      "Training Step:  2741 Loss:  1.4907027\n",
      "Training Step:  2742 Loss:  1.4900942\n",
      "Training Step:  2743 Loss:  1.4894843\n",
      "Training Step:  2744 Loss:  1.4888768\n",
      "Training Step:  2745 Loss:  1.4882681\n",
      "Training Step:  2746 Loss:  1.4876616\n",
      "Training Step:  2747 Loss:  1.4870552\n",
      "Training Step:  2748 Loss:  1.4864472\n",
      "Training Step:  2749 Loss:  1.4858418\n",
      "Training Step:  2750 Loss:  1.4852359\n",
      "Training Step:  2751 Loss:  1.4846311\n",
      "Training Step:  2752 Loss:  1.4840274\n",
      "Training Step:  2753 Loss:  1.4834225\n",
      "Training Step:  2754 Loss:  1.482819\n",
      "Training Step:  2755 Loss:  1.4822152\n",
      "Training Step:  2756 Loss:  1.481612\n",
      "Training Step:  2757 Loss:  1.4810102\n",
      "Training Step:  2758 Loss:  1.4804094\n",
      "Training Step:  2759 Loss:  1.4798065\n",
      "Training Step:  2760 Loss:  1.4792063\n",
      "Training Step:  2761 Loss:  1.4786054\n",
      "Training Step:  2762 Loss:  1.4780077\n",
      "Training Step:  2763 Loss:  1.4774063\n",
      "Training Step:  2764 Loss:  1.4768087\n",
      "Training Step:  2765 Loss:  1.4762108\n",
      "Training Step:  2766 Loss:  1.475612\n",
      "Training Step:  2767 Loss:  1.4750154\n",
      "Training Step:  2768 Loss:  1.4744172\n",
      "Training Step:  2769 Loss:  1.4738213\n",
      "Training Step:  2770 Loss:  1.4732254\n",
      "Training Step:  2771 Loss:  1.4726295\n",
      "Training Step:  2772 Loss:  1.4720349\n",
      "Training Step:  2773 Loss:  1.4714404\n",
      "Training Step:  2774 Loss:  1.4708447\n",
      "Training Step:  2775 Loss:  1.4702535\n",
      "Training Step:  2776 Loss:  1.469659\n",
      "Training Step:  2777 Loss:  1.4690665\n",
      "Training Step:  2778 Loss:  1.4684744\n",
      "Training Step:  2779 Loss:  1.4678819\n",
      "Training Step:  2780 Loss:  1.4672902\n",
      "Training Step:  2781 Loss:  1.4667001\n",
      "Training Step:  2782 Loss:  1.4661099\n",
      "Training Step:  2783 Loss:  1.4655211\n",
      "Training Step:  2784 Loss:  1.4649296\n",
      "Training Step:  2785 Loss:  1.4643409\n",
      "Training Step:  2786 Loss:  1.4637517\n",
      "Training Step:  2787 Loss:  1.4631646\n",
      "Training Step:  2788 Loss:  1.4625782\n",
      "Training Step:  2789 Loss:  1.4619899\n",
      "Training Step:  2790 Loss:  1.4614034\n",
      "Training Step:  2791 Loss:  1.4608178\n",
      "Training Step:  2792 Loss:  1.4602315\n",
      "Training Step:  2793 Loss:  1.4596463\n",
      "Training Step:  2794 Loss:  1.4590626\n",
      "Training Step:  2795 Loss:  1.4584773\n",
      "Training Step:  2796 Loss:  1.4578931\n",
      "Training Step:  2797 Loss:  1.4573096\n",
      "Training Step:  2798 Loss:  1.4567285\n",
      "Training Step:  2799 Loss:  1.4561449\n",
      "Training Step:  2800 Loss:  1.4555626\n",
      "Training Step:  2801 Loss:  1.4549804\n",
      "Training Step:  2802 Loss:  1.4544004\n",
      "Training Step:  2803 Loss:  1.45382\n",
      "Training Step:  2804 Loss:  1.4532391\n",
      "Training Step:  2805 Loss:  1.4526602\n",
      "Training Step:  2806 Loss:  1.4520813\n",
      "Training Step:  2807 Loss:  1.4515011\n",
      "Training Step:  2808 Loss:  1.4509243\n",
      "Training Step:  2809 Loss:  1.4503464\n",
      "Training Step:  2810 Loss:  1.4497693\n",
      "Training Step:  2811 Loss:  1.4491911\n",
      "Training Step:  2812 Loss:  1.4486141\n",
      "Training Step:  2813 Loss:  1.4480392\n",
      "Training Step:  2814 Loss:  1.4474632\n",
      "Training Step:  2815 Loss:  1.4468881\n",
      "Training Step:  2816 Loss:  1.4463141\n",
      "Training Step:  2817 Loss:  1.4457393\n",
      "Training Step:  2818 Loss:  1.4451656\n",
      "Training Step:  2819 Loss:  1.4445926\n",
      "Training Step:  2820 Loss:  1.4440205\n",
      "Training Step:  2821 Loss:  1.4434476\n",
      "Training Step:  2822 Loss:  1.442876\n",
      "Training Step:  2823 Loss:  1.4423034\n",
      "Training Step:  2824 Loss:  1.4417335\n",
      "Training Step:  2825 Loss:  1.4411619\n",
      "Training Step:  2826 Loss:  1.4405925\n",
      "Training Step:  2827 Loss:  1.4400222\n",
      "Training Step:  2828 Loss:  1.4394526\n",
      "Training Step:  2829 Loss:  1.438885\n",
      "Training Step:  2830 Loss:  1.4383165\n",
      "Training Step:  2831 Loss:  1.4377475\n",
      "Training Step:  2832 Loss:  1.4371823\n",
      "Training Step:  2833 Loss:  1.4366155\n",
      "Training Step:  2834 Loss:  1.4360478\n",
      "Training Step:  2835 Loss:  1.4354821\n",
      "Training Step:  2836 Loss:  1.4349159\n",
      "Training Step:  2837 Loss:  1.4343514\n",
      "Training Step:  2838 Loss:  1.4337851\n",
      "Training Step:  2839 Loss:  1.4332227\n",
      "Training Step:  2840 Loss:  1.4326587\n",
      "Training Step:  2841 Loss:  1.4320949\n",
      "Training Step:  2842 Loss:  1.4315323\n",
      "Training Step:  2843 Loss:  1.4309698\n",
      "Training Step:  2844 Loss:  1.4304091\n",
      "Training Step:  2845 Loss:  1.4298458\n",
      "Training Step:  2846 Loss:  1.4292847\n",
      "Training Step:  2847 Loss:  1.428725\n",
      "Training Step:  2848 Loss:  1.4281645\n",
      "Training Step:  2849 Loss:  1.4276049\n",
      "Training Step:  2850 Loss:  1.427045\n",
      "Training Step:  2851 Loss:  1.4264866\n",
      "Training Step:  2852 Loss:  1.4259287\n",
      "Training Step:  2853 Loss:  1.42537\n",
      "Training Step:  2854 Loss:  1.4248135\n",
      "Training Step:  2855 Loss:  1.4242551\n",
      "Training Step:  2856 Loss:  1.4236989\n",
      "Training Step:  2857 Loss:  1.4231426\n",
      "Training Step:  2858 Loss:  1.4225873\n",
      "Training Step:  2859 Loss:  1.4220309\n",
      "Training Step:  2860 Loss:  1.4214774\n",
      "Training Step:  2861 Loss:  1.4209211\n",
      "Training Step:  2862 Loss:  1.4203682\n",
      "Training Step:  2863 Loss:  1.4198146\n",
      "Training Step:  2864 Loss:  1.4192604\n",
      "Training Step:  2865 Loss:  1.4187083\n",
      "Training Step:  2866 Loss:  1.418156\n",
      "Training Step:  2867 Loss:  1.417604\n",
      "Training Step:  2868 Loss:  1.4170525\n",
      "Training Step:  2869 Loss:  1.4165013\n",
      "Training Step:  2870 Loss:  1.4159513\n",
      "Training Step:  2871 Loss:  1.4154011\n",
      "Training Step:  2872 Loss:  1.4148514\n",
      "Training Step:  2873 Loss:  1.4143034\n",
      "Training Step:  2874 Loss:  1.4137535\n",
      "Training Step:  2875 Loss:  1.4132043\n",
      "Training Step:  2876 Loss:  1.4126577\n",
      "Training Step:  2877 Loss:  1.412109\n",
      "Training Step:  2878 Loss:  1.4115628\n",
      "Training Step:  2879 Loss:  1.4110155\n",
      "Training Step:  2880 Loss:  1.4104711\n",
      "Training Step:  2881 Loss:  1.409925\n",
      "Training Step:  2882 Loss:  1.4093802\n",
      "Training Step:  2883 Loss:  1.4088346\n",
      "Training Step:  2884 Loss:  1.4082886\n",
      "Training Step:  2885 Loss:  1.4077464\n",
      "Training Step:  2886 Loss:  1.4072025\n",
      "Training Step:  2887 Loss:  1.4066589\n",
      "Training Step:  2888 Loss:  1.4061171\n",
      "Training Step:  2889 Loss:  1.4055749\n",
      "Training Step:  2890 Loss:  1.4050319\n",
      "Training Step:  2891 Loss:  1.4044904\n",
      "Training Step:  2892 Loss:  1.40395\n",
      "Training Step:  2893 Loss:  1.4034076\n",
      "Training Step:  2894 Loss:  1.4028709\n",
      "Training Step:  2895 Loss:  1.4023298\n",
      "Training Step:  2896 Loss:  1.4017898\n",
      "Training Step:  2897 Loss:  1.4012517\n",
      "Training Step:  2898 Loss:  1.4007134\n",
      "Training Step:  2899 Loss:  1.4001756\n",
      "Training Step:  2900 Loss:  1.3996391\n",
      "Training Step:  2901 Loss:  1.3991015\n",
      "Training Step:  2902 Loss:  1.3985643\n",
      "Training Step:  2903 Loss:  1.3980274\n",
      "Training Step:  2904 Loss:  1.3974905\n",
      "Training Step:  2905 Loss:  1.3969551\n",
      "Training Step:  2906 Loss:  1.3964207\n",
      "Training Step:  2907 Loss:  1.3958864\n",
      "Training Step:  2908 Loss:  1.3953527\n",
      "Training Step:  2909 Loss:  1.3948185\n",
      "Training Step:  2910 Loss:  1.3942848\n",
      "Training Step:  2911 Loss:  1.3937526\n",
      "Training Step:  2912 Loss:  1.3932192\n",
      "Training Step:  2913 Loss:  1.3926879\n",
      "Training Step:  2914 Loss:  1.3921559\n",
      "Training Step:  2915 Loss:  1.3916242\n",
      "Training Step:  2916 Loss:  1.3910947\n",
      "Training Step:  2917 Loss:  1.3905628\n",
      "Training Step:  2918 Loss:  1.3900326\n",
      "Training Step:  2919 Loss:  1.3895043\n",
      "Training Step:  2920 Loss:  1.3889741\n",
      "Training Step:  2921 Loss:  1.3884454\n",
      "Training Step:  2922 Loss:  1.3879178\n",
      "Training Step:  2923 Loss:  1.3873905\n",
      "Training Step:  2924 Loss:  1.3868634\n",
      "Training Step:  2925 Loss:  1.3863353\n",
      "Training Step:  2926 Loss:  1.3858086\n",
      "Training Step:  2927 Loss:  1.3852832\n",
      "Training Step:  2928 Loss:  1.3847562\n",
      "Training Step:  2929 Loss:  1.3842317\n",
      "Training Step:  2930 Loss:  1.3837075\n",
      "Training Step:  2931 Loss:  1.383183\n",
      "Training Step:  2932 Loss:  1.382659\n",
      "Training Step:  2933 Loss:  1.3821347\n",
      "Training Step:  2934 Loss:  1.3816113\n",
      "Training Step:  2935 Loss:  1.3810874\n",
      "Training Step:  2936 Loss:  1.3805661\n",
      "Training Step:  2937 Loss:  1.3800448\n",
      "Training Step:  2938 Loss:  1.3795223\n",
      "Training Step:  2939 Loss:  1.3790016\n",
      "Training Step:  2940 Loss:  1.3784807\n",
      "Training Step:  2941 Loss:  1.3779607\n",
      "Training Step:  2942 Loss:  1.3774407\n",
      "Training Step:  2943 Loss:  1.3769201\n",
      "Training Step:  2944 Loss:  1.3764019\n",
      "Training Step:  2945 Loss:  1.3758821\n",
      "Training Step:  2946 Loss:  1.3753638\n",
      "Training Step:  2947 Loss:  1.3748461\n",
      "Training Step:  2948 Loss:  1.3743291\n",
      "Training Step:  2949 Loss:  1.373812\n",
      "Training Step:  2950 Loss:  1.3732953\n",
      "Training Step:  2951 Loss:  1.3727788\n",
      "Training Step:  2952 Loss:  1.3722626\n",
      "Training Step:  2953 Loss:  1.3717471\n",
      "Training Step:  2954 Loss:  1.3712318\n",
      "Training Step:  2955 Loss:  1.3707182\n",
      "Training Step:  2956 Loss:  1.370202\n",
      "Training Step:  2957 Loss:  1.36969\n",
      "Training Step:  2958 Loss:  1.3691765\n",
      "Training Step:  2959 Loss:  1.3686639\n",
      "Training Step:  2960 Loss:  1.3681507\n",
      "Training Step:  2961 Loss:  1.3676381\n",
      "Training Step:  2962 Loss:  1.3671263\n",
      "Training Step:  2963 Loss:  1.3666153\n",
      "Training Step:  2964 Loss:  1.3661053\n",
      "Training Step:  2965 Loss:  1.3655945\n",
      "Training Step:  2966 Loss:  1.3650829\n",
      "Training Step:  2967 Loss:  1.3645736\n",
      "Training Step:  2968 Loss:  1.3640636\n",
      "Training Step:  2969 Loss:  1.3635556\n",
      "Training Step:  2970 Loss:  1.3630476\n",
      "Training Step:  2971 Loss:  1.3625381\n",
      "Training Step:  2972 Loss:  1.3620315\n",
      "Training Step:  2973 Loss:  1.3615237\n",
      "Training Step:  2974 Loss:  1.3610171\n",
      "Training Step:  2975 Loss:  1.3605103\n",
      "Training Step:  2976 Loss:  1.360004\n",
      "Training Step:  2977 Loss:  1.3594989\n",
      "Training Step:  2978 Loss:  1.3589922\n",
      "Training Step:  2979 Loss:  1.3584881\n",
      "Training Step:  2980 Loss:  1.3579817\n",
      "Training Step:  2981 Loss:  1.3574783\n",
      "Training Step:  2982 Loss:  1.3569752\n",
      "Training Step:  2983 Loss:  1.3564711\n",
      "Training Step:  2984 Loss:  1.355969\n",
      "Training Step:  2985 Loss:  1.3554649\n",
      "Training Step:  2986 Loss:  1.3549633\n",
      "Training Step:  2987 Loss:  1.3544621\n",
      "Training Step:  2988 Loss:  1.353961\n",
      "Training Step:  2989 Loss:  1.3534607\n",
      "Training Step:  2990 Loss:  1.3529589\n",
      "Training Step:  2991 Loss:  1.3524587\n",
      "Training Step:  2992 Loss:  1.3519595\n",
      "Training Step:  2993 Loss:  1.3514607\n",
      "Training Step:  2994 Loss:  1.3509607\n",
      "Training Step:  2995 Loss:  1.3504616\n",
      "Training Step:  2996 Loss:  1.3499645\n",
      "Training Step:  2997 Loss:  1.3494661\n",
      "Training Step:  2998 Loss:  1.3489692\n",
      "Training Step:  2999 Loss:  1.3484724\n",
      "Training Step:  3000 Loss:  1.3479753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  3001 Loss:  1.3474787\n",
      "Training Step:  3002 Loss:  1.3469827\n",
      "Training Step:  3003 Loss:  1.3464874\n",
      "Training Step:  3004 Loss:  1.3459935\n",
      "Training Step:  3005 Loss:  1.3454981\n",
      "Training Step:  3006 Loss:  1.3450035\n",
      "Training Step:  3007 Loss:  1.3445098\n",
      "Training Step:  3008 Loss:  1.3440157\n",
      "Training Step:  3009 Loss:  1.3435235\n",
      "Training Step:  3010 Loss:  1.3430312\n",
      "Training Step:  3011 Loss:  1.3425372\n",
      "Training Step:  3012 Loss:  1.3420461\n",
      "Training Step:  3013 Loss:  1.3415543\n",
      "Training Step:  3014 Loss:  1.3410628\n",
      "Training Step:  3015 Loss:  1.3405728\n",
      "Training Step:  3016 Loss:  1.3400835\n",
      "Training Step:  3017 Loss:  1.3395915\n",
      "Training Step:  3018 Loss:  1.3391037\n",
      "Training Step:  3019 Loss:  1.3386122\n",
      "Training Step:  3020 Loss:  1.338124\n",
      "Training Step:  3021 Loss:  1.3376348\n",
      "Training Step:  3022 Loss:  1.3371469\n",
      "Training Step:  3023 Loss:  1.3366592\n",
      "Training Step:  3024 Loss:  1.3361725\n",
      "Training Step:  3025 Loss:  1.3356847\n",
      "Training Step:  3026 Loss:  1.3351992\n",
      "Training Step:  3027 Loss:  1.334712\n",
      "Training Step:  3028 Loss:  1.3342268\n",
      "Training Step:  3029 Loss:  1.3337418\n",
      "Training Step:  3030 Loss:  1.3332567\n",
      "Training Step:  3031 Loss:  1.3327719\n",
      "Training Step:  3032 Loss:  1.3322883\n",
      "Training Step:  3033 Loss:  1.3318049\n",
      "Training Step:  3034 Loss:  1.3313196\n",
      "Training Step:  3035 Loss:  1.3308384\n",
      "Training Step:  3036 Loss:  1.3303552\n",
      "Training Step:  3037 Loss:  1.3298737\n",
      "Training Step:  3038 Loss:  1.3293906\n",
      "Training Step:  3039 Loss:  1.3289087\n",
      "Training Step:  3040 Loss:  1.3284276\n",
      "Training Step:  3041 Loss:  1.3279471\n",
      "Training Step:  3042 Loss:  1.3274664\n",
      "Training Step:  3043 Loss:  1.3269877\n",
      "Training Step:  3044 Loss:  1.3265071\n",
      "Training Step:  3045 Loss:  1.3260283\n",
      "Training Step:  3046 Loss:  1.3255491\n",
      "Training Step:  3047 Loss:  1.325071\n",
      "Training Step:  3048 Loss:  1.3245925\n",
      "Training Step:  3049 Loss:  1.3241142\n",
      "Training Step:  3050 Loss:  1.3236368\n",
      "Training Step:  3051 Loss:  1.3231604\n",
      "Training Step:  3052 Loss:  1.3226839\n",
      "Training Step:  3053 Loss:  1.3222077\n",
      "Training Step:  3054 Loss:  1.3217313\n",
      "Training Step:  3055 Loss:  1.3212558\n",
      "Training Step:  3056 Loss:  1.3207792\n",
      "Training Step:  3057 Loss:  1.3203053\n",
      "Training Step:  3058 Loss:  1.3198316\n",
      "Training Step:  3059 Loss:  1.3193579\n",
      "Training Step:  3060 Loss:  1.3188837\n",
      "Training Step:  3061 Loss:  1.31841\n",
      "Training Step:  3062 Loss:  1.3179386\n",
      "Training Step:  3063 Loss:  1.3174654\n",
      "Training Step:  3064 Loss:  1.3169925\n",
      "Training Step:  3065 Loss:  1.3165212\n",
      "Training Step:  3066 Loss:  1.3160497\n",
      "Training Step:  3067 Loss:  1.3155783\n",
      "Training Step:  3068 Loss:  1.3151081\n",
      "Training Step:  3069 Loss:  1.3146371\n",
      "Training Step:  3070 Loss:  1.3141674\n",
      "Training Step:  3071 Loss:  1.3136982\n",
      "Training Step:  3072 Loss:  1.3132291\n",
      "Training Step:  3073 Loss:  1.3127596\n",
      "Training Step:  3074 Loss:  1.3122916\n",
      "Training Step:  3075 Loss:  1.3118235\n",
      "Training Step:  3076 Loss:  1.3113556\n",
      "Training Step:  3077 Loss:  1.3108885\n",
      "Training Step:  3078 Loss:  1.3104218\n",
      "Training Step:  3079 Loss:  1.3099556\n",
      "Training Step:  3080 Loss:  1.309488\n",
      "Training Step:  3081 Loss:  1.3090228\n",
      "Training Step:  3082 Loss:  1.3085579\n",
      "Training Step:  3083 Loss:  1.308093\n",
      "Training Step:  3084 Loss:  1.3076276\n",
      "Training Step:  3085 Loss:  1.3071642\n",
      "Training Step:  3086 Loss:  1.3066983\n",
      "Training Step:  3087 Loss:  1.3062358\n",
      "Training Step:  3088 Loss:  1.3057723\n",
      "Training Step:  3089 Loss:  1.305309\n",
      "Training Step:  3090 Loss:  1.3048472\n",
      "Training Step:  3091 Loss:  1.3043841\n",
      "Training Step:  3092 Loss:  1.3039224\n",
      "Training Step:  3093 Loss:  1.3034618\n",
      "Training Step:  3094 Loss:  1.3030005\n",
      "Training Step:  3095 Loss:  1.3025389\n",
      "Training Step:  3096 Loss:  1.3020794\n",
      "Training Step:  3097 Loss:  1.3016196\n",
      "Training Step:  3098 Loss:  1.3011597\n",
      "Training Step:  3099 Loss:  1.3006997\n",
      "Training Step:  3100 Loss:  1.3002414\n",
      "Training Step:  3101 Loss:  1.2997822\n",
      "Training Step:  3102 Loss:  1.299324\n",
      "Training Step:  3103 Loss:  1.2988663\n",
      "Training Step:  3104 Loss:  1.2984098\n",
      "Training Step:  3105 Loss:  1.2979527\n",
      "Training Step:  3106 Loss:  1.2974954\n",
      "Training Step:  3107 Loss:  1.2970386\n",
      "Training Step:  3108 Loss:  1.2965825\n",
      "Training Step:  3109 Loss:  1.2961268\n",
      "Training Step:  3110 Loss:  1.2956717\n",
      "Training Step:  3111 Loss:  1.2952168\n",
      "Training Step:  3112 Loss:  1.294762\n",
      "Training Step:  3113 Loss:  1.2943077\n",
      "Training Step:  3114 Loss:  1.2938539\n",
      "Training Step:  3115 Loss:  1.2933998\n",
      "Training Step:  3116 Loss:  1.2929473\n",
      "Training Step:  3117 Loss:  1.2924944\n",
      "Training Step:  3118 Loss:  1.2920417\n",
      "Training Step:  3119 Loss:  1.2915906\n",
      "Training Step:  3120 Loss:  1.2911392\n",
      "Training Step:  3121 Loss:  1.2906874\n",
      "Training Step:  3122 Loss:  1.2902365\n",
      "Training Step:  3123 Loss:  1.2897847\n",
      "Training Step:  3124 Loss:  1.289336\n",
      "Training Step:  3125 Loss:  1.2888854\n",
      "Training Step:  3126 Loss:  1.2884359\n",
      "Training Step:  3127 Loss:  1.287986\n",
      "Training Step:  3128 Loss:  1.2875378\n",
      "Training Step:  3129 Loss:  1.2870891\n",
      "Training Step:  3130 Loss:  1.2866404\n",
      "Training Step:  3131 Loss:  1.2861933\n",
      "Training Step:  3132 Loss:  1.2857442\n",
      "Training Step:  3133 Loss:  1.2852993\n",
      "Training Step:  3134 Loss:  1.2848516\n",
      "Training Step:  3135 Loss:  1.2844043\n",
      "Training Step:  3136 Loss:  1.2839593\n",
      "Training Step:  3137 Loss:  1.283514\n",
      "Training Step:  3138 Loss:  1.2830682\n",
      "Training Step:  3139 Loss:  1.2826227\n",
      "Training Step:  3140 Loss:  1.2821777\n",
      "Training Step:  3141 Loss:  1.2817345\n",
      "Training Step:  3142 Loss:  1.28129\n",
      "Training Step:  3143 Loss:  1.2808458\n",
      "Training Step:  3144 Loss:  1.2804035\n",
      "Training Step:  3145 Loss:  1.2799597\n",
      "Training Step:  3146 Loss:  1.2795167\n",
      "Training Step:  3147 Loss:  1.2790766\n",
      "Training Step:  3148 Loss:  1.2786341\n",
      "Training Step:  3149 Loss:  1.2781923\n",
      "Training Step:  3150 Loss:  1.2777508\n",
      "Training Step:  3151 Loss:  1.2773092\n",
      "Training Step:  3152 Loss:  1.2768688\n",
      "Training Step:  3153 Loss:  1.2764302\n",
      "Training Step:  3154 Loss:  1.2759907\n",
      "Training Step:  3155 Loss:  1.2755494\n",
      "Training Step:  3156 Loss:  1.2751105\n",
      "Training Step:  3157 Loss:  1.274673\n",
      "Training Step:  3158 Loss:  1.274234\n",
      "Training Step:  3159 Loss:  1.2737952\n",
      "Training Step:  3160 Loss:  1.2733575\n",
      "Training Step:  3161 Loss:  1.2729205\n",
      "Training Step:  3162 Loss:  1.2724843\n",
      "Training Step:  3163 Loss:  1.2720469\n",
      "Training Step:  3164 Loss:  1.2716109\n",
      "Training Step:  3165 Loss:  1.2711754\n",
      "Training Step:  3166 Loss:  1.270739\n",
      "Training Step:  3167 Loss:  1.2703037\n",
      "Training Step:  3168 Loss:  1.2698689\n",
      "Training Step:  3169 Loss:  1.2694345\n",
      "Training Step:  3170 Loss:  1.2689989\n",
      "Training Step:  3171 Loss:  1.2685659\n",
      "Training Step:  3172 Loss:  1.2681317\n",
      "Training Step:  3173 Loss:  1.267699\n",
      "Training Step:  3174 Loss:  1.2672659\n",
      "Training Step:  3175 Loss:  1.2668346\n",
      "Training Step:  3176 Loss:  1.2663994\n",
      "Training Step:  3177 Loss:  1.2659683\n",
      "Training Step:  3178 Loss:  1.2655385\n",
      "Training Step:  3179 Loss:  1.265107\n",
      "Training Step:  3180 Loss:  1.2646765\n",
      "Training Step:  3181 Loss:  1.2642446\n",
      "Training Step:  3182 Loss:  1.263815\n",
      "Training Step:  3183 Loss:  1.2633852\n",
      "Training Step:  3184 Loss:  1.262956\n",
      "Training Step:  3185 Loss:  1.2625258\n",
      "Training Step:  3186 Loss:  1.2620971\n",
      "Training Step:  3187 Loss:  1.2616689\n",
      "Training Step:  3188 Loss:  1.2612412\n",
      "Training Step:  3189 Loss:  1.2608132\n",
      "Training Step:  3190 Loss:  1.2603859\n",
      "Training Step:  3191 Loss:  1.2599573\n",
      "Training Step:  3192 Loss:  1.259531\n",
      "Training Step:  3193 Loss:  1.259106\n",
      "Training Step:  3194 Loss:  1.2586784\n",
      "Training Step:  3195 Loss:  1.2582531\n",
      "Training Step:  3196 Loss:  1.2578273\n",
      "Training Step:  3197 Loss:  1.2574018\n",
      "Training Step:  3198 Loss:  1.2569778\n",
      "Training Step:  3199 Loss:  1.2565532\n",
      "Training Step:  3200 Loss:  1.2561283\n",
      "Training Step:  3201 Loss:  1.2557046\n",
      "Training Step:  3202 Loss:  1.2552819\n",
      "Training Step:  3203 Loss:  1.2548575\n",
      "Training Step:  3204 Loss:  1.2544351\n",
      "Training Step:  3205 Loss:  1.2540119\n",
      "Training Step:  3206 Loss:  1.2535912\n",
      "Training Step:  3207 Loss:  1.2531679\n",
      "Training Step:  3208 Loss:  1.2527474\n",
      "Training Step:  3209 Loss:  1.2523261\n",
      "Training Step:  3210 Loss:  1.2519066\n",
      "Training Step:  3211 Loss:  1.251485\n",
      "Training Step:  3212 Loss:  1.2510632\n",
      "Training Step:  3213 Loss:  1.2506447\n",
      "Training Step:  3214 Loss:  1.2502245\n",
      "Training Step:  3215 Loss:  1.249806\n",
      "Training Step:  3216 Loss:  1.249387\n",
      "Training Step:  3217 Loss:  1.2489688\n",
      "Training Step:  3218 Loss:  1.2485512\n",
      "Training Step:  3219 Loss:  1.2481328\n",
      "Training Step:  3220 Loss:  1.247715\n",
      "Training Step:  3221 Loss:  1.2472985\n",
      "Training Step:  3222 Loss:  1.2468814\n",
      "Training Step:  3223 Loss:  1.2464638\n",
      "Training Step:  3224 Loss:  1.246049\n",
      "Training Step:  3225 Loss:  1.2456322\n",
      "Training Step:  3226 Loss:  1.2452171\n",
      "Training Step:  3227 Loss:  1.2448022\n",
      "Training Step:  3228 Loss:  1.2443871\n",
      "Training Step:  3229 Loss:  1.2439721\n",
      "Training Step:  3230 Loss:  1.2435575\n",
      "Training Step:  3231 Loss:  1.2431437\n",
      "Training Step:  3232 Loss:  1.2427313\n",
      "Training Step:  3233 Loss:  1.2423165\n",
      "Training Step:  3234 Loss:  1.241904\n",
      "Training Step:  3235 Loss:  1.2414908\n",
      "Training Step:  3236 Loss:  1.2410779\n",
      "Training Step:  3237 Loss:  1.2406676\n",
      "Training Step:  3238 Loss:  1.240256\n",
      "Training Step:  3239 Loss:  1.2398444\n",
      "Training Step:  3240 Loss:  1.2394328\n",
      "Training Step:  3241 Loss:  1.2390232\n",
      "Training Step:  3242 Loss:  1.2386119\n",
      "Training Step:  3243 Loss:  1.2382011\n",
      "Training Step:  3244 Loss:  1.2377931\n",
      "Training Step:  3245 Loss:  1.2373829\n",
      "Training Step:  3246 Loss:  1.2369741\n",
      "Training Step:  3247 Loss:  1.2365646\n",
      "Training Step:  3248 Loss:  1.236156\n",
      "Training Step:  3249 Loss:  1.2357495\n",
      "Training Step:  3250 Loss:  1.2353411\n",
      "Training Step:  3251 Loss:  1.234933\n",
      "Training Step:  3252 Loss:  1.2345262\n",
      "Training Step:  3253 Loss:  1.2341189\n",
      "Training Step:  3254 Loss:  1.2337133\n",
      "Training Step:  3255 Loss:  1.2333071\n",
      "Training Step:  3256 Loss:  1.2329006\n",
      "Training Step:  3257 Loss:  1.232495\n",
      "Training Step:  3258 Loss:  1.2320892\n",
      "Training Step:  3259 Loss:  1.231684\n",
      "Training Step:  3260 Loss:  1.2312794\n",
      "Training Step:  3261 Loss:  1.2308751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  3262 Loss:  1.2304721\n",
      "Training Step:  3263 Loss:  1.230068\n",
      "Training Step:  3264 Loss:  1.2296652\n",
      "Training Step:  3265 Loss:  1.2292618\n",
      "Training Step:  3266 Loss:  1.2288601\n",
      "Training Step:  3267 Loss:  1.2284561\n",
      "Training Step:  3268 Loss:  1.2280548\n",
      "Training Step:  3269 Loss:  1.2276527\n",
      "Training Step:  3270 Loss:  1.2272514\n",
      "Training Step:  3271 Loss:  1.22685\n",
      "Training Step:  3272 Loss:  1.2264487\n",
      "Training Step:  3273 Loss:  1.2260484\n",
      "Training Step:  3274 Loss:  1.2256485\n",
      "Training Step:  3275 Loss:  1.2252493\n",
      "Training Step:  3276 Loss:  1.2248486\n",
      "Training Step:  3277 Loss:  1.2244501\n",
      "Training Step:  3278 Loss:  1.2240509\n",
      "Training Step:  3279 Loss:  1.2236528\n",
      "Training Step:  3280 Loss:  1.2232536\n",
      "Training Step:  3281 Loss:  1.2228568\n",
      "Training Step:  3282 Loss:  1.2224594\n",
      "Training Step:  3283 Loss:  1.2220612\n",
      "Training Step:  3284 Loss:  1.2216642\n",
      "Training Step:  3285 Loss:  1.2212675\n",
      "Training Step:  3286 Loss:  1.220871\n",
      "Training Step:  3287 Loss:  1.2204748\n",
      "Training Step:  3288 Loss:  1.2200792\n",
      "Training Step:  3289 Loss:  1.2196829\n",
      "Training Step:  3290 Loss:  1.2192876\n",
      "Training Step:  3291 Loss:  1.2188926\n",
      "Training Step:  3292 Loss:  1.2184992\n",
      "Training Step:  3293 Loss:  1.2181048\n",
      "Training Step:  3294 Loss:  1.2177107\n",
      "Training Step:  3295 Loss:  1.2173173\n",
      "Training Step:  3296 Loss:  1.2169238\n",
      "Training Step:  3297 Loss:  1.2165298\n",
      "Training Step:  3298 Loss:  1.2161375\n",
      "Training Step:  3299 Loss:  1.215745\n",
      "Training Step:  3300 Loss:  1.2153524\n",
      "Training Step:  3301 Loss:  1.214961\n",
      "Training Step:  3302 Loss:  1.2145699\n",
      "Training Step:  3303 Loss:  1.2141796\n",
      "Training Step:  3304 Loss:  1.2137872\n",
      "Training Step:  3305 Loss:  1.2133977\n",
      "Training Step:  3306 Loss:  1.2130071\n",
      "Training Step:  3307 Loss:  1.2126169\n",
      "Training Step:  3308 Loss:  1.2122276\n",
      "Training Step:  3309 Loss:  1.2118373\n",
      "Training Step:  3310 Loss:  1.2114494\n",
      "Training Step:  3311 Loss:  1.21106\n",
      "Training Step:  3312 Loss:  1.2106715\n",
      "Training Step:  3313 Loss:  1.2102833\n",
      "Training Step:  3314 Loss:  1.2098958\n",
      "Training Step:  3315 Loss:  1.2095089\n",
      "Training Step:  3316 Loss:  1.2091215\n",
      "Training Step:  3317 Loss:  1.2087345\n",
      "Training Step:  3318 Loss:  1.2083486\n",
      "Training Step:  3319 Loss:  1.207962\n",
      "Training Step:  3320 Loss:  1.2075757\n",
      "Training Step:  3321 Loss:  1.2071893\n",
      "Training Step:  3322 Loss:  1.2068046\n",
      "Training Step:  3323 Loss:  1.2064191\n",
      "Training Step:  3324 Loss:  1.2060351\n",
      "Training Step:  3325 Loss:  1.2056482\n",
      "Training Step:  3326 Loss:  1.205267\n",
      "Training Step:  3327 Loss:  1.2048826\n",
      "Training Step:  3328 Loss:  1.2044986\n",
      "Training Step:  3329 Loss:  1.204114\n",
      "Training Step:  3330 Loss:  1.2037317\n",
      "Training Step:  3331 Loss:  1.2033497\n",
      "Training Step:  3332 Loss:  1.2029666\n",
      "Training Step:  3333 Loss:  1.2025849\n",
      "Training Step:  3334 Loss:  1.202203\n",
      "Training Step:  3335 Loss:  1.2018217\n",
      "Training Step:  3336 Loss:  1.2014397\n",
      "Training Step:  3337 Loss:  1.20106\n",
      "Training Step:  3338 Loss:  1.2006793\n",
      "Training Step:  3339 Loss:  1.2002983\n",
      "Training Step:  3340 Loss:  1.1999183\n",
      "Training Step:  3341 Loss:  1.199538\n",
      "Training Step:  3342 Loss:  1.1991595\n",
      "Training Step:  3343 Loss:  1.1987793\n",
      "Training Step:  3344 Loss:  1.1984018\n",
      "Training Step:  3345 Loss:  1.1980217\n",
      "Training Step:  3346 Loss:  1.1976438\n",
      "Training Step:  3347 Loss:  1.1972668\n",
      "Training Step:  3348 Loss:  1.1968889\n",
      "Training Step:  3349 Loss:  1.1965106\n",
      "Training Step:  3350 Loss:  1.1961336\n",
      "Training Step:  3351 Loss:  1.195758\n",
      "Training Step:  3352 Loss:  1.1953812\n",
      "Training Step:  3353 Loss:  1.195004\n",
      "Training Step:  3354 Loss:  1.194629\n",
      "Training Step:  3355 Loss:  1.194253\n",
      "Training Step:  3356 Loss:  1.1938785\n",
      "Training Step:  3357 Loss:  1.193504\n",
      "Training Step:  3358 Loss:  1.1931294\n",
      "Training Step:  3359 Loss:  1.1927558\n",
      "Training Step:  3360 Loss:  1.192381\n",
      "Training Step:  3361 Loss:  1.1920066\n",
      "Training Step:  3362 Loss:  1.1916327\n",
      "Training Step:  3363 Loss:  1.1912597\n",
      "Training Step:  3364 Loss:  1.1908867\n",
      "Training Step:  3365 Loss:  1.1905141\n",
      "Training Step:  3366 Loss:  1.1901416\n",
      "Training Step:  3367 Loss:  1.1897691\n",
      "Training Step:  3368 Loss:  1.189398\n",
      "Training Step:  3369 Loss:  1.1890264\n",
      "Training Step:  3370 Loss:  1.1886549\n",
      "Training Step:  3371 Loss:  1.1882851\n",
      "Training Step:  3372 Loss:  1.1879135\n",
      "Training Step:  3373 Loss:  1.1875441\n",
      "Training Step:  3374 Loss:  1.1871734\n",
      "Training Step:  3375 Loss:  1.1868033\n",
      "Training Step:  3376 Loss:  1.1864345\n",
      "Training Step:  3377 Loss:  1.1860635\n",
      "Training Step:  3378 Loss:  1.1856959\n",
      "Training Step:  3379 Loss:  1.185326\n",
      "Training Step:  3380 Loss:  1.1849592\n",
      "Training Step:  3381 Loss:  1.1845908\n",
      "Training Step:  3382 Loss:  1.1842222\n",
      "Training Step:  3383 Loss:  1.1838564\n",
      "Training Step:  3384 Loss:  1.1834892\n",
      "Training Step:  3385 Loss:  1.1831219\n",
      "Training Step:  3386 Loss:  1.1827555\n",
      "Training Step:  3387 Loss:  1.18239\n",
      "Training Step:  3388 Loss:  1.1820233\n",
      "Training Step:  3389 Loss:  1.1816571\n",
      "Training Step:  3390 Loss:  1.1812905\n",
      "Training Step:  3391 Loss:  1.1809263\n",
      "Training Step:  3392 Loss:  1.1805621\n",
      "Training Step:  3393 Loss:  1.1801972\n",
      "Training Step:  3394 Loss:  1.1798334\n",
      "Training Step:  3395 Loss:  1.1794689\n",
      "Training Step:  3396 Loss:  1.179105\n",
      "Training Step:  3397 Loss:  1.1787417\n",
      "Training Step:  3398 Loss:  1.1783788\n",
      "Training Step:  3399 Loss:  1.178016\n",
      "Training Step:  3400 Loss:  1.1776527\n",
      "Training Step:  3401 Loss:  1.1772908\n",
      "Training Step:  3402 Loss:  1.1769292\n",
      "Training Step:  3403 Loss:  1.1765683\n",
      "Training Step:  3404 Loss:  1.1762062\n",
      "Training Step:  3405 Loss:  1.1758447\n",
      "Training Step:  3406 Loss:  1.1754842\n",
      "Training Step:  3407 Loss:  1.1751235\n",
      "Training Step:  3408 Loss:  1.1747622\n",
      "Training Step:  3409 Loss:  1.1744035\n",
      "Training Step:  3410 Loss:  1.1740432\n",
      "Training Step:  3411 Loss:  1.1736845\n",
      "Training Step:  3412 Loss:  1.1733235\n",
      "Training Step:  3413 Loss:  1.1729654\n",
      "Training Step:  3414 Loss:  1.1726072\n",
      "Training Step:  3415 Loss:  1.1722475\n",
      "Training Step:  3416 Loss:  1.1718903\n",
      "Training Step:  3417 Loss:  1.1715337\n",
      "Training Step:  3418 Loss:  1.171175\n",
      "Training Step:  3419 Loss:  1.1708186\n",
      "Training Step:  3420 Loss:  1.1704602\n",
      "Training Step:  3421 Loss:  1.170105\n",
      "Training Step:  3422 Loss:  1.1697477\n",
      "Training Step:  3423 Loss:  1.1693913\n",
      "Training Step:  3424 Loss:  1.1690365\n",
      "Training Step:  3425 Loss:  1.1686808\n",
      "Training Step:  3426 Loss:  1.1683264\n",
      "Training Step:  3427 Loss:  1.1679696\n",
      "Training Step:  3428 Loss:  1.1676145\n",
      "Training Step:  3429 Loss:  1.1672611\n",
      "Training Step:  3430 Loss:  1.1669066\n",
      "Training Step:  3431 Loss:  1.1665528\n",
      "Training Step:  3432 Loss:  1.1661993\n",
      "Training Step:  3433 Loss:  1.1658459\n",
      "Training Step:  3434 Loss:  1.1654937\n",
      "Training Step:  3435 Loss:  1.1651399\n",
      "Training Step:  3436 Loss:  1.1647869\n",
      "Training Step:  3437 Loss:  1.1644347\n",
      "Training Step:  3438 Loss:  1.1640828\n",
      "Training Step:  3439 Loss:  1.1637322\n",
      "Training Step:  3440 Loss:  1.1633797\n",
      "Training Step:  3441 Loss:  1.1630292\n",
      "Training Step:  3442 Loss:  1.1626775\n",
      "Training Step:  3443 Loss:  1.1623281\n",
      "Training Step:  3444 Loss:  1.1619779\n",
      "Training Step:  3445 Loss:  1.1616282\n",
      "Training Step:  3446 Loss:  1.1612779\n",
      "Training Step:  3447 Loss:  1.160928\n",
      "Training Step:  3448 Loss:  1.1605794\n",
      "Training Step:  3449 Loss:  1.1602306\n",
      "Training Step:  3450 Loss:  1.1598818\n",
      "Training Step:  3451 Loss:  1.1595334\n",
      "Training Step:  3452 Loss:  1.1591842\n",
      "Training Step:  3453 Loss:  1.1588368\n",
      "Training Step:  3454 Loss:  1.1584901\n",
      "Training Step:  3455 Loss:  1.1581421\n",
      "Training Step:  3456 Loss:  1.1577947\n",
      "Training Step:  3457 Loss:  1.157447\n",
      "Training Step:  3458 Loss:  1.157101\n",
      "Training Step:  3459 Loss:  1.1567544\n",
      "Training Step:  3460 Loss:  1.1564084\n",
      "Training Step:  3461 Loss:  1.1560631\n",
      "Training Step:  3462 Loss:  1.155718\n",
      "Training Step:  3463 Loss:  1.1553729\n",
      "Training Step:  3464 Loss:  1.1550276\n",
      "Training Step:  3465 Loss:  1.1546836\n",
      "Training Step:  3466 Loss:  1.1543388\n",
      "Training Step:  3467 Loss:  1.1539944\n",
      "Training Step:  3468 Loss:  1.1536509\n",
      "Training Step:  3469 Loss:  1.1533074\n",
      "Training Step:  3470 Loss:  1.152964\n",
      "Training Step:  3471 Loss:  1.1526203\n",
      "Training Step:  3472 Loss:  1.1522781\n",
      "Training Step:  3473 Loss:  1.1519355\n",
      "Training Step:  3474 Loss:  1.1515937\n",
      "Training Step:  3475 Loss:  1.1512526\n",
      "Training Step:  3476 Loss:  1.1509104\n",
      "Training Step:  3477 Loss:  1.1505684\n",
      "Training Step:  3478 Loss:  1.1502275\n",
      "Training Step:  3479 Loss:  1.1498859\n",
      "Training Step:  3480 Loss:  1.1495459\n",
      "Training Step:  3481 Loss:  1.1492053\n",
      "Training Step:  3482 Loss:  1.1488662\n",
      "Training Step:  3483 Loss:  1.1485251\n",
      "Training Step:  3484 Loss:  1.1481862\n",
      "Training Step:  3485 Loss:  1.1478481\n",
      "Training Step:  3486 Loss:  1.1475074\n",
      "Training Step:  3487 Loss:  1.1471692\n",
      "Training Step:  3488 Loss:  1.1468303\n",
      "Training Step:  3489 Loss:  1.1464916\n",
      "Training Step:  3490 Loss:  1.1461539\n",
      "Training Step:  3491 Loss:  1.1458168\n",
      "Training Step:  3492 Loss:  1.145479\n",
      "Training Step:  3493 Loss:  1.1451426\n",
      "Training Step:  3494 Loss:  1.1448052\n",
      "Training Step:  3495 Loss:  1.1444683\n",
      "Training Step:  3496 Loss:  1.1441325\n",
      "Training Step:  3497 Loss:  1.1437955\n",
      "Training Step:  3498 Loss:  1.1434593\n",
      "Training Step:  3499 Loss:  1.1431245\n",
      "Training Step:  3500 Loss:  1.1427879\n",
      "Training Step:  3501 Loss:  1.1424536\n",
      "Training Step:  3502 Loss:  1.1421185\n",
      "Training Step:  3503 Loss:  1.1417841\n",
      "Training Step:  3504 Loss:  1.1414495\n",
      "Training Step:  3505 Loss:  1.141116\n",
      "Training Step:  3506 Loss:  1.1407821\n",
      "Training Step:  3507 Loss:  1.1404489\n",
      "Training Step:  3508 Loss:  1.1401153\n",
      "Training Step:  3509 Loss:  1.1397836\n",
      "Training Step:  3510 Loss:  1.1394494\n",
      "Training Step:  3511 Loss:  1.1391165\n",
      "Training Step:  3512 Loss:  1.138785\n",
      "Training Step:  3513 Loss:  1.1384519\n",
      "Training Step:  3514 Loss:  1.1381216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  3515 Loss:  1.1377907\n",
      "Training Step:  3516 Loss:  1.1374577\n",
      "Training Step:  3517 Loss:  1.1371279\n",
      "Training Step:  3518 Loss:  1.1367966\n",
      "Training Step:  3519 Loss:  1.136466\n",
      "Training Step:  3520 Loss:  1.1361358\n",
      "Training Step:  3521 Loss:  1.1358061\n",
      "Training Step:  3522 Loss:  1.1354761\n",
      "Training Step:  3523 Loss:  1.1351478\n",
      "Training Step:  3524 Loss:  1.1348184\n",
      "Training Step:  3525 Loss:  1.1344887\n",
      "Training Step:  3526 Loss:  1.134161\n",
      "Training Step:  3527 Loss:  1.1338322\n",
      "Training Step:  3528 Loss:  1.1335044\n",
      "Training Step:  3529 Loss:  1.1331757\n",
      "Training Step:  3530 Loss:  1.1328487\n",
      "Training Step:  3531 Loss:  1.1325221\n",
      "Training Step:  3532 Loss:  1.1321944\n",
      "Training Step:  3533 Loss:  1.131868\n",
      "Training Step:  3534 Loss:  1.1315405\n",
      "Training Step:  3535 Loss:  1.1312144\n",
      "Training Step:  3536 Loss:  1.1308886\n",
      "Training Step:  3537 Loss:  1.1305624\n",
      "Training Step:  3538 Loss:  1.1302376\n",
      "Training Step:  3539 Loss:  1.1299121\n",
      "Training Step:  3540 Loss:  1.1295859\n",
      "Training Step:  3541 Loss:  1.1292624\n",
      "Training Step:  3542 Loss:  1.128937\n",
      "Training Step:  3543 Loss:  1.1286131\n",
      "Training Step:  3544 Loss:  1.1282891\n",
      "Training Step:  3545 Loss:  1.1279649\n",
      "Training Step:  3546 Loss:  1.1276419\n",
      "Training Step:  3547 Loss:  1.127318\n",
      "Training Step:  3548 Loss:  1.126995\n",
      "Training Step:  3549 Loss:  1.1266723\n",
      "Training Step:  3550 Loss:  1.1263494\n",
      "Training Step:  3551 Loss:  1.1260271\n",
      "Training Step:  3552 Loss:  1.1257049\n",
      "Training Step:  3553 Loss:  1.1253835\n",
      "Training Step:  3554 Loss:  1.1250615\n",
      "Training Step:  3555 Loss:  1.1247401\n",
      "Training Step:  3556 Loss:  1.124419\n",
      "Training Step:  3557 Loss:  1.1240985\n",
      "Training Step:  3558 Loss:  1.1237776\n",
      "Training Step:  3559 Loss:  1.1234572\n",
      "Training Step:  3560 Loss:  1.1231371\n",
      "Training Step:  3561 Loss:  1.1228186\n",
      "Training Step:  3562 Loss:  1.1224985\n",
      "Training Step:  3563 Loss:  1.1221792\n",
      "Training Step:  3564 Loss:  1.1218601\n",
      "Training Step:  3565 Loss:  1.1215417\n",
      "Training Step:  3566 Loss:  1.1212234\n",
      "Training Step:  3567 Loss:  1.1209047\n",
      "Training Step:  3568 Loss:  1.1205876\n",
      "Training Step:  3569 Loss:  1.120268\n",
      "Training Step:  3570 Loss:  1.1199505\n",
      "Training Step:  3571 Loss:  1.1196346\n",
      "Training Step:  3572 Loss:  1.119317\n",
      "Training Step:  3573 Loss:  1.1190004\n",
      "Training Step:  3574 Loss:  1.1186836\n",
      "Training Step:  3575 Loss:  1.1183673\n",
      "Training Step:  3576 Loss:  1.11805\n",
      "Training Step:  3577 Loss:  1.1177354\n",
      "Training Step:  3578 Loss:  1.117419\n",
      "Training Step:  3579 Loss:  1.117105\n",
      "Training Step:  3580 Loss:  1.1167883\n",
      "Training Step:  3581 Loss:  1.116474\n",
      "Training Step:  3582 Loss:  1.1161594\n",
      "Training Step:  3583 Loss:  1.1158462\n",
      "Training Step:  3584 Loss:  1.1155305\n",
      "Training Step:  3585 Loss:  1.1152167\n",
      "Training Step:  3586 Loss:  1.1149027\n",
      "Training Step:  3587 Loss:  1.1145899\n",
      "Training Step:  3588 Loss:  1.1142762\n",
      "Training Step:  3589 Loss:  1.1139637\n",
      "Training Step:  3590 Loss:  1.1136518\n",
      "Training Step:  3591 Loss:  1.1133392\n",
      "Training Step:  3592 Loss:  1.1130258\n",
      "Training Step:  3593 Loss:  1.1127148\n",
      "Training Step:  3594 Loss:  1.1124022\n",
      "Training Step:  3595 Loss:  1.1120912\n",
      "Training Step:  3596 Loss:  1.1117809\n",
      "Training Step:  3597 Loss:  1.1114694\n",
      "Training Step:  3598 Loss:  1.1111583\n",
      "Training Step:  3599 Loss:  1.1108471\n",
      "Training Step:  3600 Loss:  1.1105376\n",
      "Training Step:  3601 Loss:  1.1102276\n",
      "Training Step:  3602 Loss:  1.1099179\n",
      "Training Step:  3603 Loss:  1.1096082\n",
      "Training Step:  3604 Loss:  1.1092991\n",
      "Training Step:  3605 Loss:  1.1089897\n",
      "Training Step:  3606 Loss:  1.108681\n",
      "Training Step:  3607 Loss:  1.1083722\n",
      "Training Step:  3608 Loss:  1.1080636\n",
      "Training Step:  3609 Loss:  1.107757\n",
      "Training Step:  3610 Loss:  1.107447\n",
      "Training Step:  3611 Loss:  1.1071405\n",
      "Training Step:  3612 Loss:  1.1068332\n",
      "Training Step:  3613 Loss:  1.1065258\n",
      "Training Step:  3614 Loss:  1.1062188\n",
      "Training Step:  3615 Loss:  1.1059127\n",
      "Training Step:  3616 Loss:  1.1056062\n",
      "Training Step:  3617 Loss:  1.1053002\n",
      "Training Step:  3618 Loss:  1.1049943\n",
      "Training Step:  3619 Loss:  1.1046871\n",
      "Training Step:  3620 Loss:  1.1043828\n",
      "Training Step:  3621 Loss:  1.104077\n",
      "Training Step:  3622 Loss:  1.103773\n",
      "Training Step:  3623 Loss:  1.1034673\n",
      "Training Step:  3624 Loss:  1.103163\n",
      "Training Step:  3625 Loss:  1.1028594\n",
      "Training Step:  3626 Loss:  1.1025555\n",
      "Training Step:  3627 Loss:  1.102251\n",
      "Training Step:  3628 Loss:  1.1019477\n",
      "Training Step:  3629 Loss:  1.1016443\n",
      "Training Step:  3630 Loss:  1.1013404\n",
      "Training Step:  3631 Loss:  1.1010387\n",
      "Training Step:  3632 Loss:  1.100736\n",
      "Training Step:  3633 Loss:  1.1004328\n",
      "Training Step:  3634 Loss:  1.1001313\n",
      "Training Step:  3635 Loss:  1.0998288\n",
      "Training Step:  3636 Loss:  1.0995278\n",
      "Training Step:  3637 Loss:  1.0992242\n",
      "Training Step:  3638 Loss:  1.0989249\n",
      "Training Step:  3639 Loss:  1.0986236\n",
      "Training Step:  3640 Loss:  1.0983227\n",
      "Training Step:  3641 Loss:  1.0980219\n",
      "Training Step:  3642 Loss:  1.0977216\n",
      "Training Step:  3643 Loss:  1.0974218\n",
      "Training Step:  3644 Loss:  1.0971216\n",
      "Training Step:  3645 Loss:  1.0968224\n",
      "Training Step:  3646 Loss:  1.0965233\n",
      "Training Step:  3647 Loss:  1.0962231\n",
      "Training Step:  3648 Loss:  1.0959253\n",
      "Training Step:  3649 Loss:  1.095626\n",
      "Training Step:  3650 Loss:  1.0953276\n",
      "Training Step:  3651 Loss:  1.0950296\n",
      "Training Step:  3652 Loss:  1.094732\n",
      "Training Step:  3653 Loss:  1.0944332\n",
      "Training Step:  3654 Loss:  1.0941355\n",
      "Training Step:  3655 Loss:  1.0938388\n",
      "Training Step:  3656 Loss:  1.093543\n",
      "Training Step:  3657 Loss:  1.0932459\n",
      "Training Step:  3658 Loss:  1.0929483\n",
      "Training Step:  3659 Loss:  1.0926526\n",
      "Training Step:  3660 Loss:  1.092356\n",
      "Training Step:  3661 Loss:  1.0920593\n",
      "Training Step:  3662 Loss:  1.0917642\n",
      "Training Step:  3663 Loss:  1.0914681\n",
      "Training Step:  3664 Loss:  1.091174\n",
      "Training Step:  3665 Loss:  1.0908787\n",
      "Training Step:  3666 Loss:  1.0905842\n",
      "Training Step:  3667 Loss:  1.0902886\n",
      "Training Step:  3668 Loss:  1.0899947\n",
      "Training Step:  3669 Loss:  1.0897009\n",
      "Training Step:  3670 Loss:  1.0894074\n",
      "Training Step:  3671 Loss:  1.0891138\n",
      "Training Step:  3672 Loss:  1.0888201\n",
      "Training Step:  3673 Loss:  1.0885267\n",
      "Training Step:  3674 Loss:  1.088234\n",
      "Training Step:  3675 Loss:  1.0879403\n",
      "Training Step:  3676 Loss:  1.0876484\n",
      "Training Step:  3677 Loss:  1.0873561\n",
      "Training Step:  3678 Loss:  1.0870634\n",
      "Training Step:  3679 Loss:  1.0867716\n",
      "Training Step:  3680 Loss:  1.0864799\n",
      "Training Step:  3681 Loss:  1.0861893\n",
      "Training Step:  3682 Loss:  1.0858984\n",
      "Training Step:  3683 Loss:  1.0856066\n",
      "Training Step:  3684 Loss:  1.085316\n",
      "Training Step:  3685 Loss:  1.0850259\n",
      "Training Step:  3686 Loss:  1.0847354\n",
      "Training Step:  3687 Loss:  1.0844462\n",
      "Training Step:  3688 Loss:  1.0841557\n",
      "Training Step:  3689 Loss:  1.0838649\n",
      "Training Step:  3690 Loss:  1.0835769\n",
      "Training Step:  3691 Loss:  1.0832874\n",
      "Training Step:  3692 Loss:  1.082999\n",
      "Training Step:  3693 Loss:  1.0827088\n",
      "Training Step:  3694 Loss:  1.0824212\n",
      "Training Step:  3695 Loss:  1.0821332\n",
      "Training Step:  3696 Loss:  1.0818452\n",
      "Training Step:  3697 Loss:  1.0815563\n",
      "Training Step:  3698 Loss:  1.0812697\n",
      "Training Step:  3699 Loss:  1.0809829\n",
      "Training Step:  3700 Loss:  1.080695\n",
      "Training Step:  3701 Loss:  1.0804081\n",
      "Training Step:  3702 Loss:  1.0801221\n",
      "Training Step:  3703 Loss:  1.0798354\n",
      "Training Step:  3704 Loss:  1.0795484\n",
      "Training Step:  3705 Loss:  1.0792632\n",
      "Training Step:  3706 Loss:  1.0789772\n",
      "Training Step:  3707 Loss:  1.0786915\n",
      "Training Step:  3708 Loss:  1.0784062\n",
      "Training Step:  3709 Loss:  1.0781217\n",
      "Training Step:  3710 Loss:  1.0778356\n",
      "Training Step:  3711 Loss:  1.0775514\n",
      "Training Step:  3712 Loss:  1.0772673\n",
      "Training Step:  3713 Loss:  1.0769832\n",
      "Training Step:  3714 Loss:  1.0766984\n",
      "Training Step:  3715 Loss:  1.076415\n",
      "Training Step:  3716 Loss:  1.0761302\n",
      "Training Step:  3717 Loss:  1.075848\n",
      "Training Step:  3718 Loss:  1.0755653\n",
      "Training Step:  3719 Loss:  1.075282\n",
      "Training Step:  3720 Loss:  1.0749998\n",
      "Training Step:  3721 Loss:  1.0747161\n",
      "Training Step:  3722 Loss:  1.0744345\n",
      "Training Step:  3723 Loss:  1.0741522\n",
      "Training Step:  3724 Loss:  1.0738703\n",
      "Training Step:  3725 Loss:  1.0735881\n",
      "Training Step:  3726 Loss:  1.0733069\n",
      "Training Step:  3727 Loss:  1.0730258\n",
      "Training Step:  3728 Loss:  1.0727448\n",
      "Training Step:  3729 Loss:  1.0724636\n",
      "Training Step:  3730 Loss:  1.072183\n",
      "Training Step:  3731 Loss:  1.0719041\n",
      "Training Step:  3732 Loss:  1.0716236\n",
      "Training Step:  3733 Loss:  1.0713434\n",
      "Training Step:  3734 Loss:  1.0710629\n",
      "Training Step:  3735 Loss:  1.0707824\n",
      "Training Step:  3736 Loss:  1.0705054\n",
      "Training Step:  3737 Loss:  1.070226\n",
      "Training Step:  3738 Loss:  1.0699471\n",
      "Training Step:  3739 Loss:  1.0696678\n",
      "Training Step:  3740 Loss:  1.0693896\n",
      "Training Step:  3741 Loss:  1.0691125\n",
      "Training Step:  3742 Loss:  1.0688336\n",
      "Training Step:  3743 Loss:  1.0685552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  3744 Loss:  1.068279\n",
      "Training Step:  3745 Loss:  1.0680014\n",
      "Training Step:  3746 Loss:  1.067724\n",
      "Training Step:  3747 Loss:  1.0674459\n",
      "Training Step:  3748 Loss:  1.0671705\n",
      "Training Step:  3749 Loss:  1.0668931\n",
      "Training Step:  3750 Loss:  1.0666174\n",
      "Training Step:  3751 Loss:  1.0663412\n",
      "Training Step:  3752 Loss:  1.0660658\n",
      "Training Step:  3753 Loss:  1.0657892\n",
      "Training Step:  3754 Loss:  1.0655144\n",
      "Training Step:  3755 Loss:  1.0652394\n",
      "Training Step:  3756 Loss:  1.0649655\n",
      "Training Step:  3757 Loss:  1.0646902\n",
      "Training Step:  3758 Loss:  1.0644146\n",
      "Training Step:  3759 Loss:  1.0641401\n",
      "Training Step:  3760 Loss:  1.0638655\n",
      "Training Step:  3761 Loss:  1.0635934\n",
      "Training Step:  3762 Loss:  1.06332\n",
      "Training Step:  3763 Loss:  1.0630456\n",
      "Training Step:  3764 Loss:  1.062772\n",
      "Training Step:  3765 Loss:  1.0624981\n",
      "Training Step:  3766 Loss:  1.0622257\n",
      "Training Step:  3767 Loss:  1.0619528\n",
      "Training Step:  3768 Loss:  1.061682\n",
      "Training Step:  3769 Loss:  1.061409\n",
      "Training Step:  3770 Loss:  1.0611372\n",
      "Training Step:  3771 Loss:  1.060865\n",
      "Training Step:  3772 Loss:  1.0605937\n",
      "Training Step:  3773 Loss:  1.0603223\n",
      "Training Step:  3774 Loss:  1.0600501\n",
      "Training Step:  3775 Loss:  1.0597801\n",
      "Training Step:  3776 Loss:  1.0595089\n",
      "Training Step:  3777 Loss:  1.059238\n",
      "Training Step:  3778 Loss:  1.0589685\n",
      "Training Step:  3779 Loss:  1.0586983\n",
      "Training Step:  3780 Loss:  1.058428\n",
      "Training Step:  3781 Loss:  1.0581586\n",
      "Training Step:  3782 Loss:  1.057889\n",
      "Training Step:  3783 Loss:  1.0576198\n",
      "Training Step:  3784 Loss:  1.0573506\n",
      "Training Step:  3785 Loss:  1.0570818\n",
      "Training Step:  3786 Loss:  1.0568124\n",
      "Training Step:  3787 Loss:  1.0565438\n",
      "Training Step:  3788 Loss:  1.0562763\n",
      "Training Step:  3789 Loss:  1.0560076\n",
      "Training Step:  3790 Loss:  1.0557399\n",
      "Training Step:  3791 Loss:  1.055473\n",
      "Training Step:  3792 Loss:  1.0552051\n",
      "Training Step:  3793 Loss:  1.054938\n",
      "Training Step:  3794 Loss:  1.0546702\n",
      "Training Step:  3795 Loss:  1.0544031\n",
      "Training Step:  3796 Loss:  1.0541368\n",
      "Training Step:  3797 Loss:  1.0538702\n",
      "Training Step:  3798 Loss:  1.0536036\n",
      "Training Step:  3799 Loss:  1.0533392\n",
      "Training Step:  3800 Loss:  1.0530729\n",
      "Training Step:  3801 Loss:  1.052806\n",
      "Training Step:  3802 Loss:  1.0525416\n",
      "Training Step:  3803 Loss:  1.0522764\n",
      "Training Step:  3804 Loss:  1.0520116\n",
      "Training Step:  3805 Loss:  1.0517464\n",
      "Training Step:  3806 Loss:  1.0514807\n",
      "Training Step:  3807 Loss:  1.0512176\n",
      "Training Step:  3808 Loss:  1.0509536\n",
      "Training Step:  3809 Loss:  1.0506889\n",
      "Training Step:  3810 Loss:  1.0504258\n",
      "Training Step:  3811 Loss:  1.0501617\n",
      "Training Step:  3812 Loss:  1.0498989\n",
      "Training Step:  3813 Loss:  1.049635\n",
      "Training Step:  3814 Loss:  1.0493714\n",
      "Training Step:  3815 Loss:  1.0491096\n",
      "Training Step:  3816 Loss:  1.0488472\n",
      "Training Step:  3817 Loss:  1.0485849\n",
      "Training Step:  3818 Loss:  1.0483229\n",
      "Training Step:  3819 Loss:  1.0480609\n",
      "Training Step:  3820 Loss:  1.0477998\n",
      "Training Step:  3821 Loss:  1.0475373\n",
      "Training Step:  3822 Loss:  1.0472759\n",
      "Training Step:  3823 Loss:  1.0470148\n",
      "Training Step:  3824 Loss:  1.046754\n",
      "Training Step:  3825 Loss:  1.0464942\n",
      "Training Step:  3826 Loss:  1.0462316\n",
      "Training Step:  3827 Loss:  1.0459722\n",
      "Training Step:  3828 Loss:  1.0457127\n",
      "Training Step:  3829 Loss:  1.0454518\n",
      "Training Step:  3830 Loss:  1.0451925\n",
      "Training Step:  3831 Loss:  1.044932\n",
      "Training Step:  3832 Loss:  1.0446733\n",
      "Training Step:  3833 Loss:  1.0444149\n",
      "Training Step:  3834 Loss:  1.0441554\n",
      "Training Step:  3835 Loss:  1.043897\n",
      "Training Step:  3836 Loss:  1.043638\n",
      "Training Step:  3837 Loss:  1.04338\n",
      "Training Step:  3838 Loss:  1.0431207\n",
      "Training Step:  3839 Loss:  1.042864\n",
      "Training Step:  3840 Loss:  1.0426059\n",
      "Training Step:  3841 Loss:  1.042348\n",
      "Training Step:  3842 Loss:  1.0420905\n",
      "Training Step:  3843 Loss:  1.0418342\n",
      "Training Step:  3844 Loss:  1.0415766\n",
      "Training Step:  3845 Loss:  1.0413201\n",
      "Training Step:  3846 Loss:  1.0410626\n",
      "Training Step:  3847 Loss:  1.040807\n",
      "Training Step:  3848 Loss:  1.0405508\n",
      "Training Step:  3849 Loss:  1.0402954\n",
      "Training Step:  3850 Loss:  1.040039\n",
      "Training Step:  3851 Loss:  1.0397826\n",
      "Training Step:  3852 Loss:  1.0395279\n",
      "Training Step:  3853 Loss:  1.0392723\n",
      "Training Step:  3854 Loss:  1.0390186\n",
      "Training Step:  3855 Loss:  1.038763\n",
      "Training Step:  3856 Loss:  1.0385077\n",
      "Training Step:  3857 Loss:  1.0382545\n",
      "Training Step:  3858 Loss:  1.0379994\n",
      "Training Step:  3859 Loss:  1.0377454\n",
      "Training Step:  3860 Loss:  1.0374919\n",
      "Training Step:  3861 Loss:  1.0372376\n",
      "Training Step:  3862 Loss:  1.0369841\n",
      "Training Step:  3863 Loss:  1.0367322\n",
      "Training Step:  3864 Loss:  1.0364786\n",
      "Training Step:  3865 Loss:  1.0362252\n",
      "Training Step:  3866 Loss:  1.0359728\n",
      "Training Step:  3867 Loss:  1.03572\n",
      "Training Step:  3868 Loss:  1.0354679\n",
      "Training Step:  3869 Loss:  1.0352153\n",
      "Training Step:  3870 Loss:  1.0349641\n",
      "Training Step:  3871 Loss:  1.0347123\n",
      "Training Step:  3872 Loss:  1.0344611\n",
      "Training Step:  3873 Loss:  1.0342087\n",
      "Training Step:  3874 Loss:  1.0339592\n",
      "Training Step:  3875 Loss:  1.033707\n",
      "Training Step:  3876 Loss:  1.0334563\n",
      "Training Step:  3877 Loss:  1.0332059\n",
      "Training Step:  3878 Loss:  1.0329566\n",
      "Training Step:  3879 Loss:  1.032706\n",
      "Training Step:  3880 Loss:  1.0324554\n",
      "Training Step:  3881 Loss:  1.032206\n",
      "Training Step:  3882 Loss:  1.0319555\n",
      "Training Step:  3883 Loss:  1.0317065\n",
      "Training Step:  3884 Loss:  1.0314575\n",
      "Training Step:  3885 Loss:  1.0312086\n",
      "Training Step:  3886 Loss:  1.0309603\n",
      "Training Step:  3887 Loss:  1.0307115\n",
      "Training Step:  3888 Loss:  1.0304632\n",
      "Training Step:  3889 Loss:  1.0302141\n",
      "Training Step:  3890 Loss:  1.0299654\n",
      "Training Step:  3891 Loss:  1.0297177\n",
      "Training Step:  3892 Loss:  1.0294698\n",
      "Training Step:  3893 Loss:  1.0292245\n",
      "Training Step:  3894 Loss:  1.0289772\n",
      "Training Step:  3895 Loss:  1.0287293\n",
      "Training Step:  3896 Loss:  1.0284824\n",
      "Training Step:  3897 Loss:  1.028236\n",
      "Training Step:  3898 Loss:  1.0279886\n",
      "Training Step:  3899 Loss:  1.0277424\n",
      "Training Step:  3900 Loss:  1.0274968\n",
      "Training Step:  3901 Loss:  1.02725\n",
      "Training Step:  3902 Loss:  1.0270041\n",
      "Training Step:  3903 Loss:  1.0267589\n",
      "Training Step:  3904 Loss:  1.0265137\n",
      "Training Step:  3905 Loss:  1.0262681\n",
      "Training Step:  3906 Loss:  1.0260227\n",
      "Training Step:  3907 Loss:  1.0257791\n",
      "Training Step:  3908 Loss:  1.0255338\n",
      "Training Step:  3909 Loss:  1.0252897\n",
      "Training Step:  3910 Loss:  1.025046\n",
      "Training Step:  3911 Loss:  1.0248002\n",
      "Training Step:  3912 Loss:  1.0245563\n",
      "Training Step:  3913 Loss:  1.0243123\n",
      "Training Step:  3914 Loss:  1.0240695\n",
      "Training Step:  3915 Loss:  1.0238261\n",
      "Training Step:  3916 Loss:  1.0235834\n",
      "Training Step:  3917 Loss:  1.0233394\n",
      "Training Step:  3918 Loss:  1.0230973\n",
      "Training Step:  3919 Loss:  1.0228543\n",
      "Training Step:  3920 Loss:  1.0226119\n",
      "Training Step:  3921 Loss:  1.02237\n",
      "Training Step:  3922 Loss:  1.0221275\n",
      "Training Step:  3923 Loss:  1.0218856\n",
      "Training Step:  3924 Loss:  1.0216442\n",
      "Training Step:  3925 Loss:  1.0214032\n",
      "Training Step:  3926 Loss:  1.0211608\n",
      "Training Step:  3927 Loss:  1.0209193\n",
      "Training Step:  3928 Loss:  1.0206798\n",
      "Training Step:  3929 Loss:  1.0204389\n",
      "Training Step:  3930 Loss:  1.0201976\n",
      "Training Step:  3931 Loss:  1.0199571\n",
      "Training Step:  3932 Loss:  1.0197173\n",
      "Training Step:  3933 Loss:  1.0194776\n",
      "Training Step:  3934 Loss:  1.0192378\n",
      "Training Step:  3935 Loss:  1.018997\n",
      "Training Step:  3936 Loss:  1.0187578\n",
      "Training Step:  3937 Loss:  1.0185186\n",
      "Training Step:  3938 Loss:  1.0182803\n",
      "Training Step:  3939 Loss:  1.0180402\n",
      "Training Step:  3940 Loss:  1.0178025\n",
      "Training Step:  3941 Loss:  1.0175638\n",
      "Training Step:  3942 Loss:  1.0173254\n",
      "Training Step:  3943 Loss:  1.0170875\n",
      "Training Step:  3944 Loss:  1.0168495\n",
      "Training Step:  3945 Loss:  1.0166111\n",
      "Training Step:  3946 Loss:  1.0163741\n",
      "Training Step:  3947 Loss:  1.016136\n",
      "Training Step:  3948 Loss:  1.0158992\n",
      "Training Step:  3949 Loss:  1.0156612\n",
      "Training Step:  3950 Loss:  1.0154257\n",
      "Training Step:  3951 Loss:  1.0151877\n",
      "Training Step:  3952 Loss:  1.0149516\n",
      "Training Step:  3953 Loss:  1.0147146\n",
      "Training Step:  3954 Loss:  1.0144786\n",
      "Training Step:  3955 Loss:  1.0142429\n",
      "Training Step:  3956 Loss:  1.0140072\n",
      "Training Step:  3957 Loss:  1.0137715\n",
      "Training Step:  3958 Loss:  1.0135351\n",
      "Training Step:  3959 Loss:  1.0133004\n",
      "Training Step:  3960 Loss:  1.0130657\n",
      "Training Step:  3961 Loss:  1.0128304\n",
      "Training Step:  3962 Loss:  1.0125949\n",
      "Training Step:  3963 Loss:  1.0123615\n",
      "Training Step:  3964 Loss:  1.0121262\n",
      "Training Step:  3965 Loss:  1.011892\n",
      "Training Step:  3966 Loss:  1.0116576\n",
      "Training Step:  3967 Loss:  1.0114247\n",
      "Training Step:  3968 Loss:  1.0111898\n",
      "Training Step:  3969 Loss:  1.0109574\n",
      "Training Step:  3970 Loss:  1.0107236\n",
      "Training Step:  3971 Loss:  1.01049\n",
      "Training Step:  3972 Loss:  1.0102582\n",
      "Training Step:  3973 Loss:  1.0100247\n",
      "Training Step:  3974 Loss:  1.0097921\n",
      "Training Step:  3975 Loss:  1.0095595\n",
      "Training Step:  3976 Loss:  1.0093277\n",
      "Training Step:  3977 Loss:  1.009095\n",
      "Training Step:  3978 Loss:  1.0088629\n",
      "Training Step:  3979 Loss:  1.0086315\n",
      "Training Step:  3980 Loss:  1.008401\n",
      "Training Step:  3981 Loss:  1.0081692\n",
      "Training Step:  3982 Loss:  1.0079374\n",
      "Training Step:  3983 Loss:  1.007707\n",
      "Training Step:  3984 Loss:  1.0074751\n",
      "Training Step:  3985 Loss:  1.007245\n",
      "Training Step:  3986 Loss:  1.0070146\n",
      "Training Step:  3987 Loss:  1.0067841\n",
      "Training Step:  3988 Loss:  1.0065539\n",
      "Training Step:  3989 Loss:  1.0063249\n",
      "Training Step:  3990 Loss:  1.0060943\n",
      "Training Step:  3991 Loss:  1.0058635\n",
      "Training Step:  3992 Loss:  1.0056361\n",
      "Training Step:  3993 Loss:  1.005406\n",
      "Training Step:  3994 Loss:  1.0051769\n",
      "Training Step:  3995 Loss:  1.0049478\n",
      "Training Step:  3996 Loss:  1.004719\n",
      "Training Step:  3997 Loss:  1.00449\n",
      "Training Step:  3998 Loss:  1.0042613\n",
      "Training Step:  3999 Loss:  1.0040331\n",
      "Training Step:  4000 Loss:  1.0038058\n",
      "Training Step:  4001 Loss:  1.0035775\n",
      "Training Step:  4002 Loss:  1.0033499\n",
      "Training Step:  4003 Loss:  1.0031214\n",
      "Training Step:  4004 Loss:  1.0028951\n",
      "Training Step:  4005 Loss:  1.0026679\n",
      "Training Step:  4006 Loss:  1.0024413\n",
      "Training Step:  4007 Loss:  1.0022138\n",
      "Training Step:  4008 Loss:  1.0019878\n",
      "Training Step:  4009 Loss:  1.0017606\n",
      "Training Step:  4010 Loss:  1.001535\n",
      "Training Step:  4011 Loss:  1.0013092\n",
      "Training Step:  4012 Loss:  1.0010828\n",
      "Training Step:  4013 Loss:  1.0008564\n",
      "Training Step:  4014 Loss:  1.0006305\n",
      "Training Step:  4015 Loss:  1.0004053\n",
      "Training Step:  4016 Loss:  1.00018\n",
      "Training Step:  4017 Loss:  0.99995416\n",
      "Training Step:  4018 Loss:  0.99973\n",
      "Training Step:  4019 Loss:  0.99950635\n",
      "Training Step:  4020 Loss:  0.9992819\n",
      "Training Step:  4021 Loss:  0.99905676\n",
      "Training Step:  4022 Loss:  0.9988316\n",
      "Training Step:  4023 Loss:  0.9986074\n",
      "Training Step:  4024 Loss:  0.9983845\n",
      "Training Step:  4025 Loss:  0.9981602\n",
      "Training Step:  4026 Loss:  0.9979363\n",
      "Training Step:  4027 Loss:  0.9977134\n",
      "Training Step:  4028 Loss:  0.9974895\n",
      "Training Step:  4029 Loss:  0.9972666\n",
      "Training Step:  4030 Loss:  0.9970436\n",
      "Training Step:  4031 Loss:  0.9968213\n",
      "Training Step:  4032 Loss:  0.9965983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  4033 Loss:  0.99637586\n",
      "Training Step:  4034 Loss:  0.9961538\n",
      "Training Step:  4035 Loss:  0.99593174\n",
      "Training Step:  4036 Loss:  0.9957098\n",
      "Training Step:  4037 Loss:  0.9954881\n",
      "Training Step:  4038 Loss:  0.9952667\n",
      "Training Step:  4039 Loss:  0.9950459\n",
      "Training Step:  4040 Loss:  0.99482507\n",
      "Training Step:  4041 Loss:  0.99460286\n",
      "Training Step:  4042 Loss:  0.994381\n",
      "Training Step:  4043 Loss:  0.99416095\n",
      "Training Step:  4044 Loss:  0.9939408\n",
      "Training Step:  4045 Loss:  0.99372\n",
      "Training Step:  4046 Loss:  0.9935002\n",
      "Training Step:  4047 Loss:  0.99327993\n",
      "Training Step:  4048 Loss:  0.99306035\n",
      "Training Step:  4049 Loss:  0.9928409\n",
      "Training Step:  4050 Loss:  0.9926204\n",
      "Training Step:  4051 Loss:  0.99240184\n",
      "Training Step:  4052 Loss:  0.9921831\n",
      "Training Step:  4053 Loss:  0.9919636\n",
      "Training Step:  4054 Loss:  0.99174505\n",
      "Training Step:  4055 Loss:  0.9915253\n",
      "Training Step:  4056 Loss:  0.99130696\n",
      "Training Step:  4057 Loss:  0.99108875\n",
      "Training Step:  4058 Loss:  0.9908707\n",
      "Training Step:  4059 Loss:  0.9906534\n",
      "Training Step:  4060 Loss:  0.99043393\n",
      "Training Step:  4061 Loss:  0.9902167\n",
      "Training Step:  4062 Loss:  0.9899992\n",
      "Training Step:  4063 Loss:  0.98978233\n",
      "Training Step:  4064 Loss:  0.98956436\n",
      "Training Step:  4065 Loss:  0.9893478\n",
      "Training Step:  4066 Loss:  0.9891304\n",
      "Training Step:  4067 Loss:  0.9889145\n",
      "Training Step:  4068 Loss:  0.9886971\n",
      "Training Step:  4069 Loss:  0.98848134\n",
      "Training Step:  4070 Loss:  0.9882654\n",
      "Training Step:  4071 Loss:  0.9880477\n",
      "Training Step:  4072 Loss:  0.9878324\n",
      "Training Step:  4073 Loss:  0.9876175\n",
      "Training Step:  4074 Loss:  0.98740155\n",
      "Training Step:  4075 Loss:  0.98718596\n",
      "Training Step:  4076 Loss:  0.986971\n",
      "Training Step:  4077 Loss:  0.9867557\n",
      "Training Step:  4078 Loss:  0.9865417\n",
      "Training Step:  4079 Loss:  0.9863264\n",
      "Training Step:  4080 Loss:  0.9861115\n",
      "Training Step:  4081 Loss:  0.98589694\n",
      "Training Step:  4082 Loss:  0.98568314\n",
      "Training Step:  4083 Loss:  0.9854684\n",
      "Training Step:  4084 Loss:  0.9852557\n",
      "Training Step:  4085 Loss:  0.9850404\n",
      "Training Step:  4086 Loss:  0.9848275\n",
      "Training Step:  4087 Loss:  0.9846133\n",
      "Training Step:  4088 Loss:  0.9844006\n",
      "Training Step:  4089 Loss:  0.98418707\n",
      "Training Step:  4090 Loss:  0.98397416\n",
      "Training Step:  4091 Loss:  0.98376226\n",
      "Training Step:  4092 Loss:  0.9835489\n",
      "Training Step:  4093 Loss:  0.9833373\n",
      "Training Step:  4094 Loss:  0.9831238\n",
      "Training Step:  4095 Loss:  0.98291165\n",
      "Training Step:  4096 Loss:  0.98269993\n",
      "Training Step:  4097 Loss:  0.9824873\n",
      "Training Step:  4098 Loss:  0.98227644\n",
      "Training Step:  4099 Loss:  0.98206586\n",
      "Training Step:  4100 Loss:  0.98185325\n",
      "Training Step:  4101 Loss:  0.9816428\n",
      "Training Step:  4102 Loss:  0.9814309\n",
      "Training Step:  4103 Loss:  0.9812208\n",
      "Training Step:  4104 Loss:  0.9810102\n",
      "Training Step:  4105 Loss:  0.9807985\n",
      "Training Step:  4106 Loss:  0.9805878\n",
      "Training Step:  4107 Loss:  0.9803783\n",
      "Training Step:  4108 Loss:  0.9801685\n",
      "Training Step:  4109 Loss:  0.97995865\n",
      "Training Step:  4110 Loss:  0.97974896\n",
      "Training Step:  4111 Loss:  0.97953933\n",
      "Training Step:  4112 Loss:  0.9793297\n",
      "Training Step:  4113 Loss:  0.97912025\n",
      "Training Step:  4114 Loss:  0.9789114\n",
      "Training Step:  4115 Loss:  0.9787017\n",
      "Training Step:  4116 Loss:  0.9784937\n",
      "Training Step:  4117 Loss:  0.9782847\n",
      "Training Step:  4118 Loss:  0.97807604\n",
      "Training Step:  4119 Loss:  0.97786796\n",
      "Training Step:  4120 Loss:  0.9776596\n",
      "Training Step:  4121 Loss:  0.9774511\n",
      "Training Step:  4122 Loss:  0.97724354\n",
      "Training Step:  4123 Loss:  0.97703636\n",
      "Training Step:  4124 Loss:  0.9768275\n",
      "Training Step:  4125 Loss:  0.9766209\n",
      "Training Step:  4126 Loss:  0.9764142\n",
      "Training Step:  4127 Loss:  0.97620726\n",
      "Training Step:  4128 Loss:  0.9759999\n",
      "Training Step:  4129 Loss:  0.9757938\n",
      "Training Step:  4130 Loss:  0.97558606\n",
      "Training Step:  4131 Loss:  0.97538006\n",
      "Training Step:  4132 Loss:  0.9751735\n",
      "Training Step:  4133 Loss:  0.9749676\n",
      "Training Step:  4134 Loss:  0.97476184\n",
      "Training Step:  4135 Loss:  0.9745562\n",
      "Training Step:  4136 Loss:  0.97434944\n",
      "Training Step:  4137 Loss:  0.9741452\n",
      "Training Step:  4138 Loss:  0.97393924\n",
      "Training Step:  4139 Loss:  0.9737336\n",
      "Training Step:  4140 Loss:  0.9735293\n",
      "Training Step:  4141 Loss:  0.9733242\n",
      "Training Step:  4142 Loss:  0.97311974\n",
      "Training Step:  4143 Loss:  0.9729146\n",
      "Training Step:  4144 Loss:  0.9727109\n",
      "Training Step:  4145 Loss:  0.9725058\n",
      "Training Step:  4146 Loss:  0.9723023\n",
      "Training Step:  4147 Loss:  0.9720978\n",
      "Training Step:  4148 Loss:  0.9718934\n",
      "Training Step:  4149 Loss:  0.97168934\n",
      "Training Step:  4150 Loss:  0.97148716\n",
      "Training Step:  4151 Loss:  0.97128356\n",
      "Training Step:  4152 Loss:  0.97108006\n",
      "Training Step:  4153 Loss:  0.97087747\n",
      "Training Step:  4154 Loss:  0.9706746\n",
      "Training Step:  4155 Loss:  0.97047126\n",
      "Training Step:  4156 Loss:  0.97026896\n",
      "Training Step:  4157 Loss:  0.9700665\n",
      "Training Step:  4158 Loss:  0.96986413\n",
      "Training Step:  4159 Loss:  0.96966153\n",
      "Training Step:  4160 Loss:  0.9694593\n",
      "Training Step:  4161 Loss:  0.9692576\n",
      "Training Step:  4162 Loss:  0.96905637\n",
      "Training Step:  4163 Loss:  0.96885407\n",
      "Training Step:  4164 Loss:  0.96865344\n",
      "Training Step:  4165 Loss:  0.9684524\n",
      "Training Step:  4166 Loss:  0.9682512\n",
      "Training Step:  4167 Loss:  0.9680504\n",
      "Training Step:  4168 Loss:  0.96785\n",
      "Training Step:  4169 Loss:  0.9676492\n",
      "Training Step:  4170 Loss:  0.9674485\n",
      "Training Step:  4171 Loss:  0.9672482\n",
      "Training Step:  4172 Loss:  0.9670485\n",
      "Training Step:  4173 Loss:  0.96684825\n",
      "Training Step:  4174 Loss:  0.9666478\n",
      "Training Step:  4175 Loss:  0.9664483\n",
      "Training Step:  4176 Loss:  0.96624887\n",
      "Training Step:  4177 Loss:  0.96604973\n",
      "Training Step:  4178 Loss:  0.9658503\n",
      "Training Step:  4179 Loss:  0.9656519\n",
      "Training Step:  4180 Loss:  0.9654513\n",
      "Training Step:  4181 Loss:  0.96525216\n",
      "Training Step:  4182 Loss:  0.96505415\n",
      "Training Step:  4183 Loss:  0.96485645\n",
      "Training Step:  4184 Loss:  0.9646579\n",
      "Training Step:  4185 Loss:  0.9644588\n",
      "Training Step:  4186 Loss:  0.96426225\n",
      "Training Step:  4187 Loss:  0.96406287\n",
      "Training Step:  4188 Loss:  0.9638653\n",
      "Training Step:  4189 Loss:  0.96366805\n",
      "Training Step:  4190 Loss:  0.9634697\n",
      "Training Step:  4191 Loss:  0.9632736\n",
      "Training Step:  4192 Loss:  0.96307653\n",
      "Training Step:  4193 Loss:  0.9628794\n",
      "Training Step:  4194 Loss:  0.96268314\n",
      "Training Step:  4195 Loss:  0.9624859\n",
      "Training Step:  4196 Loss:  0.9622891\n",
      "Training Step:  4197 Loss:  0.96209246\n",
      "Training Step:  4198 Loss:  0.9618955\n",
      "Training Step:  4199 Loss:  0.9616999\n",
      "Training Step:  4200 Loss:  0.96150494\n",
      "Training Step:  4201 Loss:  0.9613072\n",
      "Training Step:  4202 Loss:  0.9611126\n",
      "Training Step:  4203 Loss:  0.9609171\n",
      "Training Step:  4204 Loss:  0.9607212\n",
      "Training Step:  4205 Loss:  0.9605262\n",
      "Training Step:  4206 Loss:  0.960332\n",
      "Training Step:  4207 Loss:  0.9601367\n",
      "Training Step:  4208 Loss:  0.9599412\n",
      "Training Step:  4209 Loss:  0.9597469\n",
      "Training Step:  4210 Loss:  0.9595527\n",
      "Training Step:  4211 Loss:  0.9593592\n",
      "Training Step:  4212 Loss:  0.95916384\n",
      "Training Step:  4213 Loss:  0.95897025\n",
      "Training Step:  4214 Loss:  0.9587749\n",
      "Training Step:  4215 Loss:  0.95858216\n",
      "Training Step:  4216 Loss:  0.95838785\n",
      "Training Step:  4217 Loss:  0.9581941\n",
      "Training Step:  4218 Loss:  0.95800126\n",
      "Training Step:  4219 Loss:  0.9578092\n",
      "Training Step:  4220 Loss:  0.95761514\n",
      "Training Step:  4221 Loss:  0.9574224\n",
      "Training Step:  4222 Loss:  0.9572293\n",
      "Training Step:  4223 Loss:  0.957037\n",
      "Training Step:  4224 Loss:  0.956845\n",
      "Training Step:  4225 Loss:  0.9566525\n",
      "Training Step:  4226 Loss:  0.9564595\n",
      "Training Step:  4227 Loss:  0.9562676\n",
      "Training Step:  4228 Loss:  0.9560764\n",
      "Training Step:  4229 Loss:  0.9558845\n",
      "Training Step:  4230 Loss:  0.95569235\n",
      "Training Step:  4231 Loss:  0.9555009\n",
      "Training Step:  4232 Loss:  0.9553099\n",
      "Training Step:  4233 Loss:  0.9551184\n",
      "Training Step:  4234 Loss:  0.9549274\n",
      "Training Step:  4235 Loss:  0.9547366\n",
      "Training Step:  4236 Loss:  0.9545461\n",
      "Training Step:  4237 Loss:  0.95435435\n",
      "Training Step:  4238 Loss:  0.9541656\n",
      "Training Step:  4239 Loss:  0.95397455\n",
      "Training Step:  4240 Loss:  0.9537842\n",
      "Training Step:  4241 Loss:  0.9535941\n",
      "Training Step:  4242 Loss:  0.95340484\n",
      "Training Step:  4243 Loss:  0.9532145\n",
      "Training Step:  4244 Loss:  0.95302594\n",
      "Training Step:  4245 Loss:  0.9528364\n",
      "Training Step:  4246 Loss:  0.9526471\n",
      "Training Step:  4247 Loss:  0.95245695\n",
      "Training Step:  4248 Loss:  0.95226794\n",
      "Training Step:  4249 Loss:  0.9520798\n",
      "Training Step:  4250 Loss:  0.95189166\n",
      "Training Step:  4251 Loss:  0.95170313\n",
      "Training Step:  4252 Loss:  0.95151377\n",
      "Training Step:  4253 Loss:  0.95132494\n",
      "Training Step:  4254 Loss:  0.95113814\n",
      "Training Step:  4255 Loss:  0.95094883\n",
      "Training Step:  4256 Loss:  0.9507602\n",
      "Training Step:  4257 Loss:  0.9505738\n",
      "Training Step:  4258 Loss:  0.9503856\n",
      "Training Step:  4259 Loss:  0.9501971\n",
      "Training Step:  4260 Loss:  0.95001066\n",
      "Training Step:  4261 Loss:  0.94982415\n",
      "Training Step:  4262 Loss:  0.94963616\n",
      "Training Step:  4263 Loss:  0.94944936\n",
      "Training Step:  4264 Loss:  0.94926304\n",
      "Training Step:  4265 Loss:  0.9490756\n",
      "Training Step:  4266 Loss:  0.9488903\n",
      "Training Step:  4267 Loss:  0.9487041\n",
      "Training Step:  4268 Loss:  0.9485177\n",
      "Training Step:  4269 Loss:  0.9483309\n",
      "Training Step:  4270 Loss:  0.9481466\n",
      "Training Step:  4271 Loss:  0.9479599\n",
      "Training Step:  4272 Loss:  0.94777375\n",
      "Training Step:  4273 Loss:  0.9475883\n",
      "Training Step:  4274 Loss:  0.9474038\n",
      "Training Step:  4275 Loss:  0.9472183\n",
      "Training Step:  4276 Loss:  0.9470331\n",
      "Training Step:  4277 Loss:  0.94684726\n",
      "Training Step:  4278 Loss:  0.9466622\n",
      "Training Step:  4279 Loss:  0.94647837\n",
      "Training Step:  4280 Loss:  0.9462949\n",
      "Training Step:  4281 Loss:  0.94610953\n",
      "Training Step:  4282 Loss:  0.94592494\n",
      "Training Step:  4283 Loss:  0.9457412\n",
      "Training Step:  4284 Loss:  0.9455574\n",
      "Training Step:  4285 Loss:  0.9453733\n",
      "Training Step:  4286 Loss:  0.9451894\n",
      "Training Step:  4287 Loss:  0.94500554\n",
      "Training Step:  4288 Loss:  0.9448228\n",
      "Training Step:  4289 Loss:  0.94463897\n",
      "Training Step:  4290 Loss:  0.9444554\n",
      "Training Step:  4291 Loss:  0.9442723\n",
      "Training Step:  4292 Loss:  0.94409084\n",
      "Training Step:  4293 Loss:  0.9439075\n",
      "Training Step:  4294 Loss:  0.94372404\n",
      "Training Step:  4295 Loss:  0.94354194\n",
      "Training Step:  4296 Loss:  0.94335985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  4297 Loss:  0.943177\n",
      "Training Step:  4298 Loss:  0.94299555\n",
      "Training Step:  4299 Loss:  0.9428128\n",
      "Training Step:  4300 Loss:  0.94263124\n",
      "Training Step:  4301 Loss:  0.9424501\n",
      "Training Step:  4302 Loss:  0.9422676\n",
      "Training Step:  4303 Loss:  0.9420863\n",
      "Training Step:  4304 Loss:  0.9419056\n",
      "Training Step:  4305 Loss:  0.9417236\n",
      "Training Step:  4306 Loss:  0.9415426\n",
      "Training Step:  4307 Loss:  0.9413623\n",
      "Training Step:  4308 Loss:  0.941181\n",
      "Training Step:  4309 Loss:  0.9410013\n",
      "Training Step:  4310 Loss:  0.9408213\n",
      "Training Step:  4311 Loss:  0.9406405\n",
      "Training Step:  4312 Loss:  0.9404602\n",
      "Training Step:  4313 Loss:  0.94027996\n",
      "Training Step:  4314 Loss:  0.94010097\n",
      "Training Step:  4315 Loss:  0.9399217\n",
      "Training Step:  4316 Loss:  0.9397415\n",
      "Training Step:  4317 Loss:  0.93956226\n",
      "Training Step:  4318 Loss:  0.9393823\n",
      "Training Step:  4319 Loss:  0.93920267\n",
      "Training Step:  4320 Loss:  0.93902403\n",
      "Training Step:  4321 Loss:  0.9388454\n",
      "Training Step:  4322 Loss:  0.9386666\n",
      "Training Step:  4323 Loss:  0.9384887\n",
      "Training Step:  4324 Loss:  0.938309\n",
      "Training Step:  4325 Loss:  0.938131\n",
      "Training Step:  4326 Loss:  0.93795264\n",
      "Training Step:  4327 Loss:  0.9377749\n",
      "Training Step:  4328 Loss:  0.93759626\n",
      "Training Step:  4329 Loss:  0.9374186\n",
      "Training Step:  4330 Loss:  0.93724096\n",
      "Training Step:  4331 Loss:  0.9370627\n",
      "Training Step:  4332 Loss:  0.9368857\n",
      "Training Step:  4333 Loss:  0.93670845\n",
      "Training Step:  4334 Loss:  0.93653095\n",
      "Training Step:  4335 Loss:  0.93635416\n",
      "Training Step:  4336 Loss:  0.93617743\n",
      "Training Step:  4337 Loss:  0.9359994\n",
      "Training Step:  4338 Loss:  0.93582356\n",
      "Training Step:  4339 Loss:  0.935646\n",
      "Training Step:  4340 Loss:  0.93547034\n",
      "Training Step:  4341 Loss:  0.9352952\n",
      "Training Step:  4342 Loss:  0.935117\n",
      "Training Step:  4343 Loss:  0.93494207\n",
      "Training Step:  4344 Loss:  0.9347659\n",
      "Training Step:  4345 Loss:  0.9345913\n",
      "Training Step:  4346 Loss:  0.934414\n",
      "Training Step:  4347 Loss:  0.93423843\n",
      "Training Step:  4348 Loss:  0.93406373\n",
      "Training Step:  4349 Loss:  0.93388855\n",
      "Training Step:  4350 Loss:  0.93371385\n",
      "Training Step:  4351 Loss:  0.9335376\n",
      "Training Step:  4352 Loss:  0.93336403\n",
      "Training Step:  4353 Loss:  0.9331876\n",
      "Training Step:  4354 Loss:  0.93301404\n",
      "Training Step:  4355 Loss:  0.9328402\n",
      "Training Step:  4356 Loss:  0.93266505\n",
      "Training Step:  4357 Loss:  0.9324913\n",
      "Training Step:  4358 Loss:  0.9323169\n",
      "Training Step:  4359 Loss:  0.9321423\n",
      "Training Step:  4360 Loss:  0.931969\n",
      "Training Step:  4361 Loss:  0.9317955\n",
      "Training Step:  4362 Loss:  0.9316216\n",
      "Training Step:  4363 Loss:  0.9314484\n",
      "Training Step:  4364 Loss:  0.93127537\n",
      "Training Step:  4365 Loss:  0.9311024\n",
      "Training Step:  4366 Loss:  0.93092895\n",
      "Training Step:  4367 Loss:  0.930755\n",
      "Training Step:  4368 Loss:  0.93058324\n",
      "Training Step:  4369 Loss:  0.9304113\n",
      "Training Step:  4370 Loss:  0.9302384\n",
      "Training Step:  4371 Loss:  0.9300668\n",
      "Training Step:  4372 Loss:  0.9298926\n",
      "Training Step:  4373 Loss:  0.9297212\n",
      "Training Step:  4374 Loss:  0.929548\n",
      "Training Step:  4375 Loss:  0.9293769\n",
      "Training Step:  4376 Loss:  0.92920494\n",
      "Training Step:  4377 Loss:  0.9290337\n",
      "Training Step:  4378 Loss:  0.92886287\n",
      "Training Step:  4379 Loss:  0.9286913\n",
      "Training Step:  4380 Loss:  0.9285202\n",
      "Training Step:  4381 Loss:  0.9283496\n",
      "Training Step:  4382 Loss:  0.92817837\n",
      "Training Step:  4383 Loss:  0.9280067\n",
      "Training Step:  4384 Loss:  0.9278372\n",
      "Training Step:  4385 Loss:  0.927665\n",
      "Training Step:  4386 Loss:  0.9274951\n",
      "Training Step:  4387 Loss:  0.9273249\n",
      "Training Step:  4388 Loss:  0.92715454\n",
      "Training Step:  4389 Loss:  0.92698455\n",
      "Training Step:  4390 Loss:  0.92681384\n",
      "Training Step:  4391 Loss:  0.9266449\n",
      "Training Step:  4392 Loss:  0.9264738\n",
      "Training Step:  4393 Loss:  0.9263054\n",
      "Training Step:  4394 Loss:  0.9261359\n",
      "Training Step:  4395 Loss:  0.9259668\n",
      "Training Step:  4396 Loss:  0.92579657\n",
      "Training Step:  4397 Loss:  0.92562765\n",
      "Training Step:  4398 Loss:  0.9254588\n",
      "Training Step:  4399 Loss:  0.9252904\n",
      "Training Step:  4400 Loss:  0.92512155\n",
      "Training Step:  4401 Loss:  0.9249524\n",
      "Training Step:  4402 Loss:  0.92478406\n",
      "Training Step:  4403 Loss:  0.9246163\n",
      "Training Step:  4404 Loss:  0.9244474\n",
      "Training Step:  4405 Loss:  0.92427915\n",
      "Training Step:  4406 Loss:  0.92411196\n",
      "Training Step:  4407 Loss:  0.92394406\n",
      "Training Step:  4408 Loss:  0.9237747\n",
      "Training Step:  4409 Loss:  0.9236086\n",
      "Training Step:  4410 Loss:  0.9234409\n",
      "Training Step:  4411 Loss:  0.9232724\n",
      "Training Step:  4412 Loss:  0.9231064\n",
      "Training Step:  4413 Loss:  0.9229392\n",
      "Training Step:  4414 Loss:  0.9227719\n",
      "Training Step:  4415 Loss:  0.9226062\n",
      "Training Step:  4416 Loss:  0.9224386\n",
      "Training Step:  4417 Loss:  0.92227185\n",
      "Training Step:  4418 Loss:  0.9221052\n",
      "Training Step:  4419 Loss:  0.9219399\n",
      "Training Step:  4420 Loss:  0.9217731\n",
      "Training Step:  4421 Loss:  0.92160666\n",
      "Training Step:  4422 Loss:  0.92144054\n",
      "Training Step:  4423 Loss:  0.92127585\n",
      "Training Step:  4424 Loss:  0.92110884\n",
      "Training Step:  4425 Loss:  0.9209435\n",
      "Training Step:  4426 Loss:  0.920778\n",
      "Training Step:  4427 Loss:  0.92061186\n",
      "Training Step:  4428 Loss:  0.92044777\n",
      "Training Step:  4429 Loss:  0.9202823\n",
      "Training Step:  4430 Loss:  0.92011774\n",
      "Training Step:  4431 Loss:  0.9199519\n",
      "Training Step:  4432 Loss:  0.91978663\n",
      "Training Step:  4433 Loss:  0.9196235\n",
      "Training Step:  4434 Loss:  0.919458\n",
      "Training Step:  4435 Loss:  0.9192939\n",
      "Training Step:  4436 Loss:  0.9191298\n",
      "Training Step:  4437 Loss:  0.918965\n",
      "Training Step:  4438 Loss:  0.9188013\n",
      "Training Step:  4439 Loss:  0.9186372\n",
      "Training Step:  4440 Loss:  0.91847426\n",
      "Training Step:  4441 Loss:  0.9183104\n",
      "Training Step:  4442 Loss:  0.9181463\n",
      "Training Step:  4443 Loss:  0.9179836\n",
      "Training Step:  4444 Loss:  0.91782004\n",
      "Training Step:  4445 Loss:  0.91765636\n",
      "Training Step:  4446 Loss:  0.91749346\n",
      "Training Step:  4447 Loss:  0.9173304\n",
      "Training Step:  4448 Loss:  0.9171676\n",
      "Training Step:  4449 Loss:  0.9170048\n",
      "Training Step:  4450 Loss:  0.91684234\n",
      "Training Step:  4451 Loss:  0.9166807\n",
      "Training Step:  4452 Loss:  0.9165186\n",
      "Training Step:  4453 Loss:  0.9163556\n",
      "Training Step:  4454 Loss:  0.916194\n",
      "Training Step:  4455 Loss:  0.91603196\n",
      "Training Step:  4456 Loss:  0.9158697\n",
      "Training Step:  4457 Loss:  0.91570795\n",
      "Training Step:  4458 Loss:  0.9155464\n",
      "Training Step:  4459 Loss:  0.9153844\n",
      "Training Step:  4460 Loss:  0.9152231\n",
      "Training Step:  4461 Loss:  0.91506237\n",
      "Training Step:  4462 Loss:  0.91490126\n",
      "Training Step:  4463 Loss:  0.91474015\n",
      "Training Step:  4464 Loss:  0.91457903\n",
      "Training Step:  4465 Loss:  0.91441876\n",
      "Training Step:  4466 Loss:  0.9142585\n",
      "Training Step:  4467 Loss:  0.9140972\n",
      "Training Step:  4468 Loss:  0.9139372\n",
      "Training Step:  4469 Loss:  0.9137758\n",
      "Training Step:  4470 Loss:  0.91361547\n",
      "Training Step:  4471 Loss:  0.91345614\n",
      "Training Step:  4472 Loss:  0.9132951\n",
      "Training Step:  4473 Loss:  0.91313624\n",
      "Training Step:  4474 Loss:  0.91297734\n",
      "Training Step:  4475 Loss:  0.912817\n",
      "Training Step:  4476 Loss:  0.9126576\n",
      "Training Step:  4477 Loss:  0.91249835\n",
      "Training Step:  4478 Loss:  0.9123393\n",
      "Training Step:  4479 Loss:  0.9121797\n",
      "Training Step:  4480 Loss:  0.9120199\n",
      "Training Step:  4481 Loss:  0.9118613\n",
      "Training Step:  4482 Loss:  0.91170377\n",
      "Training Step:  4483 Loss:  0.9115451\n",
      "Training Step:  4484 Loss:  0.91138667\n",
      "Training Step:  4485 Loss:  0.9112281\n",
      "Training Step:  4486 Loss:  0.91106814\n",
      "Training Step:  4487 Loss:  0.9109102\n",
      "Training Step:  4488 Loss:  0.91075224\n",
      "Training Step:  4489 Loss:  0.9105947\n",
      "Training Step:  4490 Loss:  0.91043735\n",
      "Training Step:  4491 Loss:  0.9102797\n",
      "Training Step:  4492 Loss:  0.91012144\n",
      "Training Step:  4493 Loss:  0.9099641\n",
      "Training Step:  4494 Loss:  0.9098072\n",
      "Training Step:  4495 Loss:  0.9096496\n",
      "Training Step:  4496 Loss:  0.9094918\n",
      "Training Step:  4497 Loss:  0.90933496\n",
      "Training Step:  4498 Loss:  0.90917856\n",
      "Training Step:  4499 Loss:  0.9090222\n",
      "Training Step:  4500 Loss:  0.90886563\n",
      "Training Step:  4501 Loss:  0.9087086\n",
      "Training Step:  4502 Loss:  0.90855217\n",
      "Training Step:  4503 Loss:  0.9083959\n",
      "Training Step:  4504 Loss:  0.9082388\n",
      "Training Step:  4505 Loss:  0.9080845\n",
      "Training Step:  4506 Loss:  0.90792733\n",
      "Training Step:  4507 Loss:  0.90777206\n",
      "Training Step:  4508 Loss:  0.9076153\n",
      "Training Step:  4509 Loss:  0.9074594\n",
      "Training Step:  4510 Loss:  0.90730566\n",
      "Training Step:  4511 Loss:  0.9071488\n",
      "Training Step:  4512 Loss:  0.90699446\n",
      "Training Step:  4513 Loss:  0.9068399\n",
      "Training Step:  4514 Loss:  0.90668344\n",
      "Training Step:  4515 Loss:  0.9065287\n",
      "Training Step:  4516 Loss:  0.9063744\n",
      "Training Step:  4517 Loss:  0.90621835\n",
      "Training Step:  4518 Loss:  0.9060649\n",
      "Training Step:  4519 Loss:  0.9059099\n",
      "Training Step:  4520 Loss:  0.90575606\n",
      "Training Step:  4521 Loss:  0.9056023\n",
      "Training Step:  4522 Loss:  0.9054474\n",
      "Training Step:  4523 Loss:  0.9052934\n",
      "Training Step:  4524 Loss:  0.9051391\n",
      "Training Step:  4525 Loss:  0.9049858\n",
      "Training Step:  4526 Loss:  0.9048321\n",
      "Training Step:  4527 Loss:  0.9046779\n",
      "Training Step:  4528 Loss:  0.9045248\n",
      "Training Step:  4529 Loss:  0.90437144\n",
      "Training Step:  4530 Loss:  0.9042188\n",
      "Training Step:  4531 Loss:  0.90406466\n",
      "Training Step:  4532 Loss:  0.90391207\n",
      "Training Step:  4533 Loss:  0.90375984\n",
      "Training Step:  4534 Loss:  0.90360636\n",
      "Training Step:  4535 Loss:  0.9034544\n",
      "Training Step:  4536 Loss:  0.9033008\n",
      "Training Step:  4537 Loss:  0.90314895\n",
      "Training Step:  4538 Loss:  0.90299577\n",
      "Training Step:  4539 Loss:  0.90284395\n",
      "Training Step:  4540 Loss:  0.9026915\n",
      "Training Step:  4541 Loss:  0.9025401\n",
      "Training Step:  4542 Loss:  0.902388\n",
      "Training Step:  4543 Loss:  0.90223753\n",
      "Training Step:  4544 Loss:  0.9020848\n",
      "Training Step:  4545 Loss:  0.90193355\n",
      "Training Step:  4546 Loss:  0.9017811\n",
      "Training Step:  4547 Loss:  0.90162927\n",
      "Training Step:  4548 Loss:  0.9014785\n",
      "Training Step:  4549 Loss:  0.9013272\n",
      "Training Step:  4550 Loss:  0.90117645\n",
      "Training Step:  4551 Loss:  0.9010258\n",
      "Training Step:  4552 Loss:  0.9008753\n",
      "Training Step:  4553 Loss:  0.9007238\n",
      "Training Step:  4554 Loss:  0.90057343\n",
      "Training Step:  4555 Loss:  0.90042377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  4556 Loss:  0.90027344\n",
      "Training Step:  4557 Loss:  0.9001236\n",
      "Training Step:  4558 Loss:  0.8999736\n",
      "Training Step:  4559 Loss:  0.89982224\n",
      "Training Step:  4560 Loss:  0.8996731\n",
      "Training Step:  4561 Loss:  0.899523\n",
      "Training Step:  4562 Loss:  0.8993739\n",
      "Training Step:  4563 Loss:  0.8992241\n",
      "Training Step:  4564 Loss:  0.8990746\n",
      "Training Step:  4565 Loss:  0.8989259\n",
      "Training Step:  4566 Loss:  0.8987752\n",
      "Training Step:  4567 Loss:  0.89862645\n",
      "Training Step:  4568 Loss:  0.8984786\n",
      "Training Step:  4569 Loss:  0.8983284\n",
      "Training Step:  4570 Loss:  0.8981799\n",
      "Training Step:  4571 Loss:  0.898031\n",
      "Training Step:  4572 Loss:  0.89788246\n",
      "Training Step:  4573 Loss:  0.8977343\n",
      "Training Step:  4574 Loss:  0.8975859\n",
      "Training Step:  4575 Loss:  0.89743775\n",
      "Training Step:  4576 Loss:  0.8972896\n",
      "Training Step:  4577 Loss:  0.8971408\n",
      "Training Step:  4578 Loss:  0.89699334\n",
      "Training Step:  4579 Loss:  0.8968459\n",
      "Training Step:  4580 Loss:  0.89669824\n",
      "Training Step:  4581 Loss:  0.8965504\n",
      "Training Step:  4582 Loss:  0.89640266\n",
      "Training Step:  4583 Loss:  0.8962553\n",
      "Training Step:  4584 Loss:  0.8961084\n",
      "Training Step:  4585 Loss:  0.895961\n",
      "Training Step:  4586 Loss:  0.8958133\n",
      "Training Step:  4587 Loss:  0.8956667\n",
      "Training Step:  4588 Loss:  0.89552003\n",
      "Training Step:  4589 Loss:  0.89537346\n",
      "Training Step:  4590 Loss:  0.89522654\n",
      "Training Step:  4591 Loss:  0.89507985\n",
      "Training Step:  4592 Loss:  0.8949334\n",
      "Training Step:  4593 Loss:  0.89478767\n",
      "Training Step:  4594 Loss:  0.89464074\n",
      "Training Step:  4595 Loss:  0.8944955\n",
      "Training Step:  4596 Loss:  0.8943495\n",
      "Training Step:  4597 Loss:  0.89420307\n",
      "Training Step:  4598 Loss:  0.89405656\n",
      "Training Step:  4599 Loss:  0.8939122\n",
      "Training Step:  4600 Loss:  0.8937666\n",
      "Training Step:  4601 Loss:  0.89362085\n",
      "Training Step:  4602 Loss:  0.89347523\n",
      "Training Step:  4603 Loss:  0.8933301\n",
      "Training Step:  4604 Loss:  0.8931841\n",
      "Training Step:  4605 Loss:  0.8930401\n",
      "Training Step:  4606 Loss:  0.8928946\n",
      "Training Step:  4607 Loss:  0.89274967\n",
      "Training Step:  4608 Loss:  0.8926058\n",
      "Training Step:  4609 Loss:  0.8924611\n",
      "Training Step:  4610 Loss:  0.8923159\n",
      "Training Step:  4611 Loss:  0.8921721\n",
      "Training Step:  4612 Loss:  0.8920274\n",
      "Training Step:  4613 Loss:  0.89188313\n",
      "Training Step:  4614 Loss:  0.8917393\n",
      "Training Step:  4615 Loss:  0.89159507\n",
      "Training Step:  4616 Loss:  0.8914517\n",
      "Training Step:  4617 Loss:  0.89130753\n",
      "Training Step:  4618 Loss:  0.8911647\n",
      "Training Step:  4619 Loss:  0.89102066\n",
      "Training Step:  4620 Loss:  0.890877\n",
      "Training Step:  4621 Loss:  0.8907334\n",
      "Training Step:  4622 Loss:  0.89059067\n",
      "Training Step:  4623 Loss:  0.89044714\n",
      "Training Step:  4624 Loss:  0.89030504\n",
      "Training Step:  4625 Loss:  0.8901619\n",
      "Training Step:  4626 Loss:  0.89001817\n",
      "Training Step:  4627 Loss:  0.8898758\n",
      "Training Step:  4628 Loss:  0.88973284\n",
      "Training Step:  4629 Loss:  0.88959026\n",
      "Training Step:  4630 Loss:  0.88944834\n",
      "Training Step:  4631 Loss:  0.88930655\n",
      "Training Step:  4632 Loss:  0.88916403\n",
      "Training Step:  4633 Loss:  0.8890214\n",
      "Training Step:  4634 Loss:  0.88888055\n",
      "Training Step:  4635 Loss:  0.8887381\n",
      "Training Step:  4636 Loss:  0.8885957\n",
      "Training Step:  4637 Loss:  0.8884537\n",
      "Training Step:  4638 Loss:  0.8883134\n",
      "Training Step:  4639 Loss:  0.8881711\n",
      "Training Step:  4640 Loss:  0.8880298\n",
      "Training Step:  4641 Loss:  0.88788843\n",
      "Training Step:  4642 Loss:  0.88774705\n",
      "Training Step:  4643 Loss:  0.8876066\n",
      "Training Step:  4644 Loss:  0.8874663\n",
      "Training Step:  4645 Loss:  0.8873239\n",
      "Training Step:  4646 Loss:  0.887184\n",
      "Training Step:  4647 Loss:  0.8870435\n",
      "Training Step:  4648 Loss:  0.88690233\n",
      "Training Step:  4649 Loss:  0.8867626\n",
      "Training Step:  4650 Loss:  0.8866222\n",
      "Training Step:  4651 Loss:  0.88648134\n",
      "Training Step:  4652 Loss:  0.88634133\n",
      "Training Step:  4653 Loss:  0.88620174\n",
      "Training Step:  4654 Loss:  0.88606143\n",
      "Training Step:  4655 Loss:  0.88592184\n",
      "Training Step:  4656 Loss:  0.88578224\n",
      "Training Step:  4657 Loss:  0.8856409\n",
      "Training Step:  4658 Loss:  0.8855026\n",
      "Training Step:  4659 Loss:  0.8853637\n",
      "Training Step:  4660 Loss:  0.8852246\n",
      "Training Step:  4661 Loss:  0.88508445\n",
      "Training Step:  4662 Loss:  0.88494503\n",
      "Training Step:  4663 Loss:  0.88480616\n",
      "Training Step:  4664 Loss:  0.88466775\n",
      "Training Step:  4665 Loss:  0.88452834\n",
      "Training Step:  4666 Loss:  0.88438994\n",
      "Training Step:  4667 Loss:  0.8842518\n",
      "Training Step:  4668 Loss:  0.8841139\n",
      "Training Step:  4669 Loss:  0.8839754\n",
      "Training Step:  4670 Loss:  0.88383687\n",
      "Training Step:  4671 Loss:  0.8836993\n",
      "Training Step:  4672 Loss:  0.88355947\n",
      "Training Step:  4673 Loss:  0.8834221\n",
      "Training Step:  4674 Loss:  0.883284\n",
      "Training Step:  4675 Loss:  0.88314706\n",
      "Training Step:  4676 Loss:  0.8830084\n",
      "Training Step:  4677 Loss:  0.8828712\n",
      "Training Step:  4678 Loss:  0.88273364\n",
      "Training Step:  4679 Loss:  0.88259554\n",
      "Training Step:  4680 Loss:  0.8824586\n",
      "Training Step:  4681 Loss:  0.8823216\n",
      "Training Step:  4682 Loss:  0.8821848\n",
      "Training Step:  4683 Loss:  0.882047\n",
      "Training Step:  4684 Loss:  0.8819103\n",
      "Training Step:  4685 Loss:  0.88177407\n",
      "Training Step:  4686 Loss:  0.8816378\n",
      "Training Step:  4687 Loss:  0.8815004\n",
      "Training Step:  4688 Loss:  0.881364\n",
      "Training Step:  4689 Loss:  0.8812275\n",
      "Training Step:  4690 Loss:  0.8810906\n",
      "Training Step:  4691 Loss:  0.88095605\n",
      "Training Step:  4692 Loss:  0.88081825\n",
      "Training Step:  4693 Loss:  0.8806823\n",
      "Training Step:  4694 Loss:  0.88054705\n",
      "Training Step:  4695 Loss:  0.8804107\n",
      "Training Step:  4696 Loss:  0.88027644\n",
      "Training Step:  4697 Loss:  0.8801401\n",
      "Training Step:  4698 Loss:  0.88000405\n",
      "Training Step:  4699 Loss:  0.8798688\n",
      "Training Step:  4700 Loss:  0.8797332\n",
      "Training Step:  4701 Loss:  0.879598\n",
      "Training Step:  4702 Loss:  0.87946326\n",
      "Training Step:  4703 Loss:  0.8793281\n",
      "Training Step:  4704 Loss:  0.87919307\n",
      "Training Step:  4705 Loss:  0.8790581\n",
      "Training Step:  4706 Loss:  0.87892294\n",
      "Training Step:  4707 Loss:  0.8787887\n",
      "Training Step:  4708 Loss:  0.87865424\n",
      "Training Step:  4709 Loss:  0.8785192\n",
      "Training Step:  4710 Loss:  0.878385\n",
      "Training Step:  4711 Loss:  0.8782508\n",
      "Training Step:  4712 Loss:  0.878117\n",
      "Training Step:  4713 Loss:  0.87798274\n",
      "Training Step:  4714 Loss:  0.8778488\n",
      "Training Step:  4715 Loss:  0.8777159\n",
      "Training Step:  4716 Loss:  0.8775825\n",
      "Training Step:  4717 Loss:  0.87744856\n",
      "Training Step:  4718 Loss:  0.87731475\n",
      "Training Step:  4719 Loss:  0.87718105\n",
      "Training Step:  4720 Loss:  0.87704676\n",
      "Training Step:  4721 Loss:  0.8769143\n",
      "Training Step:  4722 Loss:  0.8767808\n",
      "Training Step:  4723 Loss:  0.87664783\n",
      "Training Step:  4724 Loss:  0.8765151\n",
      "Training Step:  4725 Loss:  0.8763821\n",
      "Training Step:  4726 Loss:  0.8762497\n",
      "Training Step:  4727 Loss:  0.87611675\n",
      "Training Step:  4728 Loss:  0.87598443\n",
      "Training Step:  4729 Loss:  0.87585115\n",
      "Training Step:  4730 Loss:  0.8757186\n",
      "Training Step:  4731 Loss:  0.87558645\n",
      "Training Step:  4732 Loss:  0.8754542\n",
      "Training Step:  4733 Loss:  0.8753214\n",
      "Training Step:  4734 Loss:  0.87519026\n",
      "Training Step:  4735 Loss:  0.87505853\n",
      "Training Step:  4736 Loss:  0.8749255\n",
      "Training Step:  4737 Loss:  0.87479424\n",
      "Training Step:  4738 Loss:  0.87466264\n",
      "Training Step:  4739 Loss:  0.87453026\n",
      "Training Step:  4740 Loss:  0.8744006\n",
      "Training Step:  4741 Loss:  0.8742686\n",
      "Training Step:  4742 Loss:  0.8741377\n",
      "Training Step:  4743 Loss:  0.87400687\n",
      "Training Step:  4744 Loss:  0.8738753\n",
      "Training Step:  4745 Loss:  0.87374413\n",
      "Training Step:  4746 Loss:  0.87361264\n",
      "Training Step:  4747 Loss:  0.8734824\n",
      "Training Step:  4748 Loss:  0.87335247\n",
      "Training Step:  4749 Loss:  0.87322104\n",
      "Training Step:  4750 Loss:  0.873091\n",
      "Training Step:  4751 Loss:  0.8729598\n",
      "Training Step:  4752 Loss:  0.8728302\n",
      "Training Step:  4753 Loss:  0.87269974\n",
      "Training Step:  4754 Loss:  0.8725698\n",
      "Training Step:  4755 Loss:  0.87244\n",
      "Training Step:  4756 Loss:  0.8723104\n",
      "Training Step:  4757 Loss:  0.8721797\n",
      "Training Step:  4758 Loss:  0.87204957\n",
      "Training Step:  4759 Loss:  0.8719207\n",
      "Training Step:  4760 Loss:  0.87178993\n",
      "Training Step:  4761 Loss:  0.87166154\n",
      "Training Step:  4762 Loss:  0.8715317\n",
      "Training Step:  4763 Loss:  0.8714025\n",
      "Training Step:  4764 Loss:  0.8712734\n",
      "Training Step:  4765 Loss:  0.8711443\n",
      "Training Step:  4766 Loss:  0.8710153\n",
      "Training Step:  4767 Loss:  0.87088656\n",
      "Training Step:  4768 Loss:  0.8707575\n",
      "Training Step:  4769 Loss:  0.8706288\n",
      "Training Step:  4770 Loss:  0.8704995\n",
      "Training Step:  4771 Loss:  0.87037206\n",
      "Training Step:  4772 Loss:  0.87024343\n",
      "Training Step:  4773 Loss:  0.87011534\n",
      "Training Step:  4774 Loss:  0.8699862\n",
      "Training Step:  4775 Loss:  0.86985826\n",
      "Training Step:  4776 Loss:  0.8697306\n",
      "Training Step:  4777 Loss:  0.8696021\n",
      "Training Step:  4778 Loss:  0.8694736\n",
      "Training Step:  4779 Loss:  0.8693465\n",
      "Training Step:  4780 Loss:  0.86921924\n",
      "Training Step:  4781 Loss:  0.8690915\n",
      "Training Step:  4782 Loss:  0.8689641\n",
      "Training Step:  4783 Loss:  0.86883664\n",
      "Training Step:  4784 Loss:  0.86870915\n",
      "Training Step:  4785 Loss:  0.86858094\n",
      "Training Step:  4786 Loss:  0.8684547\n",
      "Training Step:  4787 Loss:  0.86832786\n",
      "Training Step:  4788 Loss:  0.8682004\n",
      "Training Step:  4789 Loss:  0.8680736\n",
      "Training Step:  4790 Loss:  0.8679467\n",
      "Training Step:  4791 Loss:  0.8678197\n",
      "Training Step:  4792 Loss:  0.8676929\n",
      "Training Step:  4793 Loss:  0.86756635\n",
      "Training Step:  4794 Loss:  0.8674402\n",
      "Training Step:  4795 Loss:  0.8673143\n",
      "Training Step:  4796 Loss:  0.8671882\n",
      "Training Step:  4797 Loss:  0.8670617\n",
      "Training Step:  4798 Loss:  0.86693585\n",
      "Training Step:  4799 Loss:  0.8668096\n",
      "Training Step:  4800 Loss:  0.8666836\n",
      "Training Step:  4801 Loss:  0.8665587\n",
      "Training Step:  4802 Loss:  0.8664322\n",
      "Training Step:  4803 Loss:  0.8663067\n",
      "Training Step:  4804 Loss:  0.86618096\n",
      "Training Step:  4805 Loss:  0.8660555\n",
      "Training Step:  4806 Loss:  0.8659297\n",
      "Training Step:  4807 Loss:  0.8658054\n",
      "Training Step:  4808 Loss:  0.86568004\n",
      "Training Step:  4809 Loss:  0.8655559\n",
      "Training Step:  4810 Loss:  0.86543036\n",
      "Training Step:  4811 Loss:  0.8653053\n",
      "Training Step:  4812 Loss:  0.8651796\n",
      "Training Step:  4813 Loss:  0.8650549\n",
      "Training Step:  4814 Loss:  0.86493075\n",
      "Training Step:  4815 Loss:  0.8648069\n",
      "Training Step:  4816 Loss:  0.8646817\n",
      "Training Step:  4817 Loss:  0.86455727\n",
      "Training Step:  4818 Loss:  0.8644335\n",
      "Training Step:  4819 Loss:  0.8643094\n",
      "Training Step:  4820 Loss:  0.8641852\n",
      "Training Step:  4821 Loss:  0.8640611\n",
      "Training Step:  4822 Loss:  0.8639374\n",
      "Training Step:  4823 Loss:  0.86381364\n",
      "Training Step:  4824 Loss:  0.8636889\n",
      "Training Step:  4825 Loss:  0.8635657\n",
      "Training Step:  4826 Loss:  0.8634428\n",
      "Training Step:  4827 Loss:  0.8633189\n",
      "Training Step:  4828 Loss:  0.8631952\n",
      "Training Step:  4829 Loss:  0.8630726\n",
      "Training Step:  4830 Loss:  0.8629493\n",
      "Training Step:  4831 Loss:  0.8628265\n",
      "Training Step:  4832 Loss:  0.86270195\n",
      "Training Step:  4833 Loss:  0.8625794\n",
      "Training Step:  4834 Loss:  0.8624563\n",
      "Training Step:  4835 Loss:  0.8623344\n",
      "Training Step:  4836 Loss:  0.8622123\n",
      "Training Step:  4837 Loss:  0.8620884\n",
      "Training Step:  4838 Loss:  0.86196655\n",
      "Training Step:  4839 Loss:  0.8618439\n",
      "Training Step:  4840 Loss:  0.86172205\n",
      "Training Step:  4841 Loss:  0.86159825\n",
      "Training Step:  4842 Loss:  0.8614774\n",
      "Training Step:  4843 Loss:  0.8613553\n",
      "Training Step:  4844 Loss:  0.8612337\n",
      "Training Step:  4845 Loss:  0.861111\n",
      "Training Step:  4846 Loss:  0.8609899\n",
      "Training Step:  4847 Loss:  0.8608688\n",
      "Training Step:  4848 Loss:  0.8607462\n",
      "Training Step:  4849 Loss:  0.8606252\n",
      "Training Step:  4850 Loss:  0.86050355\n",
      "Training Step:  4851 Loss:  0.8603828\n",
      "Training Step:  4852 Loss:  0.86026\n",
      "Training Step:  4853 Loss:  0.8601392\n",
      "Training Step:  4854 Loss:  0.8600186\n",
      "Training Step:  4855 Loss:  0.85989726\n",
      "Training Step:  4856 Loss:  0.85977656\n",
      "Training Step:  4857 Loss:  0.85965633\n",
      "Training Step:  4858 Loss:  0.85953397\n",
      "Training Step:  4859 Loss:  0.8594138\n",
      "Training Step:  4860 Loss:  0.8592939\n",
      "Training Step:  4861 Loss:  0.8591726\n",
      "Training Step:  4862 Loss:  0.85905236\n",
      "Training Step:  4863 Loss:  0.8589329\n",
      "Training Step:  4864 Loss:  0.858812\n",
      "Training Step:  4865 Loss:  0.85869205\n",
      "Training Step:  4866 Loss:  0.85857147\n",
      "Training Step:  4867 Loss:  0.8584521\n",
      "Training Step:  4868 Loss:  0.85833263\n",
      "Training Step:  4869 Loss:  0.85821223\n",
      "Training Step:  4870 Loss:  0.8580925\n",
      "Training Step:  4871 Loss:  0.85797286\n",
      "Training Step:  4872 Loss:  0.8578533\n",
      "Training Step:  4873 Loss:  0.8577345\n",
      "Training Step:  4874 Loss:  0.85761505\n",
      "Training Step:  4875 Loss:  0.8574959\n",
      "Training Step:  4876 Loss:  0.85737586\n",
      "Training Step:  4877 Loss:  0.8572569\n",
      "Training Step:  4878 Loss:  0.85713696\n",
      "Training Step:  4879 Loss:  0.85701907\n",
      "Training Step:  4880 Loss:  0.85689986\n",
      "Training Step:  4881 Loss:  0.8567813\n",
      "Training Step:  4882 Loss:  0.8566627\n",
      "Training Step:  4883 Loss:  0.8565448\n",
      "Training Step:  4884 Loss:  0.8564263\n",
      "Training Step:  4885 Loss:  0.856307\n",
      "Training Step:  4886 Loss:  0.8561894\n",
      "Training Step:  4887 Loss:  0.85607076\n",
      "Training Step:  4888 Loss:  0.8559523\n",
      "Training Step:  4889 Loss:  0.85583425\n",
      "Training Step:  4890 Loss:  0.8557171\n",
      "Training Step:  4891 Loss:  0.85559845\n",
      "Training Step:  4892 Loss:  0.8554808\n",
      "Training Step:  4893 Loss:  0.8553623\n",
      "Training Step:  4894 Loss:  0.8552448\n",
      "Training Step:  4895 Loss:  0.8551269\n",
      "Training Step:  4896 Loss:  0.8550086\n",
      "Training Step:  4897 Loss:  0.85489225\n",
      "Training Step:  4898 Loss:  0.85477436\n",
      "Training Step:  4899 Loss:  0.8546583\n",
      "Training Step:  4900 Loss:  0.85454\n",
      "Training Step:  4901 Loss:  0.85442436\n",
      "Training Step:  4902 Loss:  0.85430616\n",
      "Training Step:  4903 Loss:  0.8541893\n",
      "Training Step:  4904 Loss:  0.8540722\n",
      "Training Step:  4905 Loss:  0.85395575\n",
      "Training Step:  4906 Loss:  0.8538394\n",
      "Training Step:  4907 Loss:  0.8537223\n",
      "Training Step:  4908 Loss:  0.8536058\n",
      "Training Step:  4909 Loss:  0.85349035\n",
      "Training Step:  4910 Loss:  0.85337317\n",
      "Training Step:  4911 Loss:  0.8532576\n",
      "Training Step:  4912 Loss:  0.8531404\n",
      "Training Step:  4913 Loss:  0.8530238\n",
      "Training Step:  4914 Loss:  0.8529091\n",
      "Training Step:  4915 Loss:  0.8527932\n",
      "Training Step:  4916 Loss:  0.85267663\n",
      "Training Step:  4917 Loss:  0.8525615\n",
      "Training Step:  4918 Loss:  0.8524456\n",
      "Training Step:  4919 Loss:  0.8523295\n",
      "Training Step:  4920 Loss:  0.8522153\n",
      "Training Step:  4921 Loss:  0.85209924\n",
      "Training Step:  4922 Loss:  0.8519827\n",
      "Training Step:  4923 Loss:  0.8518684\n",
      "Training Step:  4924 Loss:  0.8517534\n",
      "Training Step:  4925 Loss:  0.851638\n",
      "Training Step:  4926 Loss:  0.85152406\n",
      "Training Step:  4927 Loss:  0.8514078\n",
      "Training Step:  4928 Loss:  0.85129315\n",
      "Training Step:  4929 Loss:  0.8511778\n",
      "Training Step:  4930 Loss:  0.8510635\n",
      "Training Step:  4931 Loss:  0.8509489\n",
      "Training Step:  4932 Loss:  0.85083383\n",
      "Training Step:  4933 Loss:  0.8507204\n",
      "Training Step:  4934 Loss:  0.85060555\n",
      "Training Step:  4935 Loss:  0.8504916\n",
      "Training Step:  4936 Loss:  0.85037655\n",
      "Training Step:  4937 Loss:  0.85026264\n",
      "Training Step:  4938 Loss:  0.85014963\n",
      "Training Step:  4939 Loss:  0.8500353\n",
      "Training Step:  4940 Loss:  0.8499217\n",
      "Training Step:  4941 Loss:  0.84980667\n",
      "Training Step:  4942 Loss:  0.8496939\n",
      "Training Step:  4943 Loss:  0.8495796\n",
      "Training Step:  4944 Loss:  0.84946555\n",
      "Training Step:  4945 Loss:  0.8493526\n",
      "Training Step:  4946 Loss:  0.849239\n",
      "Training Step:  4947 Loss:  0.8491267\n",
      "Training Step:  4948 Loss:  0.84901243\n",
      "Training Step:  4949 Loss:  0.848899\n",
      "Training Step:  4950 Loss:  0.8487867\n",
      "Training Step:  4951 Loss:  0.8486736\n",
      "Training Step:  4952 Loss:  0.8485602\n",
      "Training Step:  4953 Loss:  0.848448\n",
      "Training Step:  4954 Loss:  0.8483357\n",
      "Training Step:  4955 Loss:  0.8482224\n",
      "Training Step:  4956 Loss:  0.8481095\n",
      "Training Step:  4957 Loss:  0.8479966\n",
      "Training Step:  4958 Loss:  0.8478844\n",
      "Training Step:  4959 Loss:  0.8477713\n",
      "Training Step:  4960 Loss:  0.8476605\n",
      "Training Step:  4961 Loss:  0.84754807\n",
      "Training Step:  4962 Loss:  0.8474358\n",
      "Training Step:  4963 Loss:  0.8473241\n",
      "Training Step:  4964 Loss:  0.8472108\n",
      "Training Step:  4965 Loss:  0.84709996\n",
      "Training Step:  4966 Loss:  0.8469875\n",
      "Training Step:  4967 Loss:  0.8468761\n",
      "Training Step:  4968 Loss:  0.8467639\n",
      "Training Step:  4969 Loss:  0.84665287\n",
      "Training Step:  4970 Loss:  0.8465408\n",
      "Training Step:  4971 Loss:  0.84643036\n",
      "Training Step:  4972 Loss:  0.8463191\n",
      "Training Step:  4973 Loss:  0.8462084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  4974 Loss:  0.8460959\n",
      "Training Step:  4975 Loss:  0.8459856\n",
      "Training Step:  4976 Loss:  0.84587425\n",
      "Training Step:  4977 Loss:  0.8457625\n",
      "Training Step:  4978 Loss:  0.84565246\n",
      "Training Step:  4979 Loss:  0.84554136\n",
      "Training Step:  4980 Loss:  0.8454303\n",
      "Training Step:  4981 Loss:  0.84532017\n",
      "Training Step:  4982 Loss:  0.8452096\n",
      "Training Step:  4983 Loss:  0.84509873\n",
      "Training Step:  4984 Loss:  0.84498864\n",
      "Training Step:  4985 Loss:  0.8448787\n",
      "Training Step:  4986 Loss:  0.84476846\n",
      "Training Step:  4987 Loss:  0.84465736\n",
      "Training Step:  4988 Loss:  0.844548\n",
      "Training Step:  4989 Loss:  0.84443796\n",
      "Training Step:  4990 Loss:  0.8443282\n",
      "Training Step:  4991 Loss:  0.8442185\n",
      "Training Step:  4992 Loss:  0.8441086\n",
      "Training Step:  4993 Loss:  0.84399885\n",
      "Training Step:  4994 Loss:  0.8438892\n",
      "Training Step:  4995 Loss:  0.8437792\n",
      "Training Step:  4996 Loss:  0.84366894\n",
      "Training Step:  4997 Loss:  0.8435605\n",
      "Training Step:  4998 Loss:  0.8434511\n",
      "Training Step:  4999 Loss:  0.84334147\n",
      "Training Step:  5000 Loss:  0.8432333\n",
      "Training Step:  5001 Loss:  0.8431238\n",
      "Training Step:  5002 Loss:  0.8430144\n",
      "Training Step:  5003 Loss:  0.8429057\n",
      "Training Step:  5004 Loss:  0.84279734\n",
      "Training Step:  5005 Loss:  0.84268844\n",
      "Training Step:  5006 Loss:  0.842579\n",
      "Training Step:  5007 Loss:  0.8424702\n",
      "Training Step:  5008 Loss:  0.8423627\n",
      "Training Step:  5009 Loss:  0.84225416\n",
      "Training Step:  5010 Loss:  0.8421455\n",
      "Training Step:  5011 Loss:  0.8420373\n",
      "Training Step:  5012 Loss:  0.84192926\n",
      "Training Step:  5013 Loss:  0.8418203\n",
      "Training Step:  5014 Loss:  0.84171206\n",
      "Training Step:  5015 Loss:  0.84160453\n",
      "Training Step:  5016 Loss:  0.84149706\n",
      "Training Step:  5017 Loss:  0.84138966\n",
      "Training Step:  5018 Loss:  0.8412815\n",
      "Training Step:  5019 Loss:  0.8411727\n",
      "Training Step:  5020 Loss:  0.84106547\n",
      "Training Step:  5021 Loss:  0.8409587\n",
      "Training Step:  5022 Loss:  0.8408506\n",
      "Training Step:  5023 Loss:  0.8407435\n",
      "Training Step:  5024 Loss:  0.84063613\n",
      "Training Step:  5025 Loss:  0.8405285\n",
      "Training Step:  5026 Loss:  0.8404212\n",
      "Training Step:  5027 Loss:  0.84031415\n",
      "Training Step:  5028 Loss:  0.8402083\n",
      "Training Step:  5029 Loss:  0.8400997\n",
      "Training Step:  5030 Loss:  0.8399935\n",
      "Training Step:  5031 Loss:  0.8398865\n",
      "Training Step:  5032 Loss:  0.8397801\n",
      "Training Step:  5033 Loss:  0.83967304\n",
      "Training Step:  5034 Loss:  0.8395663\n",
      "Training Step:  5035 Loss:  0.8394605\n",
      "Training Step:  5036 Loss:  0.8393541\n",
      "Training Step:  5037 Loss:  0.839247\n",
      "Training Step:  5038 Loss:  0.83914137\n",
      "Training Step:  5039 Loss:  0.8390348\n",
      "Training Step:  5040 Loss:  0.8389285\n",
      "Training Step:  5041 Loss:  0.8388229\n",
      "Training Step:  5042 Loss:  0.8387165\n",
      "Training Step:  5043 Loss:  0.83861136\n",
      "Training Step:  5044 Loss:  0.8385044\n",
      "Training Step:  5045 Loss:  0.83839935\n",
      "Training Step:  5046 Loss:  0.83829373\n",
      "Training Step:  5047 Loss:  0.8381875\n",
      "Training Step:  5048 Loss:  0.8380825\n",
      "Training Step:  5049 Loss:  0.83797705\n",
      "Training Step:  5050 Loss:  0.8378718\n",
      "Training Step:  5051 Loss:  0.8377657\n",
      "Training Step:  5052 Loss:  0.83766115\n",
      "Training Step:  5053 Loss:  0.8375554\n",
      "Training Step:  5054 Loss:  0.83745\n",
      "Training Step:  5055 Loss:  0.83734643\n",
      "Training Step:  5056 Loss:  0.8372408\n",
      "Training Step:  5057 Loss:  0.8371361\n",
      "Training Step:  5058 Loss:  0.8370306\n",
      "Training Step:  5059 Loss:  0.8369262\n",
      "Training Step:  5060 Loss:  0.836822\n",
      "Training Step:  5061 Loss:  0.836717\n",
      "Training Step:  5062 Loss:  0.8366133\n",
      "Training Step:  5063 Loss:  0.83650756\n",
      "Training Step:  5064 Loss:  0.83640367\n",
      "Training Step:  5065 Loss:  0.8362996\n",
      "Training Step:  5066 Loss:  0.83619547\n",
      "Training Step:  5067 Loss:  0.83609134\n",
      "Training Step:  5068 Loss:  0.83598745\n",
      "Training Step:  5069 Loss:  0.83588326\n",
      "Training Step:  5070 Loss:  0.8357787\n",
      "Training Step:  5071 Loss:  0.8356762\n",
      "Training Step:  5072 Loss:  0.83557224\n",
      "Training Step:  5073 Loss:  0.8354677\n",
      "Training Step:  5074 Loss:  0.83536434\n",
      "Training Step:  5075 Loss:  0.83526075\n",
      "Training Step:  5076 Loss:  0.83515763\n",
      "Training Step:  5077 Loss:  0.83505356\n",
      "Training Step:  5078 Loss:  0.8349499\n",
      "Training Step:  5079 Loss:  0.83484745\n",
      "Training Step:  5080 Loss:  0.8347448\n",
      "Training Step:  5081 Loss:  0.8346404\n",
      "Training Step:  5082 Loss:  0.8345382\n",
      "Training Step:  5083 Loss:  0.83443475\n",
      "Training Step:  5084 Loss:  0.8343321\n",
      "Training Step:  5085 Loss:  0.8342287\n",
      "Training Step:  5086 Loss:  0.83412576\n",
      "Training Step:  5087 Loss:  0.83402276\n",
      "Training Step:  5088 Loss:  0.83392084\n",
      "Training Step:  5089 Loss:  0.83381885\n",
      "Training Step:  5090 Loss:  0.8337161\n",
      "Training Step:  5091 Loss:  0.83361375\n",
      "Training Step:  5092 Loss:  0.8335123\n",
      "Training Step:  5093 Loss:  0.8334085\n",
      "Training Step:  5094 Loss:  0.8333073\n",
      "Training Step:  5095 Loss:  0.8332045\n",
      "Training Step:  5096 Loss:  0.8331022\n",
      "Training Step:  5097 Loss:  0.83300126\n",
      "Training Step:  5098 Loss:  0.8328988\n",
      "Training Step:  5099 Loss:  0.83279663\n",
      "Training Step:  5100 Loss:  0.832695\n",
      "Training Step:  5101 Loss:  0.8325932\n",
      "Training Step:  5102 Loss:  0.83249116\n",
      "Training Step:  5103 Loss:  0.83239055\n",
      "Training Step:  5104 Loss:  0.832289\n",
      "Training Step:  5105 Loss:  0.8321873\n",
      "Training Step:  5106 Loss:  0.8320853\n",
      "Training Step:  5107 Loss:  0.83198494\n",
      "Training Step:  5108 Loss:  0.83188385\n",
      "Training Step:  5109 Loss:  0.8317824\n",
      "Training Step:  5110 Loss:  0.8316818\n",
      "Training Step:  5111 Loss:  0.8315798\n",
      "Training Step:  5112 Loss:  0.83148015\n",
      "Training Step:  5113 Loss:  0.83137846\n",
      "Training Step:  5114 Loss:  0.83127767\n",
      "Training Step:  5115 Loss:  0.83117723\n",
      "Training Step:  5116 Loss:  0.83107644\n",
      "Training Step:  5117 Loss:  0.83097506\n",
      "Training Step:  5118 Loss:  0.8308758\n",
      "Training Step:  5119 Loss:  0.8307754\n",
      "Training Step:  5120 Loss:  0.8306744\n",
      "Training Step:  5121 Loss:  0.8305744\n",
      "Training Step:  5122 Loss:  0.8304739\n",
      "Training Step:  5123 Loss:  0.8303734\n",
      "Training Step:  5124 Loss:  0.83027434\n",
      "Training Step:  5125 Loss:  0.83017385\n",
      "Training Step:  5126 Loss:  0.83007413\n",
      "Training Step:  5127 Loss:  0.82997364\n",
      "Training Step:  5128 Loss:  0.8298743\n",
      "Training Step:  5129 Loss:  0.8297743\n",
      "Training Step:  5130 Loss:  0.8296746\n",
      "Training Step:  5131 Loss:  0.829576\n",
      "Training Step:  5132 Loss:  0.8294753\n",
      "Training Step:  5133 Loss:  0.8293768\n",
      "Training Step:  5134 Loss:  0.8292767\n",
      "Training Step:  5135 Loss:  0.8291787\n",
      "Training Step:  5136 Loss:  0.82907844\n",
      "Training Step:  5137 Loss:  0.8289787\n",
      "Training Step:  5138 Loss:  0.82888055\n",
      "Training Step:  5139 Loss:  0.82878065\n",
      "Training Step:  5140 Loss:  0.8286827\n",
      "Training Step:  5141 Loss:  0.82858366\n",
      "Training Step:  5142 Loss:  0.8284843\n",
      "Training Step:  5143 Loss:  0.82838494\n",
      "Training Step:  5144 Loss:  0.8282874\n",
      "Training Step:  5145 Loss:  0.8281891\n",
      "Training Step:  5146 Loss:  0.8280896\n",
      "Training Step:  5147 Loss:  0.82799137\n",
      "Training Step:  5148 Loss:  0.8278922\n",
      "Training Step:  5149 Loss:  0.82779396\n",
      "Training Step:  5150 Loss:  0.82769567\n",
      "Training Step:  5151 Loss:  0.82759774\n",
      "Training Step:  5152 Loss:  0.8274996\n",
      "Training Step:  5153 Loss:  0.827402\n",
      "Training Step:  5154 Loss:  0.82730275\n",
      "Training Step:  5155 Loss:  0.82720536\n",
      "Training Step:  5156 Loss:  0.82710785\n",
      "Training Step:  5157 Loss:  0.8270107\n",
      "Training Step:  5158 Loss:  0.8269124\n",
      "Training Step:  5159 Loss:  0.8268149\n",
      "Training Step:  5160 Loss:  0.8267173\n",
      "Training Step:  5161 Loss:  0.8266197\n",
      "Training Step:  5162 Loss:  0.826522\n",
      "Training Step:  5163 Loss:  0.82642376\n",
      "Training Step:  5164 Loss:  0.8263283\n",
      "Training Step:  5165 Loss:  0.82623076\n",
      "Training Step:  5166 Loss:  0.8261329\n",
      "Training Step:  5167 Loss:  0.82603556\n",
      "Training Step:  5168 Loss:  0.82593876\n",
      "Training Step:  5169 Loss:  0.82584167\n",
      "Training Step:  5170 Loss:  0.82574505\n",
      "Training Step:  5171 Loss:  0.82564753\n",
      "Training Step:  5172 Loss:  0.82555133\n",
      "Training Step:  5173 Loss:  0.82545406\n",
      "Training Step:  5174 Loss:  0.8253571\n",
      "Training Step:  5175 Loss:  0.8252611\n",
      "Training Step:  5176 Loss:  0.8251638\n",
      "Training Step:  5177 Loss:  0.82506824\n",
      "Training Step:  5178 Loss:  0.8249723\n",
      "Training Step:  5179 Loss:  0.8248756\n",
      "Training Step:  5180 Loss:  0.8247796\n",
      "Training Step:  5181 Loss:  0.8246834\n",
      "Training Step:  5182 Loss:  0.8245867\n",
      "Training Step:  5183 Loss:  0.8244915\n",
      "Training Step:  5184 Loss:  0.82439524\n",
      "Training Step:  5185 Loss:  0.8242993\n",
      "Training Step:  5186 Loss:  0.8242034\n",
      "Training Step:  5187 Loss:  0.82410824\n",
      "Training Step:  5188 Loss:  0.8240119\n",
      "Training Step:  5189 Loss:  0.8239161\n",
      "Training Step:  5190 Loss:  0.82382\n",
      "Training Step:  5191 Loss:  0.8237248\n",
      "Training Step:  5192 Loss:  0.82362926\n",
      "Training Step:  5193 Loss:  0.82353467\n",
      "Training Step:  5194 Loss:  0.82343847\n",
      "Training Step:  5195 Loss:  0.8233434\n",
      "Training Step:  5196 Loss:  0.8232467\n",
      "Training Step:  5197 Loss:  0.8231524\n",
      "Training Step:  5198 Loss:  0.8230574\n",
      "Training Step:  5199 Loss:  0.8229623\n",
      "Training Step:  5200 Loss:  0.8228675\n",
      "Training Step:  5201 Loss:  0.82277334\n",
      "Training Step:  5202 Loss:  0.82267857\n",
      "Training Step:  5203 Loss:  0.8225831\n",
      "Training Step:  5204 Loss:  0.8224884\n",
      "Training Step:  5205 Loss:  0.8223935\n",
      "Training Step:  5206 Loss:  0.8222991\n",
      "Training Step:  5207 Loss:  0.8222049\n",
      "Training Step:  5208 Loss:  0.8221116\n",
      "Training Step:  5209 Loss:  0.8220159\n",
      "Training Step:  5210 Loss:  0.8219212\n",
      "Training Step:  5211 Loss:  0.8218279\n",
      "Training Step:  5212 Loss:  0.82173395\n",
      "Training Step:  5213 Loss:  0.8216398\n",
      "Training Step:  5214 Loss:  0.8215448\n",
      "Training Step:  5215 Loss:  0.8214509\n",
      "Training Step:  5216 Loss:  0.82135785\n",
      "Training Step:  5217 Loss:  0.821263\n",
      "Training Step:  5218 Loss:  0.8211695\n",
      "Training Step:  5219 Loss:  0.8210756\n",
      "Training Step:  5220 Loss:  0.82098234\n",
      "Training Step:  5221 Loss:  0.82088935\n",
      "Training Step:  5222 Loss:  0.82079566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  5223 Loss:  0.82070166\n",
      "Training Step:  5224 Loss:  0.8206081\n",
      "Training Step:  5225 Loss:  0.82051426\n",
      "Training Step:  5226 Loss:  0.82042164\n",
      "Training Step:  5227 Loss:  0.82032835\n",
      "Training Step:  5228 Loss:  0.8202357\n",
      "Training Step:  5229 Loss:  0.8201422\n",
      "Training Step:  5230 Loss:  0.82004964\n",
      "Training Step:  5231 Loss:  0.819957\n",
      "Training Step:  5232 Loss:  0.81986433\n",
      "Training Step:  5233 Loss:  0.8197714\n",
      "Training Step:  5234 Loss:  0.81967777\n",
      "Training Step:  5235 Loss:  0.8195853\n",
      "Training Step:  5236 Loss:  0.8194928\n",
      "Training Step:  5237 Loss:  0.8194006\n",
      "Training Step:  5238 Loss:  0.8193065\n",
      "Training Step:  5239 Loss:  0.8192152\n",
      "Training Step:  5240 Loss:  0.81912327\n",
      "Training Step:  5241 Loss:  0.8190307\n",
      "Training Step:  5242 Loss:  0.8189383\n",
      "Training Step:  5243 Loss:  0.8188465\n",
      "Training Step:  5244 Loss:  0.81875455\n",
      "Training Step:  5245 Loss:  0.81866264\n",
      "Training Step:  5246 Loss:  0.81857\n",
      "Training Step:  5247 Loss:  0.8184776\n",
      "Training Step:  5248 Loss:  0.8183867\n",
      "Training Step:  5249 Loss:  0.81829447\n",
      "Training Step:  5250 Loss:  0.818202\n",
      "Training Step:  5251 Loss:  0.8181105\n",
      "Training Step:  5252 Loss:  0.81802016\n",
      "Training Step:  5253 Loss:  0.8179277\n",
      "Training Step:  5254 Loss:  0.8178364\n",
      "Training Step:  5255 Loss:  0.8177458\n",
      "Training Step:  5256 Loss:  0.8176526\n",
      "Training Step:  5257 Loss:  0.81756186\n",
      "Training Step:  5258 Loss:  0.8174716\n",
      "Training Step:  5259 Loss:  0.8173797\n",
      "Training Step:  5260 Loss:  0.8172883\n",
      "Training Step:  5261 Loss:  0.8171977\n",
      "Training Step:  5262 Loss:  0.81710577\n",
      "Training Step:  5263 Loss:  0.8170155\n",
      "Training Step:  5264 Loss:  0.8169259\n",
      "Training Step:  5265 Loss:  0.81683373\n",
      "Training Step:  5266 Loss:  0.81674373\n",
      "Training Step:  5267 Loss:  0.8166521\n",
      "Training Step:  5268 Loss:  0.81656265\n",
      "Training Step:  5269 Loss:  0.81647176\n",
      "Training Step:  5270 Loss:  0.8163803\n",
      "Training Step:  5271 Loss:  0.81629\n",
      "Training Step:  5272 Loss:  0.8162001\n",
      "Training Step:  5273 Loss:  0.81610954\n",
      "Training Step:  5274 Loss:  0.81601983\n",
      "Training Step:  5275 Loss:  0.81592906\n",
      "Training Step:  5276 Loss:  0.81583893\n",
      "Training Step:  5277 Loss:  0.815749\n",
      "Training Step:  5278 Loss:  0.8156598\n",
      "Training Step:  5279 Loss:  0.8155693\n",
      "Training Step:  5280 Loss:  0.8154795\n",
      "Training Step:  5281 Loss:  0.8153904\n",
      "Training Step:  5282 Loss:  0.8152989\n",
      "Training Step:  5283 Loss:  0.81521046\n",
      "Training Step:  5284 Loss:  0.8151204\n",
      "Training Step:  5285 Loss:  0.8150309\n",
      "Training Step:  5286 Loss:  0.81494075\n",
      "Training Step:  5287 Loss:  0.814852\n",
      "Training Step:  5288 Loss:  0.8147633\n",
      "Training Step:  5289 Loss:  0.8146735\n",
      "Training Step:  5290 Loss:  0.81458396\n",
      "Training Step:  5291 Loss:  0.8144946\n",
      "Training Step:  5292 Loss:  0.81440604\n",
      "Training Step:  5293 Loss:  0.81431675\n",
      "Training Step:  5294 Loss:  0.8142281\n",
      "Training Step:  5295 Loss:  0.8141386\n",
      "Training Step:  5296 Loss:  0.81405014\n",
      "Training Step:  5297 Loss:  0.8139603\n",
      "Training Step:  5298 Loss:  0.81387216\n",
      "Training Step:  5299 Loss:  0.8137835\n",
      "Training Step:  5300 Loss:  0.81369525\n",
      "Training Step:  5301 Loss:  0.8136064\n",
      "Training Step:  5302 Loss:  0.8135181\n",
      "Training Step:  5303 Loss:  0.81342936\n",
      "Training Step:  5304 Loss:  0.81334054\n",
      "Training Step:  5305 Loss:  0.8132516\n",
      "Training Step:  5306 Loss:  0.8131634\n",
      "Training Step:  5307 Loss:  0.813076\n",
      "Training Step:  5308 Loss:  0.81298757\n",
      "Training Step:  5309 Loss:  0.81289995\n",
      "Training Step:  5310 Loss:  0.8128118\n",
      "Training Step:  5311 Loss:  0.81272435\n",
      "Training Step:  5312 Loss:  0.81263554\n",
      "Training Step:  5313 Loss:  0.81254715\n",
      "Training Step:  5314 Loss:  0.81245965\n",
      "Training Step:  5315 Loss:  0.8123715\n",
      "Training Step:  5316 Loss:  0.8122848\n",
      "Training Step:  5317 Loss:  0.81219673\n",
      "Training Step:  5318 Loss:  0.81210893\n",
      "Training Step:  5319 Loss:  0.81202173\n",
      "Training Step:  5320 Loss:  0.8119342\n",
      "Training Step:  5321 Loss:  0.8118468\n",
      "Training Step:  5322 Loss:  0.8117596\n",
      "Training Step:  5323 Loss:  0.8116721\n",
      "Training Step:  5324 Loss:  0.8115853\n",
      "Training Step:  5325 Loss:  0.81149787\n",
      "Training Step:  5326 Loss:  0.8114109\n",
      "Training Step:  5327 Loss:  0.8113232\n",
      "Training Step:  5328 Loss:  0.81123656\n",
      "Training Step:  5329 Loss:  0.81114876\n",
      "Training Step:  5330 Loss:  0.811063\n",
      "Training Step:  5331 Loss:  0.8109758\n",
      "Training Step:  5332 Loss:  0.8108894\n",
      "Training Step:  5333 Loss:  0.81080246\n",
      "Training Step:  5334 Loss:  0.8107163\n",
      "Training Step:  5335 Loss:  0.8106289\n",
      "Training Step:  5336 Loss:  0.81054246\n",
      "Training Step:  5337 Loss:  0.8104566\n",
      "Training Step:  5338 Loss:  0.81036896\n",
      "Training Step:  5339 Loss:  0.81028455\n",
      "Training Step:  5340 Loss:  0.81019706\n",
      "Training Step:  5341 Loss:  0.8101114\n",
      "Training Step:  5342 Loss:  0.81002533\n",
      "Training Step:  5343 Loss:  0.8099388\n",
      "Training Step:  5344 Loss:  0.80985296\n",
      "Training Step:  5345 Loss:  0.8097659\n",
      "Training Step:  5346 Loss:  0.8096811\n",
      "Training Step:  5347 Loss:  0.80959505\n",
      "Training Step:  5348 Loss:  0.809509\n",
      "Training Step:  5349 Loss:  0.8094232\n",
      "Training Step:  5350 Loss:  0.80933774\n",
      "Training Step:  5351 Loss:  0.80925274\n",
      "Training Step:  5352 Loss:  0.8091661\n",
      "Training Step:  5353 Loss:  0.80908185\n",
      "Training Step:  5354 Loss:  0.80899584\n",
      "Training Step:  5355 Loss:  0.80891025\n",
      "Training Step:  5356 Loss:  0.80882454\n",
      "Training Step:  5357 Loss:  0.80873877\n",
      "Training Step:  5358 Loss:  0.80865484\n",
      "Training Step:  5359 Loss:  0.80856925\n",
      "Training Step:  5360 Loss:  0.80848384\n",
      "Training Step:  5361 Loss:  0.8083987\n",
      "Training Step:  5362 Loss:  0.8083144\n",
      "Training Step:  5363 Loss:  0.80822873\n",
      "Training Step:  5364 Loss:  0.80814373\n",
      "Training Step:  5365 Loss:  0.8080595\n",
      "Training Step:  5366 Loss:  0.8079735\n",
      "Training Step:  5367 Loss:  0.807889\n",
      "Training Step:  5368 Loss:  0.80780506\n",
      "Training Step:  5369 Loss:  0.8077209\n",
      "Training Step:  5370 Loss:  0.807636\n",
      "Training Step:  5371 Loss:  0.8075505\n",
      "Training Step:  5372 Loss:  0.8074666\n",
      "Training Step:  5373 Loss:  0.8073822\n",
      "Training Step:  5374 Loss:  0.80729884\n",
      "Training Step:  5375 Loss:  0.8072145\n",
      "Training Step:  5376 Loss:  0.80712986\n",
      "Training Step:  5377 Loss:  0.8070457\n",
      "Training Step:  5378 Loss:  0.80696094\n",
      "Training Step:  5379 Loss:  0.80687696\n",
      "Training Step:  5380 Loss:  0.8067928\n",
      "Training Step:  5381 Loss:  0.80670965\n",
      "Training Step:  5382 Loss:  0.80662596\n",
      "Training Step:  5383 Loss:  0.80654174\n",
      "Training Step:  5384 Loss:  0.8064583\n",
      "Training Step:  5385 Loss:  0.8063749\n",
      "Training Step:  5386 Loss:  0.8062916\n",
      "Training Step:  5387 Loss:  0.806207\n",
      "Training Step:  5388 Loss:  0.8061237\n",
      "Training Step:  5389 Loss:  0.8060398\n",
      "Training Step:  5390 Loss:  0.80595636\n",
      "Training Step:  5391 Loss:  0.80587363\n",
      "Training Step:  5392 Loss:  0.80579054\n",
      "Training Step:  5393 Loss:  0.8057069\n",
      "Training Step:  5394 Loss:  0.80562395\n",
      "Training Step:  5395 Loss:  0.80554026\n",
      "Training Step:  5396 Loss:  0.8054571\n",
      "Training Step:  5397 Loss:  0.80537397\n",
      "Training Step:  5398 Loss:  0.80529124\n",
      "Training Step:  5399 Loss:  0.8052088\n",
      "Training Step:  5400 Loss:  0.80512524\n",
      "Training Step:  5401 Loss:  0.80504304\n",
      "Training Step:  5402 Loss:  0.80495983\n",
      "Training Step:  5403 Loss:  0.8048775\n",
      "Training Step:  5404 Loss:  0.8047941\n",
      "Training Step:  5405 Loss:  0.80471164\n",
      "Training Step:  5406 Loss:  0.80462974\n",
      "Training Step:  5407 Loss:  0.8045466\n",
      "Training Step:  5408 Loss:  0.80446464\n",
      "Training Step:  5409 Loss:  0.80438197\n",
      "Training Step:  5410 Loss:  0.80429965\n",
      "Training Step:  5411 Loss:  0.8042179\n",
      "Training Step:  5412 Loss:  0.80413544\n",
      "Training Step:  5413 Loss:  0.8040538\n",
      "Training Step:  5414 Loss:  0.8039712\n",
      "Training Step:  5415 Loss:  0.8038885\n",
      "Training Step:  5416 Loss:  0.8038076\n",
      "Training Step:  5417 Loss:  0.80372477\n",
      "Training Step:  5418 Loss:  0.8036436\n",
      "Training Step:  5419 Loss:  0.80356187\n",
      "Training Step:  5420 Loss:  0.80347985\n",
      "Training Step:  5421 Loss:  0.80339897\n",
      "Training Step:  5422 Loss:  0.80331665\n",
      "Training Step:  5423 Loss:  0.8032351\n",
      "Training Step:  5424 Loss:  0.8031534\n",
      "Training Step:  5425 Loss:  0.80307245\n",
      "Training Step:  5426 Loss:  0.8029908\n",
      "Training Step:  5427 Loss:  0.8029087\n",
      "Training Step:  5428 Loss:  0.8028275\n",
      "Training Step:  5429 Loss:  0.8027464\n",
      "Training Step:  5430 Loss:  0.8026658\n",
      "Training Step:  5431 Loss:  0.8025842\n",
      "Training Step:  5432 Loss:  0.8025032\n",
      "Training Step:  5433 Loss:  0.8024224\n",
      "Training Step:  5434 Loss:  0.8023412\n",
      "Training Step:  5435 Loss:  0.8022599\n",
      "Training Step:  5436 Loss:  0.8021791\n",
      "Training Step:  5437 Loss:  0.80209804\n",
      "Training Step:  5438 Loss:  0.8020171\n",
      "Training Step:  5439 Loss:  0.80193686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  5440 Loss:  0.80185586\n",
      "Training Step:  5441 Loss:  0.8017757\n",
      "Training Step:  5442 Loss:  0.80169535\n",
      "Training Step:  5443 Loss:  0.8016145\n",
      "Training Step:  5444 Loss:  0.80153376\n",
      "Training Step:  5445 Loss:  0.8014534\n",
      "Training Step:  5446 Loss:  0.8013725\n",
      "Training Step:  5447 Loss:  0.8012929\n",
      "Training Step:  5448 Loss:  0.80121285\n",
      "Training Step:  5449 Loss:  0.80113226\n",
      "Training Step:  5450 Loss:  0.80105215\n",
      "Training Step:  5451 Loss:  0.80097187\n",
      "Training Step:  5452 Loss:  0.8008925\n",
      "Training Step:  5453 Loss:  0.80081224\n",
      "Training Step:  5454 Loss:  0.80073225\n",
      "Training Step:  5455 Loss:  0.80065274\n",
      "Training Step:  5456 Loss:  0.80057263\n",
      "Training Step:  5457 Loss:  0.80049324\n",
      "Training Step:  5458 Loss:  0.8004135\n",
      "Training Step:  5459 Loss:  0.8003337\n",
      "Training Step:  5460 Loss:  0.8002532\n",
      "Training Step:  5461 Loss:  0.800174\n",
      "Training Step:  5462 Loss:  0.80009496\n",
      "Training Step:  5463 Loss:  0.80001605\n",
      "Training Step:  5464 Loss:  0.79993683\n",
      "Training Step:  5465 Loss:  0.7998562\n",
      "Training Step:  5466 Loss:  0.79977757\n",
      "Training Step:  5467 Loss:  0.799698\n",
      "Training Step:  5468 Loss:  0.7996193\n",
      "Training Step:  5469 Loss:  0.7995395\n",
      "Training Step:  5470 Loss:  0.7994602\n",
      "Training Step:  5471 Loss:  0.7993821\n",
      "Training Step:  5472 Loss:  0.79930294\n",
      "Training Step:  5473 Loss:  0.7992236\n",
      "Training Step:  5474 Loss:  0.79914504\n",
      "Training Step:  5475 Loss:  0.79906666\n",
      "Training Step:  5476 Loss:  0.79898727\n",
      "Training Step:  5477 Loss:  0.7989085\n",
      "Training Step:  5478 Loss:  0.79883015\n",
      "Training Step:  5479 Loss:  0.79875165\n",
      "Training Step:  5480 Loss:  0.7986727\n",
      "Training Step:  5481 Loss:  0.7985945\n",
      "Training Step:  5482 Loss:  0.79851633\n",
      "Training Step:  5483 Loss:  0.79843706\n",
      "Training Step:  5484 Loss:  0.7983588\n",
      "Training Step:  5485 Loss:  0.7982814\n",
      "Training Step:  5486 Loss:  0.7982018\n",
      "Training Step:  5487 Loss:  0.79812473\n",
      "Training Step:  5488 Loss:  0.79804695\n",
      "Training Step:  5489 Loss:  0.7979681\n",
      "Training Step:  5490 Loss:  0.7978901\n",
      "Training Step:  5491 Loss:  0.79781276\n",
      "Training Step:  5492 Loss:  0.79773366\n",
      "Training Step:  5493 Loss:  0.7976563\n",
      "Training Step:  5494 Loss:  0.79757875\n",
      "Training Step:  5495 Loss:  0.7975007\n",
      "Training Step:  5496 Loss:  0.7974227\n",
      "Training Step:  5497 Loss:  0.79734594\n",
      "Training Step:  5498 Loss:  0.79726756\n",
      "Training Step:  5499 Loss:  0.7971905\n",
      "Training Step:  5500 Loss:  0.7971124\n",
      "Training Step:  5501 Loss:  0.79703516\n",
      "Training Step:  5502 Loss:  0.79695797\n",
      "Training Step:  5503 Loss:  0.7968804\n",
      "Training Step:  5504 Loss:  0.79680276\n",
      "Training Step:  5505 Loss:  0.7967253\n",
      "Training Step:  5506 Loss:  0.7966486\n",
      "Training Step:  5507 Loss:  0.7965716\n",
      "Training Step:  5508 Loss:  0.79649436\n",
      "Training Step:  5509 Loss:  0.79641783\n",
      "Training Step:  5510 Loss:  0.79634035\n",
      "Training Step:  5511 Loss:  0.79626316\n",
      "Training Step:  5512 Loss:  0.7961861\n",
      "Training Step:  5513 Loss:  0.79610926\n",
      "Training Step:  5514 Loss:  0.7960334\n",
      "Training Step:  5515 Loss:  0.7959558\n",
      "Training Step:  5516 Loss:  0.79587954\n",
      "Training Step:  5517 Loss:  0.7958027\n",
      "Training Step:  5518 Loss:  0.7957259\n",
      "Training Step:  5519 Loss:  0.7956493\n",
      "Training Step:  5520 Loss:  0.795573\n",
      "Training Step:  5521 Loss:  0.79549557\n",
      "Training Step:  5522 Loss:  0.7954198\n",
      "Training Step:  5523 Loss:  0.79534316\n",
      "Training Step:  5524 Loss:  0.79526734\n",
      "Training Step:  5525 Loss:  0.7951914\n",
      "Training Step:  5526 Loss:  0.795115\n",
      "Training Step:  5527 Loss:  0.79503864\n",
      "Training Step:  5528 Loss:  0.7949628\n",
      "Training Step:  5529 Loss:  0.7948864\n",
      "Training Step:  5530 Loss:  0.79481053\n",
      "Training Step:  5531 Loss:  0.7947345\n",
      "Training Step:  5532 Loss:  0.79465884\n",
      "Training Step:  5533 Loss:  0.7945831\n",
      "Training Step:  5534 Loss:  0.794507\n",
      "Training Step:  5535 Loss:  0.79443103\n",
      "Training Step:  5536 Loss:  0.7943561\n",
      "Training Step:  5537 Loss:  0.7942797\n",
      "Training Step:  5538 Loss:  0.7942047\n",
      "Training Step:  5539 Loss:  0.7941289\n",
      "Training Step:  5540 Loss:  0.794054\n",
      "Training Step:  5541 Loss:  0.7939783\n",
      "Training Step:  5542 Loss:  0.79390275\n",
      "Training Step:  5543 Loss:  0.79382735\n",
      "Training Step:  5544 Loss:  0.793751\n",
      "Training Step:  5545 Loss:  0.7936764\n",
      "Training Step:  5546 Loss:  0.7936015\n",
      "Training Step:  5547 Loss:  0.79352677\n",
      "Training Step:  5548 Loss:  0.79345125\n",
      "Training Step:  5549 Loss:  0.7933766\n",
      "Training Step:  5550 Loss:  0.7933013\n",
      "Training Step:  5551 Loss:  0.7932255\n",
      "Training Step:  5552 Loss:  0.7931514\n",
      "Training Step:  5553 Loss:  0.7930762\n",
      "Training Step:  5554 Loss:  0.793002\n",
      "Training Step:  5555 Loss:  0.7929269\n",
      "Training Step:  5556 Loss:  0.79285216\n",
      "Training Step:  5557 Loss:  0.79277784\n",
      "Training Step:  5558 Loss:  0.7927032\n",
      "Training Step:  5559 Loss:  0.7926278\n",
      "Training Step:  5560 Loss:  0.79255366\n",
      "Training Step:  5561 Loss:  0.7924791\n",
      "Training Step:  5562 Loss:  0.7924059\n",
      "Training Step:  5563 Loss:  0.7923309\n",
      "Training Step:  5564 Loss:  0.7922559\n",
      "Training Step:  5565 Loss:  0.79218256\n",
      "Training Step:  5566 Loss:  0.7921079\n",
      "Training Step:  5567 Loss:  0.79203314\n",
      "Training Step:  5568 Loss:  0.79195964\n",
      "Training Step:  5569 Loss:  0.79188526\n",
      "Training Step:  5570 Loss:  0.7918112\n",
      "Training Step:  5571 Loss:  0.7917379\n",
      "Training Step:  5572 Loss:  0.7916638\n",
      "Training Step:  5573 Loss:  0.79158926\n",
      "Training Step:  5574 Loss:  0.79151565\n",
      "Training Step:  5575 Loss:  0.7914418\n",
      "Training Step:  5576 Loss:  0.7913674\n",
      "Training Step:  5577 Loss:  0.79129535\n",
      "Training Step:  5578 Loss:  0.7912203\n",
      "Training Step:  5579 Loss:  0.7911473\n",
      "Training Step:  5580 Loss:  0.791073\n",
      "Training Step:  5581 Loss:  0.79100037\n",
      "Training Step:  5582 Loss:  0.79092735\n",
      "Training Step:  5583 Loss:  0.7908532\n",
      "Training Step:  5584 Loss:  0.7907805\n",
      "Training Step:  5585 Loss:  0.79070616\n",
      "Training Step:  5586 Loss:  0.7906338\n",
      "Training Step:  5587 Loss:  0.7905595\n",
      "Training Step:  5588 Loss:  0.7904876\n",
      "Training Step:  5589 Loss:  0.7904147\n",
      "Training Step:  5590 Loss:  0.79034096\n",
      "Training Step:  5591 Loss:  0.79026765\n",
      "Training Step:  5592 Loss:  0.790195\n",
      "Training Step:  5593 Loss:  0.7901218\n",
      "Training Step:  5594 Loss:  0.79004884\n",
      "Training Step:  5595 Loss:  0.78997666\n",
      "Training Step:  5596 Loss:  0.7899037\n",
      "Training Step:  5597 Loss:  0.7898306\n",
      "Training Step:  5598 Loss:  0.7897582\n",
      "Training Step:  5599 Loss:  0.78968585\n",
      "Training Step:  5600 Loss:  0.7896127\n",
      "Training Step:  5601 Loss:  0.7895403\n",
      "Training Step:  5602 Loss:  0.789468\n",
      "Training Step:  5603 Loss:  0.7893961\n",
      "Training Step:  5604 Loss:  0.78932333\n",
      "Training Step:  5605 Loss:  0.78925014\n",
      "Training Step:  5606 Loss:  0.78917867\n",
      "Training Step:  5607 Loss:  0.78910625\n",
      "Training Step:  5608 Loss:  0.78903407\n",
      "Training Step:  5609 Loss:  0.7889615\n",
      "Training Step:  5610 Loss:  0.78888935\n",
      "Training Step:  5611 Loss:  0.788817\n",
      "Training Step:  5612 Loss:  0.78874576\n",
      "Training Step:  5613 Loss:  0.788674\n",
      "Training Step:  5614 Loss:  0.7886016\n",
      "Training Step:  5615 Loss:  0.78852993\n",
      "Training Step:  5616 Loss:  0.7884572\n",
      "Training Step:  5617 Loss:  0.7883854\n",
      "Training Step:  5618 Loss:  0.7883146\n",
      "Training Step:  5619 Loss:  0.78824294\n",
      "Training Step:  5620 Loss:  0.78817105\n",
      "Training Step:  5621 Loss:  0.7880997\n",
      "Training Step:  5622 Loss:  0.78802794\n",
      "Training Step:  5623 Loss:  0.7879562\n",
      "Training Step:  5624 Loss:  0.787885\n",
      "Training Step:  5625 Loss:  0.787813\n",
      "Training Step:  5626 Loss:  0.7877424\n",
      "Training Step:  5627 Loss:  0.7876712\n",
      "Training Step:  5628 Loss:  0.7875999\n",
      "Training Step:  5629 Loss:  0.7875281\n",
      "Training Step:  5630 Loss:  0.787457\n",
      "Training Step:  5631 Loss:  0.7873861\n",
      "Training Step:  5632 Loss:  0.78731436\n",
      "Training Step:  5633 Loss:  0.78724366\n",
      "Training Step:  5634 Loss:  0.7871722\n",
      "Training Step:  5635 Loss:  0.7871012\n",
      "Training Step:  5636 Loss:  0.78703076\n",
      "Training Step:  5637 Loss:  0.7869602\n",
      "Training Step:  5638 Loss:  0.7868897\n",
      "Training Step:  5639 Loss:  0.7868189\n",
      "Training Step:  5640 Loss:  0.7867479\n",
      "Training Step:  5641 Loss:  0.7866773\n",
      "Training Step:  5642 Loss:  0.786606\n",
      "Training Step:  5643 Loss:  0.78653514\n",
      "Training Step:  5644 Loss:  0.7864649\n",
      "Training Step:  5645 Loss:  0.7863942\n",
      "Training Step:  5646 Loss:  0.7863244\n",
      "Training Step:  5647 Loss:  0.78625345\n",
      "Training Step:  5648 Loss:  0.78618354\n",
      "Training Step:  5649 Loss:  0.78611314\n",
      "Training Step:  5650 Loss:  0.7860429\n",
      "Training Step:  5651 Loss:  0.78597265\n",
      "Training Step:  5652 Loss:  0.7859025\n",
      "Training Step:  5653 Loss:  0.7858326\n",
      "Training Step:  5654 Loss:  0.78576213\n",
      "Training Step:  5655 Loss:  0.785692\n",
      "Training Step:  5656 Loss:  0.78562176\n",
      "Training Step:  5657 Loss:  0.78555167\n",
      "Training Step:  5658 Loss:  0.78548276\n",
      "Training Step:  5659 Loss:  0.78541243\n",
      "Training Step:  5660 Loss:  0.78534245\n",
      "Training Step:  5661 Loss:  0.7852723\n",
      "Training Step:  5662 Loss:  0.7852033\n",
      "Training Step:  5663 Loss:  0.78513336\n",
      "Training Step:  5664 Loss:  0.7850638\n",
      "Training Step:  5665 Loss:  0.7849942\n",
      "Training Step:  5666 Loss:  0.7849248\n",
      "Training Step:  5667 Loss:  0.7848555\n",
      "Training Step:  5668 Loss:  0.78478646\n",
      "Training Step:  5669 Loss:  0.784717\n",
      "Training Step:  5670 Loss:  0.7846473\n",
      "Training Step:  5671 Loss:  0.7845763\n",
      "Training Step:  5672 Loss:  0.7845075\n",
      "Training Step:  5673 Loss:  0.7844385\n",
      "Training Step:  5674 Loss:  0.78437\n",
      "Training Step:  5675 Loss:  0.78430027\n",
      "Training Step:  5676 Loss:  0.7842316\n",
      "Training Step:  5677 Loss:  0.7841626\n",
      "Training Step:  5678 Loss:  0.78409314\n",
      "Training Step:  5679 Loss:  0.7840244\n",
      "Training Step:  5680 Loss:  0.78395575\n",
      "Training Step:  5681 Loss:  0.7838869\n",
      "Training Step:  5682 Loss:  0.78381723\n",
      "Training Step:  5683 Loss:  0.783749\n",
      "Training Step:  5684 Loss:  0.7836805\n",
      "Training Step:  5685 Loss:  0.78361166\n",
      "Training Step:  5686 Loss:  0.783543\n",
      "Training Step:  5687 Loss:  0.7834744\n",
      "Training Step:  5688 Loss:  0.7834058\n",
      "Training Step:  5689 Loss:  0.78333753\n",
      "Training Step:  5690 Loss:  0.7832688\n",
      "Training Step:  5691 Loss:  0.7831997\n",
      "Training Step:  5692 Loss:  0.7831316\n",
      "Training Step:  5693 Loss:  0.78306335\n",
      "Training Step:  5694 Loss:  0.78299487\n",
      "Training Step:  5695 Loss:  0.78292704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  5696 Loss:  0.7828591\n",
      "Training Step:  5697 Loss:  0.7827895\n",
      "Training Step:  5698 Loss:  0.7827224\n",
      "Training Step:  5699 Loss:  0.782655\n",
      "Training Step:  5700 Loss:  0.7825861\n",
      "Training Step:  5701 Loss:  0.7825185\n",
      "Training Step:  5702 Loss:  0.7824504\n",
      "Training Step:  5703 Loss:  0.7823817\n",
      "Training Step:  5704 Loss:  0.78231406\n",
      "Training Step:  5705 Loss:  0.78224653\n",
      "Training Step:  5706 Loss:  0.78217864\n",
      "Training Step:  5707 Loss:  0.78211117\n",
      "Training Step:  5708 Loss:  0.7820432\n",
      "Training Step:  5709 Loss:  0.7819755\n",
      "Training Step:  5710 Loss:  0.7819086\n",
      "Training Step:  5711 Loss:  0.7818409\n",
      "Training Step:  5712 Loss:  0.7817727\n",
      "Training Step:  5713 Loss:  0.78170556\n",
      "Training Step:  5714 Loss:  0.78163815\n",
      "Training Step:  5715 Loss:  0.7815705\n",
      "Training Step:  5716 Loss:  0.78150415\n",
      "Training Step:  5717 Loss:  0.7814358\n",
      "Training Step:  5718 Loss:  0.78136873\n",
      "Training Step:  5719 Loss:  0.78130174\n",
      "Training Step:  5720 Loss:  0.7812349\n",
      "Training Step:  5721 Loss:  0.78116673\n",
      "Training Step:  5722 Loss:  0.78110087\n",
      "Training Step:  5723 Loss:  0.7810336\n",
      "Training Step:  5724 Loss:  0.7809671\n",
      "Training Step:  5725 Loss:  0.78089875\n",
      "Training Step:  5726 Loss:  0.78083247\n",
      "Training Step:  5727 Loss:  0.7807659\n",
      "Training Step:  5728 Loss:  0.78069836\n",
      "Training Step:  5729 Loss:  0.78063214\n",
      "Training Step:  5730 Loss:  0.78056556\n",
      "Training Step:  5731 Loss:  0.78049886\n",
      "Training Step:  5732 Loss:  0.78043133\n",
      "Training Step:  5733 Loss:  0.7803648\n",
      "Training Step:  5734 Loss:  0.78029895\n",
      "Training Step:  5735 Loss:  0.7802318\n",
      "Training Step:  5736 Loss:  0.7801659\n",
      "Training Step:  5737 Loss:  0.7800999\n",
      "Training Step:  5738 Loss:  0.78003263\n",
      "Training Step:  5739 Loss:  0.7799663\n",
      "Training Step:  5740 Loss:  0.7798999\n",
      "Training Step:  5741 Loss:  0.77983314\n",
      "Training Step:  5742 Loss:  0.77976745\n",
      "Training Step:  5743 Loss:  0.77970123\n",
      "Training Step:  5744 Loss:  0.77963436\n",
      "Training Step:  5745 Loss:  0.7795687\n",
      "Training Step:  5746 Loss:  0.77950394\n",
      "Training Step:  5747 Loss:  0.7794363\n",
      "Training Step:  5748 Loss:  0.7793708\n",
      "Training Step:  5749 Loss:  0.7793052\n",
      "Training Step:  5750 Loss:  0.77923924\n",
      "Training Step:  5751 Loss:  0.77917254\n",
      "Training Step:  5752 Loss:  0.7791077\n",
      "Training Step:  5753 Loss:  0.7790416\n",
      "Training Step:  5754 Loss:  0.778976\n",
      "Training Step:  5755 Loss:  0.77890956\n",
      "Training Step:  5756 Loss:  0.77884465\n",
      "Training Step:  5757 Loss:  0.7787788\n",
      "Training Step:  5758 Loss:  0.7787124\n",
      "Training Step:  5759 Loss:  0.7786479\n",
      "Training Step:  5760 Loss:  0.778582\n",
      "Training Step:  5761 Loss:  0.77851653\n",
      "Training Step:  5762 Loss:  0.7784515\n",
      "Training Step:  5763 Loss:  0.778386\n",
      "Training Step:  5764 Loss:  0.77832013\n",
      "Training Step:  5765 Loss:  0.77825487\n",
      "Training Step:  5766 Loss:  0.7781906\n",
      "Training Step:  5767 Loss:  0.77812546\n",
      "Training Step:  5768 Loss:  0.7780596\n",
      "Training Step:  5769 Loss:  0.7779942\n",
      "Training Step:  5770 Loss:  0.77792966\n",
      "Training Step:  5771 Loss:  0.7778647\n",
      "Training Step:  5772 Loss:  0.77779883\n",
      "Training Step:  5773 Loss:  0.7777344\n",
      "Training Step:  5774 Loss:  0.7776697\n",
      "Training Step:  5775 Loss:  0.7776045\n",
      "Training Step:  5776 Loss:  0.7775402\n",
      "Training Step:  5777 Loss:  0.7774752\n",
      "Training Step:  5778 Loss:  0.77741104\n",
      "Training Step:  5779 Loss:  0.77734506\n",
      "Training Step:  5780 Loss:  0.7772817\n",
      "Training Step:  5781 Loss:  0.7772162\n",
      "Training Step:  5782 Loss:  0.7771519\n",
      "Training Step:  5783 Loss:  0.7770873\n",
      "Training Step:  5784 Loss:  0.77702355\n",
      "Training Step:  5785 Loss:  0.7769587\n",
      "Training Step:  5786 Loss:  0.776894\n",
      "Training Step:  5787 Loss:  0.77683\n",
      "Training Step:  5788 Loss:  0.77676535\n",
      "Training Step:  5789 Loss:  0.7767008\n",
      "Training Step:  5790 Loss:  0.7766367\n",
      "Training Step:  5791 Loss:  0.77657235\n",
      "Training Step:  5792 Loss:  0.7765081\n",
      "Training Step:  5793 Loss:  0.77644384\n",
      "Training Step:  5794 Loss:  0.7763799\n",
      "Training Step:  5795 Loss:  0.7763161\n",
      "Training Step:  5796 Loss:  0.7762523\n",
      "Training Step:  5797 Loss:  0.77618736\n",
      "Training Step:  5798 Loss:  0.7761243\n",
      "Training Step:  5799 Loss:  0.77606\n",
      "Training Step:  5800 Loss:  0.77599585\n",
      "Training Step:  5801 Loss:  0.77593213\n",
      "Training Step:  5802 Loss:  0.77586824\n",
      "Training Step:  5803 Loss:  0.7758055\n",
      "Training Step:  5804 Loss:  0.7757406\n",
      "Training Step:  5805 Loss:  0.7756779\n",
      "Training Step:  5806 Loss:  0.7756141\n",
      "Training Step:  5807 Loss:  0.77554977\n",
      "Training Step:  5808 Loss:  0.77548665\n",
      "Training Step:  5809 Loss:  0.77542245\n",
      "Training Step:  5810 Loss:  0.7753598\n",
      "Training Step:  5811 Loss:  0.77529675\n",
      "Training Step:  5812 Loss:  0.7752334\n",
      "Training Step:  5813 Loss:  0.77516925\n",
      "Training Step:  5814 Loss:  0.7751065\n",
      "Training Step:  5815 Loss:  0.775043\n",
      "Training Step:  5816 Loss:  0.7749796\n",
      "Training Step:  5817 Loss:  0.77491707\n",
      "Training Step:  5818 Loss:  0.77485305\n",
      "Training Step:  5819 Loss:  0.77478987\n",
      "Training Step:  5820 Loss:  0.77472806\n",
      "Training Step:  5821 Loss:  0.7746649\n",
      "Training Step:  5822 Loss:  0.77460086\n",
      "Training Step:  5823 Loss:  0.77453905\n",
      "Training Step:  5824 Loss:  0.7744758\n",
      "Training Step:  5825 Loss:  0.7744128\n",
      "Training Step:  5826 Loss:  0.7743498\n",
      "Training Step:  5827 Loss:  0.7742866\n",
      "Training Step:  5828 Loss:  0.77422506\n",
      "Training Step:  5829 Loss:  0.7741615\n",
      "Training Step:  5830 Loss:  0.77409816\n",
      "Training Step:  5831 Loss:  0.77403617\n",
      "Training Step:  5832 Loss:  0.77397364\n",
      "Training Step:  5833 Loss:  0.77391076\n",
      "Training Step:  5834 Loss:  0.773849\n",
      "Training Step:  5835 Loss:  0.77378577\n",
      "Training Step:  5836 Loss:  0.7737237\n",
      "Training Step:  5837 Loss:  0.77366173\n",
      "Training Step:  5838 Loss:  0.7735993\n",
      "Training Step:  5839 Loss:  0.77353686\n",
      "Training Step:  5840 Loss:  0.77347463\n",
      "Training Step:  5841 Loss:  0.77341264\n",
      "Training Step:  5842 Loss:  0.77335\n",
      "Training Step:  5843 Loss:  0.7732878\n",
      "Training Step:  5844 Loss:  0.7732262\n",
      "Training Step:  5845 Loss:  0.77316344\n",
      "Training Step:  5846 Loss:  0.773102\n",
      "Training Step:  5847 Loss:  0.77304\n",
      "Training Step:  5848 Loss:  0.77297807\n",
      "Training Step:  5849 Loss:  0.7729154\n",
      "Training Step:  5850 Loss:  0.77285296\n",
      "Training Step:  5851 Loss:  0.7727924\n",
      "Training Step:  5852 Loss:  0.7727302\n",
      "Training Step:  5853 Loss:  0.7726686\n",
      "Training Step:  5854 Loss:  0.772606\n",
      "Training Step:  5855 Loss:  0.7725454\n",
      "Training Step:  5856 Loss:  0.7724835\n",
      "Training Step:  5857 Loss:  0.77242166\n",
      "Training Step:  5858 Loss:  0.77235985\n",
      "Training Step:  5859 Loss:  0.7722992\n",
      "Training Step:  5860 Loss:  0.7722369\n",
      "Training Step:  5861 Loss:  0.7721755\n",
      "Training Step:  5862 Loss:  0.7721139\n",
      "Training Step:  5863 Loss:  0.772053\n",
      "Training Step:  5864 Loss:  0.7719915\n",
      "Training Step:  5865 Loss:  0.77193034\n",
      "Training Step:  5866 Loss:  0.77186847\n",
      "Training Step:  5867 Loss:  0.7718076\n",
      "Training Step:  5868 Loss:  0.77174664\n",
      "Training Step:  5869 Loss:  0.7716854\n",
      "Training Step:  5870 Loss:  0.77162427\n",
      "Training Step:  5871 Loss:  0.7715629\n",
      "Training Step:  5872 Loss:  0.7715024\n",
      "Training Step:  5873 Loss:  0.7714416\n",
      "Training Step:  5874 Loss:  0.7713798\n",
      "Training Step:  5875 Loss:  0.77131927\n",
      "Training Step:  5876 Loss:  0.77125823\n",
      "Training Step:  5877 Loss:  0.77119696\n",
      "Training Step:  5878 Loss:  0.77113736\n",
      "Training Step:  5879 Loss:  0.7710762\n",
      "Training Step:  5880 Loss:  0.771015\n",
      "Training Step:  5881 Loss:  0.77095455\n",
      "Training Step:  5882 Loss:  0.7708932\n",
      "Training Step:  5883 Loss:  0.77083296\n",
      "Training Step:  5884 Loss:  0.7707729\n",
      "Training Step:  5885 Loss:  0.7707123\n",
      "Training Step:  5886 Loss:  0.77065146\n",
      "Training Step:  5887 Loss:  0.77059114\n",
      "Training Step:  5888 Loss:  0.77053034\n",
      "Training Step:  5889 Loss:  0.7704701\n",
      "Training Step:  5890 Loss:  0.77040905\n",
      "Training Step:  5891 Loss:  0.7703501\n",
      "Training Step:  5892 Loss:  0.7702894\n",
      "Training Step:  5893 Loss:  0.77022856\n",
      "Training Step:  5894 Loss:  0.7701687\n",
      "Training Step:  5895 Loss:  0.77010864\n",
      "Training Step:  5896 Loss:  0.7700483\n",
      "Training Step:  5897 Loss:  0.7699891\n",
      "Training Step:  5898 Loss:  0.76992756\n",
      "Training Step:  5899 Loss:  0.7698673\n",
      "Training Step:  5900 Loss:  0.7698087\n",
      "Training Step:  5901 Loss:  0.76974857\n",
      "Training Step:  5902 Loss:  0.76968855\n",
      "Training Step:  5903 Loss:  0.7696284\n",
      "Training Step:  5904 Loss:  0.7695689\n",
      "Training Step:  5905 Loss:  0.76950884\n",
      "Training Step:  5906 Loss:  0.7694493\n",
      "Training Step:  5907 Loss:  0.76938957\n",
      "Training Step:  5908 Loss:  0.7693292\n",
      "Training Step:  5909 Loss:  0.7692694\n",
      "Training Step:  5910 Loss:  0.7692102\n",
      "Training Step:  5911 Loss:  0.76915044\n",
      "Training Step:  5912 Loss:  0.76909083\n",
      "Training Step:  5913 Loss:  0.76903105\n",
      "Training Step:  5914 Loss:  0.7689721\n",
      "Training Step:  5915 Loss:  0.7689134\n",
      "Training Step:  5916 Loss:  0.768854\n",
      "Training Step:  5917 Loss:  0.76879376\n",
      "Training Step:  5918 Loss:  0.7687347\n",
      "Training Step:  5919 Loss:  0.76867574\n",
      "Training Step:  5920 Loss:  0.7686164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  5921 Loss:  0.7685571\n",
      "Training Step:  5922 Loss:  0.7684976\n",
      "Training Step:  5923 Loss:  0.76843864\n",
      "Training Step:  5924 Loss:  0.7683794\n",
      "Training Step:  5925 Loss:  0.76832\n",
      "Training Step:  5926 Loss:  0.7682613\n",
      "Training Step:  5927 Loss:  0.7682024\n",
      "Training Step:  5928 Loss:  0.7681433\n",
      "Training Step:  5929 Loss:  0.76808417\n",
      "Training Step:  5930 Loss:  0.76802534\n",
      "Training Step:  5931 Loss:  0.7679671\n",
      "Training Step:  5932 Loss:  0.767908\n",
      "Training Step:  5933 Loss:  0.7678493\n",
      "Training Step:  5934 Loss:  0.76779044\n",
      "Training Step:  5935 Loss:  0.7677323\n",
      "Training Step:  5936 Loss:  0.76767325\n",
      "Training Step:  5937 Loss:  0.7676146\n",
      "Training Step:  5938 Loss:  0.76755595\n",
      "Training Step:  5939 Loss:  0.76749724\n",
      "Training Step:  5940 Loss:  0.767439\n",
      "Training Step:  5941 Loss:  0.7673799\n",
      "Training Step:  5942 Loss:  0.7673214\n",
      "Training Step:  5943 Loss:  0.76726204\n",
      "Training Step:  5944 Loss:  0.7672044\n",
      "Training Step:  5945 Loss:  0.76714647\n",
      "Training Step:  5946 Loss:  0.76708806\n",
      "Training Step:  5947 Loss:  0.7670299\n",
      "Training Step:  5948 Loss:  0.7669715\n",
      "Training Step:  5949 Loss:  0.7669136\n",
      "Training Step:  5950 Loss:  0.76685536\n",
      "Training Step:  5951 Loss:  0.76679754\n",
      "Training Step:  5952 Loss:  0.76673865\n",
      "Training Step:  5953 Loss:  0.76668066\n",
      "Training Step:  5954 Loss:  0.7666234\n",
      "Training Step:  5955 Loss:  0.76656467\n",
      "Training Step:  5956 Loss:  0.7665062\n",
      "Training Step:  5957 Loss:  0.766449\n",
      "Training Step:  5958 Loss:  0.7663913\n",
      "Training Step:  5959 Loss:  0.76633346\n",
      "Training Step:  5960 Loss:  0.76627547\n",
      "Training Step:  5961 Loss:  0.76621795\n",
      "Training Step:  5962 Loss:  0.7661602\n",
      "Training Step:  5963 Loss:  0.7661025\n",
      "Training Step:  5964 Loss:  0.7660444\n",
      "Training Step:  5965 Loss:  0.7659868\n",
      "Training Step:  5966 Loss:  0.76592886\n",
      "Training Step:  5967 Loss:  0.76587206\n",
      "Training Step:  5968 Loss:  0.76581407\n",
      "Training Step:  5969 Loss:  0.7657569\n",
      "Training Step:  5970 Loss:  0.7656995\n",
      "Training Step:  5971 Loss:  0.76564205\n",
      "Training Step:  5972 Loss:  0.7655841\n",
      "Training Step:  5973 Loss:  0.76552725\n",
      "Training Step:  5974 Loss:  0.76546955\n",
      "Training Step:  5975 Loss:  0.7654123\n",
      "Training Step:  5976 Loss:  0.76535547\n",
      "Training Step:  5977 Loss:  0.7652982\n",
      "Training Step:  5978 Loss:  0.7652402\n",
      "Training Step:  5979 Loss:  0.76518357\n",
      "Training Step:  5980 Loss:  0.7651266\n",
      "Training Step:  5981 Loss:  0.76506925\n",
      "Training Step:  5982 Loss:  0.7650125\n",
      "Training Step:  5983 Loss:  0.7649553\n",
      "Training Step:  5984 Loss:  0.7648978\n",
      "Training Step:  5985 Loss:  0.76484174\n",
      "Training Step:  5986 Loss:  0.7647843\n",
      "Training Step:  5987 Loss:  0.76472837\n",
      "Training Step:  5988 Loss:  0.764671\n",
      "Training Step:  5989 Loss:  0.7646134\n",
      "Training Step:  5990 Loss:  0.7645567\n",
      "Training Step:  5991 Loss:  0.7645003\n",
      "Training Step:  5992 Loss:  0.7644438\n",
      "Training Step:  5993 Loss:  0.764387\n",
      "Training Step:  5994 Loss:  0.76432985\n",
      "Training Step:  5995 Loss:  0.76427424\n",
      "Training Step:  5996 Loss:  0.76421726\n",
      "Training Step:  5997 Loss:  0.7641606\n",
      "Training Step:  5998 Loss:  0.76410365\n",
      "Training Step:  5999 Loss:  0.76404804\n",
      "Training Step:  6000 Loss:  0.7639912\n",
      "Training Step:  6001 Loss:  0.76393545\n",
      "Training Step:  6002 Loss:  0.763878\n",
      "Training Step:  6003 Loss:  0.76382256\n",
      "Training Step:  6004 Loss:  0.7637659\n",
      "Training Step:  6005 Loss:  0.7637093\n",
      "Training Step:  6006 Loss:  0.76365364\n",
      "Training Step:  6007 Loss:  0.7635972\n",
      "Training Step:  6008 Loss:  0.76354116\n",
      "Training Step:  6009 Loss:  0.7634843\n",
      "Training Step:  6010 Loss:  0.763429\n",
      "Training Step:  6011 Loss:  0.7633723\n",
      "Training Step:  6012 Loss:  0.763317\n",
      "Training Step:  6013 Loss:  0.76326036\n",
      "Training Step:  6014 Loss:  0.76320463\n",
      "Training Step:  6015 Loss:  0.7631492\n",
      "Training Step:  6016 Loss:  0.763093\n",
      "Training Step:  6017 Loss:  0.76303697\n",
      "Training Step:  6018 Loss:  0.7629814\n",
      "Training Step:  6019 Loss:  0.762925\n",
      "Training Step:  6020 Loss:  0.76286936\n",
      "Training Step:  6021 Loss:  0.76281446\n",
      "Training Step:  6022 Loss:  0.7627591\n",
      "Training Step:  6023 Loss:  0.76270276\n",
      "Training Step:  6024 Loss:  0.7626472\n",
      "Training Step:  6025 Loss:  0.7625926\n",
      "Training Step:  6026 Loss:  0.7625362\n",
      "Training Step:  6027 Loss:  0.7624809\n",
      "Training Step:  6028 Loss:  0.7624251\n",
      "Training Step:  6029 Loss:  0.76236963\n",
      "Training Step:  6030 Loss:  0.7623148\n",
      "Training Step:  6031 Loss:  0.7622585\n",
      "Training Step:  6032 Loss:  0.76220274\n",
      "Training Step:  6033 Loss:  0.76214814\n",
      "Training Step:  6034 Loss:  0.7620928\n",
      "Training Step:  6035 Loss:  0.762038\n",
      "Training Step:  6036 Loss:  0.76198244\n",
      "Training Step:  6037 Loss:  0.7619271\n",
      "Training Step:  6038 Loss:  0.7618721\n",
      "Training Step:  6039 Loss:  0.7618174\n",
      "Training Step:  6040 Loss:  0.7617618\n",
      "Training Step:  6041 Loss:  0.76170695\n",
      "Training Step:  6042 Loss:  0.7616521\n",
      "Training Step:  6043 Loss:  0.7615966\n",
      "Training Step:  6044 Loss:  0.76154256\n",
      "Training Step:  6045 Loss:  0.7614866\n",
      "Training Step:  6046 Loss:  0.76143163\n",
      "Training Step:  6047 Loss:  0.7613764\n",
      "Training Step:  6048 Loss:  0.76132226\n",
      "Training Step:  6049 Loss:  0.7612674\n",
      "Training Step:  6050 Loss:  0.7612133\n",
      "Training Step:  6051 Loss:  0.7611584\n",
      "Training Step:  6052 Loss:  0.7611033\n",
      "Training Step:  6053 Loss:  0.7610487\n",
      "Training Step:  6054 Loss:  0.7609948\n",
      "Training Step:  6055 Loss:  0.7609396\n",
      "Training Step:  6056 Loss:  0.76088536\n",
      "Training Step:  6057 Loss:  0.76083016\n",
      "Training Step:  6058 Loss:  0.76077646\n",
      "Training Step:  6059 Loss:  0.7607205\n",
      "Training Step:  6060 Loss:  0.76066667\n",
      "Training Step:  6061 Loss:  0.7606135\n",
      "Training Step:  6062 Loss:  0.7605587\n",
      "Training Step:  6063 Loss:  0.76050454\n",
      "Training Step:  6064 Loss:  0.7604499\n",
      "Training Step:  6065 Loss:  0.76039594\n",
      "Training Step:  6066 Loss:  0.7603415\n",
      "Training Step:  6067 Loss:  0.760287\n",
      "Training Step:  6068 Loss:  0.76023245\n",
      "Training Step:  6069 Loss:  0.76017904\n",
      "Training Step:  6070 Loss:  0.76012486\n",
      "Training Step:  6071 Loss:  0.76007074\n",
      "Training Step:  6072 Loss:  0.7600162\n",
      "Training Step:  6073 Loss:  0.7599623\n",
      "Training Step:  6074 Loss:  0.75990826\n",
      "Training Step:  6075 Loss:  0.7598547\n",
      "Training Step:  6076 Loss:  0.75980055\n",
      "Training Step:  6077 Loss:  0.7597469\n",
      "Training Step:  6078 Loss:  0.75969225\n",
      "Training Step:  6079 Loss:  0.75963914\n",
      "Training Step:  6080 Loss:  0.75958496\n",
      "Training Step:  6081 Loss:  0.7595317\n",
      "Training Step:  6082 Loss:  0.75947773\n",
      "Training Step:  6083 Loss:  0.7594243\n",
      "Training Step:  6084 Loss:  0.7593706\n",
      "Training Step:  6085 Loss:  0.75931716\n",
      "Training Step:  6086 Loss:  0.75926363\n",
      "Training Step:  6087 Loss:  0.75920963\n",
      "Training Step:  6088 Loss:  0.75915647\n",
      "Training Step:  6089 Loss:  0.75910306\n",
      "Training Step:  6090 Loss:  0.7590493\n",
      "Training Step:  6091 Loss:  0.7589964\n",
      "Training Step:  6092 Loss:  0.75894284\n",
      "Training Step:  6093 Loss:  0.7588893\n",
      "Training Step:  6094 Loss:  0.7588359\n",
      "Training Step:  6095 Loss:  0.75878227\n",
      "Training Step:  6096 Loss:  0.75872946\n",
      "Training Step:  6097 Loss:  0.758676\n",
      "Training Step:  6098 Loss:  0.7586235\n",
      "Training Step:  6099 Loss:  0.75857013\n",
      "Training Step:  6100 Loss:  0.7585171\n",
      "Training Step:  6101 Loss:  0.7584632\n",
      "Training Step:  6102 Loss:  0.75841004\n",
      "Training Step:  6103 Loss:  0.75835764\n",
      "Training Step:  6104 Loss:  0.7583042\n",
      "Training Step:  6105 Loss:  0.7582517\n",
      "Training Step:  6106 Loss:  0.75819796\n",
      "Training Step:  6107 Loss:  0.7581458\n",
      "Training Step:  6108 Loss:  0.7580925\n",
      "Training Step:  6109 Loss:  0.7580399\n",
      "Training Step:  6110 Loss:  0.7579868\n",
      "Training Step:  6111 Loss:  0.75793445\n",
      "Training Step:  6112 Loss:  0.7578806\n",
      "Training Step:  6113 Loss:  0.7578286\n",
      "Training Step:  6114 Loss:  0.75777555\n",
      "Training Step:  6115 Loss:  0.7577239\n",
      "Training Step:  6116 Loss:  0.757671\n",
      "Training Step:  6117 Loss:  0.75761735\n",
      "Training Step:  6118 Loss:  0.75756574\n",
      "Training Step:  6119 Loss:  0.7575126\n",
      "Training Step:  6120 Loss:  0.7574603\n",
      "Training Step:  6121 Loss:  0.7574079\n",
      "Training Step:  6122 Loss:  0.7573555\n",
      "Training Step:  6123 Loss:  0.75730264\n",
      "Training Step:  6124 Loss:  0.7572502\n",
      "Training Step:  6125 Loss:  0.7571983\n",
      "Training Step:  6126 Loss:  0.7571467\n",
      "Training Step:  6127 Loss:  0.7570933\n",
      "Training Step:  6128 Loss:  0.7570416\n",
      "Training Step:  6129 Loss:  0.75698984\n",
      "Training Step:  6130 Loss:  0.7569372\n",
      "Training Step:  6131 Loss:  0.7568848\n",
      "Training Step:  6132 Loss:  0.75683284\n",
      "Training Step:  6133 Loss:  0.7567805\n",
      "Training Step:  6134 Loss:  0.756728\n",
      "Training Step:  6135 Loss:  0.75667644\n",
      "Training Step:  6136 Loss:  0.75662416\n",
      "Training Step:  6137 Loss:  0.756572\n",
      "Training Step:  6138 Loss:  0.7565195\n",
      "Training Step:  6139 Loss:  0.75646883\n",
      "Training Step:  6140 Loss:  0.7564167\n",
      "Training Step:  6141 Loss:  0.7563645\n",
      "Training Step:  6142 Loss:  0.75631285\n",
      "Training Step:  6143 Loss:  0.75626045\n",
      "Training Step:  6144 Loss:  0.7562096\n",
      "Training Step:  6145 Loss:  0.75615716\n",
      "Training Step:  6146 Loss:  0.75610596\n",
      "Training Step:  6147 Loss:  0.7560543\n",
      "Training Step:  6148 Loss:  0.756002\n",
      "Training Step:  6149 Loss:  0.75595117\n",
      "Training Step:  6150 Loss:  0.7558989\n",
      "Training Step:  6151 Loss:  0.755848\n",
      "Training Step:  6152 Loss:  0.75579566\n",
      "Training Step:  6153 Loss:  0.75574493\n",
      "Training Step:  6154 Loss:  0.7556932\n",
      "Training Step:  6155 Loss:  0.755642\n",
      "Training Step:  6156 Loss:  0.7555901\n",
      "Training Step:  6157 Loss:  0.755539\n",
      "Training Step:  6158 Loss:  0.7554878\n",
      "Training Step:  6159 Loss:  0.75543624\n",
      "Training Step:  6160 Loss:  0.75538486\n",
      "Training Step:  6161 Loss:  0.7553341\n",
      "Training Step:  6162 Loss:  0.75528234\n",
      "Training Step:  6163 Loss:  0.755231\n",
      "Training Step:  6164 Loss:  0.75517976\n",
      "Training Step:  6165 Loss:  0.7551286\n",
      "Training Step:  6166 Loss:  0.75507766\n",
      "Training Step:  6167 Loss:  0.7550261\n",
      "Training Step:  6168 Loss:  0.75497556\n",
      "Training Step:  6169 Loss:  0.7549241\n",
      "Training Step:  6170 Loss:  0.7548741\n",
      "Training Step:  6171 Loss:  0.7548231\n",
      "Training Step:  6172 Loss:  0.75477135\n",
      "Training Step:  6173 Loss:  0.7547203\n",
      "Training Step:  6174 Loss:  0.7546696\n",
      "Training Step:  6175 Loss:  0.75461996\n",
      "Training Step:  6176 Loss:  0.75456834\n",
      "Training Step:  6177 Loss:  0.75451744\n",
      "Training Step:  6178 Loss:  0.7544669\n",
      "Training Step:  6179 Loss:  0.7544159\n",
      "Training Step:  6180 Loss:  0.7543649\n",
      "Training Step:  6181 Loss:  0.75431484\n",
      "Training Step:  6182 Loss:  0.7542641\n",
      "Training Step:  6183 Loss:  0.7542132\n",
      "Training Step:  6184 Loss:  0.7541628\n",
      "Training Step:  6185 Loss:  0.7541125\n",
      "Training Step:  6186 Loss:  0.754061\n",
      "Training Step:  6187 Loss:  0.75401163\n",
      "Training Step:  6188 Loss:  0.7539611\n",
      "Training Step:  6189 Loss:  0.75391\n",
      "Training Step:  6190 Loss:  0.7538602\n",
      "Training Step:  6191 Loss:  0.75381\n",
      "Training Step:  6192 Loss:  0.7537586\n",
      "Training Step:  6193 Loss:  0.7537082\n",
      "Training Step:  6194 Loss:  0.75365883\n",
      "Training Step:  6195 Loss:  0.7536083\n",
      "Training Step:  6196 Loss:  0.7535581\n",
      "Training Step:  6197 Loss:  0.7535077\n",
      "Training Step:  6198 Loss:  0.7534581\n",
      "Training Step:  6199 Loss:  0.7534074\n",
      "Training Step:  6200 Loss:  0.75335735\n",
      "Training Step:  6201 Loss:  0.75330764\n",
      "Training Step:  6202 Loss:  0.75325686\n",
      "Training Step:  6203 Loss:  0.7532079\n",
      "Training Step:  6204 Loss:  0.75315773\n",
      "Training Step:  6205 Loss:  0.753107\n",
      "Training Step:  6206 Loss:  0.75305724\n",
      "Training Step:  6207 Loss:  0.75300765\n",
      "Training Step:  6208 Loss:  0.75295734\n",
      "Training Step:  6209 Loss:  0.7529076\n",
      "Training Step:  6210 Loss:  0.75285804\n",
      "Training Step:  6211 Loss:  0.75280803\n",
      "Training Step:  6212 Loss:  0.7527584\n",
      "Training Step:  6213 Loss:  0.7527082\n",
      "Training Step:  6214 Loss:  0.75265926\n",
      "Training Step:  6215 Loss:  0.7526096\n",
      "Training Step:  6216 Loss:  0.7525596\n",
      "Training Step:  6217 Loss:  0.75251013\n",
      "Training Step:  6218 Loss:  0.7524602\n",
      "Training Step:  6219 Loss:  0.7524115\n",
      "Training Step:  6220 Loss:  0.7523608\n",
      "Training Step:  6221 Loss:  0.7523119\n",
      "Training Step:  6222 Loss:  0.75226223\n",
      "Training Step:  6223 Loss:  0.7522132\n",
      "Training Step:  6224 Loss:  0.7521631\n",
      "Training Step:  6225 Loss:  0.7521144\n",
      "Training Step:  6226 Loss:  0.7520647\n",
      "Training Step:  6227 Loss:  0.75201523\n",
      "Training Step:  6228 Loss:  0.75196594\n",
      "Training Step:  6229 Loss:  0.7519167\n",
      "Training Step:  6230 Loss:  0.7518674\n",
      "Training Step:  6231 Loss:  0.7518189\n",
      "Training Step:  6232 Loss:  0.75176847\n",
      "Training Step:  6233 Loss:  0.7517191\n",
      "Training Step:  6234 Loss:  0.7516706\n",
      "Training Step:  6235 Loss:  0.7516213\n",
      "Training Step:  6236 Loss:  0.7515719\n",
      "Training Step:  6237 Loss:  0.75152373\n",
      "Training Step:  6238 Loss:  0.7514742\n",
      "Training Step:  6239 Loss:  0.75142485\n",
      "Training Step:  6240 Loss:  0.75137657\n",
      "Training Step:  6241 Loss:  0.751327\n",
      "Training Step:  6242 Loss:  0.7512788\n",
      "Training Step:  6243 Loss:  0.7512303\n",
      "Training Step:  6244 Loss:  0.751181\n",
      "Training Step:  6245 Loss:  0.75113195\n",
      "Training Step:  6246 Loss:  0.75108385\n",
      "Training Step:  6247 Loss:  0.75103486\n",
      "Training Step:  6248 Loss:  0.75098556\n",
      "Training Step:  6249 Loss:  0.75093716\n",
      "Training Step:  6250 Loss:  0.7508886\n",
      "Training Step:  6251 Loss:  0.7508391\n",
      "Training Step:  6252 Loss:  0.75079083\n",
      "Training Step:  6253 Loss:  0.7507428\n",
      "Training Step:  6254 Loss:  0.75069416\n",
      "Training Step:  6255 Loss:  0.75064564\n",
      "Training Step:  6256 Loss:  0.75059724\n",
      "Training Step:  6257 Loss:  0.7505489\n",
      "Training Step:  6258 Loss:  0.7505004\n",
      "Training Step:  6259 Loss:  0.75045156\n",
      "Training Step:  6260 Loss:  0.7504027\n",
      "Training Step:  6261 Loss:  0.7503541\n",
      "Training Step:  6262 Loss:  0.7503061\n",
      "Training Step:  6263 Loss:  0.7502583\n",
      "Training Step:  6264 Loss:  0.7502093\n",
      "Training Step:  6265 Loss:  0.7501614\n",
      "Training Step:  6266 Loss:  0.75011355\n",
      "Training Step:  6267 Loss:  0.75006443\n",
      "Training Step:  6268 Loss:  0.7500169\n",
      "Training Step:  6269 Loss:  0.749969\n",
      "Training Step:  6270 Loss:  0.74992156\n",
      "Training Step:  6271 Loss:  0.7498726\n",
      "Training Step:  6272 Loss:  0.7498244\n",
      "Training Step:  6273 Loss:  0.74977666\n",
      "Training Step:  6274 Loss:  0.7497284\n",
      "Training Step:  6275 Loss:  0.74968046\n",
      "Training Step:  6276 Loss:  0.74963176\n",
      "Training Step:  6277 Loss:  0.7495842\n",
      "Training Step:  6278 Loss:  0.7495368\n",
      "Training Step:  6279 Loss:  0.74948895\n",
      "Training Step:  6280 Loss:  0.7494403\n",
      "Training Step:  6281 Loss:  0.74939257\n",
      "Training Step:  6282 Loss:  0.74934524\n",
      "Training Step:  6283 Loss:  0.7492976\n",
      "Training Step:  6284 Loss:  0.74924964\n",
      "Training Step:  6285 Loss:  0.7492025\n",
      "Training Step:  6286 Loss:  0.7491548\n",
      "Training Step:  6287 Loss:  0.7491064\n",
      "Training Step:  6288 Loss:  0.7490587\n",
      "Training Step:  6289 Loss:  0.749011\n",
      "Training Step:  6290 Loss:  0.7489633\n",
      "Training Step:  6291 Loss:  0.74891645\n",
      "Training Step:  6292 Loss:  0.74886894\n",
      "Training Step:  6293 Loss:  0.74882126\n",
      "Training Step:  6294 Loss:  0.7487739\n",
      "Training Step:  6295 Loss:  0.74872625\n",
      "Training Step:  6296 Loss:  0.7486783\n",
      "Training Step:  6297 Loss:  0.74863106\n",
      "Training Step:  6298 Loss:  0.7485846\n",
      "Training Step:  6299 Loss:  0.7485367\n",
      "Training Step:  6300 Loss:  0.74848974\n",
      "Training Step:  6301 Loss:  0.7484426\n",
      "Training Step:  6302 Loss:  0.7483948\n",
      "Training Step:  6303 Loss:  0.7483477\n",
      "Training Step:  6304 Loss:  0.7483003\n",
      "Training Step:  6305 Loss:  0.74825287\n",
      "Training Step:  6306 Loss:  0.7482058\n",
      "Training Step:  6307 Loss:  0.74815917\n",
      "Training Step:  6308 Loss:  0.7481121\n",
      "Training Step:  6309 Loss:  0.7480645\n",
      "Training Step:  6310 Loss:  0.7480179\n",
      "Training Step:  6311 Loss:  0.7479712\n",
      "Training Step:  6312 Loss:  0.7479236\n",
      "Training Step:  6313 Loss:  0.74787694\n",
      "Training Step:  6314 Loss:  0.74782956\n",
      "Training Step:  6315 Loss:  0.74778277\n",
      "Training Step:  6316 Loss:  0.747736\n",
      "Training Step:  6317 Loss:  0.7476892\n",
      "Training Step:  6318 Loss:  0.7476426\n",
      "Training Step:  6319 Loss:  0.74759585\n",
      "Training Step:  6320 Loss:  0.7475484\n",
      "Training Step:  6321 Loss:  0.7475026\n",
      "Training Step:  6322 Loss:  0.7474552\n",
      "Training Step:  6323 Loss:  0.7474083\n",
      "Training Step:  6324 Loss:  0.7473624\n",
      "Training Step:  6325 Loss:  0.7473157\n",
      "Training Step:  6326 Loss:  0.74726856\n",
      "Training Step:  6327 Loss:  0.7472221\n",
      "Training Step:  6328 Loss:  0.7471753\n",
      "Training Step:  6329 Loss:  0.74712944\n",
      "Training Step:  6330 Loss:  0.7470826\n",
      "Training Step:  6331 Loss:  0.7470354\n",
      "Training Step:  6332 Loss:  0.7469896\n",
      "Training Step:  6333 Loss:  0.7469421\n",
      "Training Step:  6334 Loss:  0.74689674\n",
      "Training Step:  6335 Loss:  0.74685\n",
      "Training Step:  6336 Loss:  0.74680406\n",
      "Training Step:  6337 Loss:  0.74675775\n",
      "Training Step:  6338 Loss:  0.7467107\n",
      "Training Step:  6339 Loss:  0.7466657\n",
      "Training Step:  6340 Loss:  0.7466177\n",
      "Training Step:  6341 Loss:  0.74657273\n",
      "Training Step:  6342 Loss:  0.74652684\n",
      "Training Step:  6343 Loss:  0.7464798\n",
      "Training Step:  6344 Loss:  0.7464343\n",
      "Training Step:  6345 Loss:  0.74638784\n",
      "Training Step:  6346 Loss:  0.74634147\n",
      "Training Step:  6347 Loss:  0.74629605\n",
      "Training Step:  6348 Loss:  0.74624926\n",
      "Training Step:  6349 Loss:  0.74620306\n",
      "Training Step:  6350 Loss:  0.74615777\n",
      "Training Step:  6351 Loss:  0.7461113\n",
      "Training Step:  6352 Loss:  0.7460651\n",
      "Training Step:  6353 Loss:  0.74602\n",
      "Training Step:  6354 Loss:  0.74597406\n",
      "Training Step:  6355 Loss:  0.7459273\n",
      "Training Step:  6356 Loss:  0.745882\n",
      "Training Step:  6357 Loss:  0.7458364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  6358 Loss:  0.7457901\n",
      "Training Step:  6359 Loss:  0.74574476\n",
      "Training Step:  6360 Loss:  0.7456995\n",
      "Training Step:  6361 Loss:  0.74565333\n",
      "Training Step:  6362 Loss:  0.7456076\n",
      "Training Step:  6363 Loss:  0.745562\n",
      "Training Step:  6364 Loss:  0.74551636\n",
      "Training Step:  6365 Loss:  0.7454706\n",
      "Training Step:  6366 Loss:  0.7454243\n",
      "Training Step:  6367 Loss:  0.7453794\n",
      "Training Step:  6368 Loss:  0.7453344\n",
      "Training Step:  6369 Loss:  0.7452885\n",
      "Training Step:  6370 Loss:  0.7452428\n",
      "Training Step:  6371 Loss:  0.745197\n",
      "Training Step:  6372 Loss:  0.7451523\n",
      "Training Step:  6373 Loss:  0.745107\n",
      "Training Step:  6374 Loss:  0.74506104\n",
      "Training Step:  6375 Loss:  0.745016\n",
      "Training Step:  6376 Loss:  0.74497026\n",
      "Training Step:  6377 Loss:  0.7449254\n",
      "Training Step:  6378 Loss:  0.74488\n",
      "Training Step:  6379 Loss:  0.74483454\n",
      "Training Step:  6380 Loss:  0.7447894\n",
      "Training Step:  6381 Loss:  0.74474365\n",
      "Training Step:  6382 Loss:  0.744699\n",
      "Training Step:  6383 Loss:  0.7446539\n",
      "Training Step:  6384 Loss:  0.74460876\n",
      "Training Step:  6385 Loss:  0.74456394\n",
      "Training Step:  6386 Loss:  0.74451846\n",
      "Training Step:  6387 Loss:  0.7444739\n",
      "Training Step:  6388 Loss:  0.7444284\n",
      "Training Step:  6389 Loss:  0.7443833\n",
      "Training Step:  6390 Loss:  0.7443383\n",
      "Training Step:  6391 Loss:  0.74429303\n",
      "Training Step:  6392 Loss:  0.7442484\n",
      "Training Step:  6393 Loss:  0.744204\n",
      "Training Step:  6394 Loss:  0.74415934\n",
      "Training Step:  6395 Loss:  0.74411416\n",
      "Training Step:  6396 Loss:  0.7440697\n",
      "Training Step:  6397 Loss:  0.74402434\n",
      "Training Step:  6398 Loss:  0.74397963\n",
      "Training Step:  6399 Loss:  0.74393463\n",
      "Training Step:  6400 Loss:  0.7438898\n",
      "Training Step:  6401 Loss:  0.7438446\n",
      "Training Step:  6402 Loss:  0.74380004\n",
      "Training Step:  6403 Loss:  0.7437557\n",
      "Training Step:  6404 Loss:  0.74371165\n",
      "Training Step:  6405 Loss:  0.74366605\n",
      "Training Step:  6406 Loss:  0.74362224\n",
      "Training Step:  6407 Loss:  0.7435777\n",
      "Training Step:  6408 Loss:  0.7435331\n",
      "Training Step:  6409 Loss:  0.7434883\n",
      "Training Step:  6410 Loss:  0.74344385\n",
      "Training Step:  6411 Loss:  0.74339944\n",
      "Training Step:  6412 Loss:  0.7433549\n",
      "Training Step:  6413 Loss:  0.7433098\n",
      "Training Step:  6414 Loss:  0.7432661\n",
      "Training Step:  6415 Loss:  0.7432212\n",
      "Training Step:  6416 Loss:  0.74317724\n",
      "Training Step:  6417 Loss:  0.74313354\n",
      "Training Step:  6418 Loss:  0.7430892\n",
      "Training Step:  6419 Loss:  0.7430448\n",
      "Training Step:  6420 Loss:  0.74300027\n",
      "Training Step:  6421 Loss:  0.7429565\n",
      "Training Step:  6422 Loss:  0.7429122\n",
      "Training Step:  6423 Loss:  0.7428675\n",
      "Training Step:  6424 Loss:  0.7428236\n",
      "Training Step:  6425 Loss:  0.74277973\n",
      "Training Step:  6426 Loss:  0.74273497\n",
      "Training Step:  6427 Loss:  0.742691\n",
      "Training Step:  6428 Loss:  0.74264693\n",
      "Training Step:  6429 Loss:  0.74260354\n",
      "Training Step:  6430 Loss:  0.74255884\n",
      "Training Step:  6431 Loss:  0.7425153\n",
      "Training Step:  6432 Loss:  0.7424713\n",
      "Training Step:  6433 Loss:  0.74242723\n",
      "Training Step:  6434 Loss:  0.74238294\n",
      "Training Step:  6435 Loss:  0.74233913\n",
      "Training Step:  6436 Loss:  0.74229634\n",
      "Training Step:  6437 Loss:  0.7422516\n",
      "Training Step:  6438 Loss:  0.74220836\n",
      "Training Step:  6439 Loss:  0.74216473\n",
      "Training Step:  6440 Loss:  0.74211955\n",
      "Training Step:  6441 Loss:  0.74207664\n",
      "Training Step:  6442 Loss:  0.74203354\n",
      "Training Step:  6443 Loss:  0.74198973\n",
      "Training Step:  6444 Loss:  0.74194515\n",
      "Training Step:  6445 Loss:  0.74190176\n",
      "Training Step:  6446 Loss:  0.7418583\n",
      "Training Step:  6447 Loss:  0.74181503\n",
      "Training Step:  6448 Loss:  0.7417717\n",
      "Training Step:  6449 Loss:  0.74172753\n",
      "Training Step:  6450 Loss:  0.74168426\n",
      "Training Step:  6451 Loss:  0.7416413\n",
      "Training Step:  6452 Loss:  0.7415975\n",
      "Training Step:  6453 Loss:  0.74155307\n",
      "Training Step:  6454 Loss:  0.7415106\n",
      "Training Step:  6455 Loss:  0.74146694\n",
      "Training Step:  6456 Loss:  0.74142426\n",
      "Training Step:  6457 Loss:  0.74138063\n",
      "Training Step:  6458 Loss:  0.7413366\n",
      "Training Step:  6459 Loss:  0.74129385\n",
      "Training Step:  6460 Loss:  0.74125004\n",
      "Training Step:  6461 Loss:  0.7412065\n",
      "Training Step:  6462 Loss:  0.7411638\n",
      "Training Step:  6463 Loss:  0.7411209\n",
      "Training Step:  6464 Loss:  0.74107724\n",
      "Training Step:  6465 Loss:  0.7410347\n",
      "Training Step:  6466 Loss:  0.74099094\n",
      "Training Step:  6467 Loss:  0.7409473\n",
      "Training Step:  6468 Loss:  0.74090487\n",
      "Training Step:  6469 Loss:  0.74086165\n",
      "Training Step:  6470 Loss:  0.740818\n",
      "Training Step:  6471 Loss:  0.7407757\n",
      "Training Step:  6472 Loss:  0.74073297\n",
      "Training Step:  6473 Loss:  0.74068934\n",
      "Training Step:  6474 Loss:  0.74064714\n",
      "Training Step:  6475 Loss:  0.74060416\n",
      "Training Step:  6476 Loss:  0.74056077\n",
      "Training Step:  6477 Loss:  0.7405183\n",
      "Training Step:  6478 Loss:  0.74047494\n",
      "Training Step:  6479 Loss:  0.74043244\n",
      "Training Step:  6480 Loss:  0.74038976\n",
      "Training Step:  6481 Loss:  0.7403464\n",
      "Training Step:  6482 Loss:  0.74030375\n",
      "Training Step:  6483 Loss:  0.7402606\n",
      "Training Step:  6484 Loss:  0.74021804\n",
      "Training Step:  6485 Loss:  0.74017626\n",
      "Training Step:  6486 Loss:  0.7401328\n",
      "Training Step:  6487 Loss:  0.7400908\n",
      "Training Step:  6488 Loss:  0.7400473\n",
      "Training Step:  6489 Loss:  0.7400054\n",
      "Training Step:  6490 Loss:  0.7399628\n",
      "Training Step:  6491 Loss:  0.73992085\n",
      "Training Step:  6492 Loss:  0.7398777\n",
      "Training Step:  6493 Loss:  0.7398343\n",
      "Training Step:  6494 Loss:  0.73979264\n",
      "Training Step:  6495 Loss:  0.73974967\n",
      "Training Step:  6496 Loss:  0.7397078\n",
      "Training Step:  6497 Loss:  0.7396649\n",
      "Training Step:  6498 Loss:  0.73962307\n",
      "Training Step:  6499 Loss:  0.7395799\n",
      "Training Step:  6500 Loss:  0.73953813\n",
      "Training Step:  6501 Loss:  0.73949575\n",
      "Training Step:  6502 Loss:  0.7394533\n",
      "Training Step:  6503 Loss:  0.7394111\n",
      "Training Step:  6504 Loss:  0.73936856\n",
      "Training Step:  6505 Loss:  0.7393266\n",
      "Training Step:  6506 Loss:  0.7392838\n",
      "Training Step:  6507 Loss:  0.7392423\n",
      "Training Step:  6508 Loss:  0.7392002\n",
      "Training Step:  6509 Loss:  0.7391579\n",
      "Training Step:  6510 Loss:  0.7391157\n",
      "Training Step:  6511 Loss:  0.73907393\n",
      "Training Step:  6512 Loss:  0.7390314\n",
      "Training Step:  6513 Loss:  0.7389894\n",
      "Training Step:  6514 Loss:  0.7389474\n",
      "Training Step:  6515 Loss:  0.7389051\n",
      "Training Step:  6516 Loss:  0.7388635\n",
      "Training Step:  6517 Loss:  0.73882097\n",
      "Training Step:  6518 Loss:  0.7387793\n",
      "Training Step:  6519 Loss:  0.73873794\n",
      "Training Step:  6520 Loss:  0.7386957\n",
      "Training Step:  6521 Loss:  0.73865455\n",
      "Training Step:  6522 Loss:  0.73861223\n",
      "Training Step:  6523 Loss:  0.7385696\n",
      "Training Step:  6524 Loss:  0.7385283\n",
      "Training Step:  6525 Loss:  0.73848593\n",
      "Training Step:  6526 Loss:  0.73844486\n",
      "Training Step:  6527 Loss:  0.7384027\n",
      "Training Step:  6528 Loss:  0.7383609\n",
      "Training Step:  6529 Loss:  0.7383201\n",
      "Training Step:  6530 Loss:  0.73827755\n",
      "Training Step:  6531 Loss:  0.7382355\n",
      "Training Step:  6532 Loss:  0.738195\n",
      "Training Step:  6533 Loss:  0.7381526\n",
      "Training Step:  6534 Loss:  0.7381111\n",
      "Training Step:  6535 Loss:  0.7380693\n",
      "Training Step:  6536 Loss:  0.7380282\n",
      "Training Step:  6537 Loss:  0.73798627\n",
      "Training Step:  6538 Loss:  0.7379446\n",
      "Training Step:  6539 Loss:  0.737904\n",
      "Training Step:  6540 Loss:  0.7378626\n",
      "Training Step:  6541 Loss:  0.7378212\n",
      "Training Step:  6542 Loss:  0.73777974\n",
      "Training Step:  6543 Loss:  0.73773795\n",
      "Training Step:  6544 Loss:  0.7376966\n",
      "Training Step:  6545 Loss:  0.73765486\n",
      "Training Step:  6546 Loss:  0.73761445\n",
      "Training Step:  6547 Loss:  0.7375726\n",
      "Training Step:  6548 Loss:  0.73753136\n",
      "Training Step:  6549 Loss:  0.73749024\n",
      "Training Step:  6550 Loss:  0.7374489\n",
      "Training Step:  6551 Loss:  0.7374078\n",
      "Training Step:  6552 Loss:  0.7373667\n",
      "Training Step:  6553 Loss:  0.7373249\n",
      "Training Step:  6554 Loss:  0.7372837\n",
      "Training Step:  6555 Loss:  0.73724365\n",
      "Training Step:  6556 Loss:  0.7372022\n",
      "Training Step:  6557 Loss:  0.7371616\n",
      "Training Step:  6558 Loss:  0.73712\n",
      "Training Step:  6559 Loss:  0.7370782\n",
      "Training Step:  6560 Loss:  0.73703766\n",
      "Training Step:  6561 Loss:  0.7369971\n",
      "Training Step:  6562 Loss:  0.7369556\n",
      "Training Step:  6563 Loss:  0.7369144\n",
      "Training Step:  6564 Loss:  0.73687345\n",
      "Training Step:  6565 Loss:  0.73683244\n",
      "Training Step:  6566 Loss:  0.7367918\n",
      "Training Step:  6567 Loss:  0.7367517\n",
      "Training Step:  6568 Loss:  0.7367107\n",
      "Training Step:  6569 Loss:  0.7366699\n",
      "Training Step:  6570 Loss:  0.7366288\n",
      "Training Step:  6571 Loss:  0.7365884\n",
      "Training Step:  6572 Loss:  0.73654735\n",
      "Training Step:  6573 Loss:  0.7365062\n",
      "Training Step:  6574 Loss:  0.7364653\n",
      "Training Step:  6575 Loss:  0.73642504\n",
      "Training Step:  6576 Loss:  0.73638433\n",
      "Training Step:  6577 Loss:  0.73634386\n",
      "Training Step:  6578 Loss:  0.7363029\n",
      "Training Step:  6579 Loss:  0.73626256\n",
      "Training Step:  6580 Loss:  0.7362214\n",
      "Training Step:  6581 Loss:  0.73618114\n",
      "Training Step:  6582 Loss:  0.7361411\n",
      "Training Step:  6583 Loss:  0.73610014\n",
      "Training Step:  6584 Loss:  0.736059\n",
      "Training Step:  6585 Loss:  0.7360187\n",
      "Training Step:  6586 Loss:  0.7359785\n",
      "Training Step:  6587 Loss:  0.73593783\n",
      "Training Step:  6588 Loss:  0.73589766\n",
      "Training Step:  6589 Loss:  0.7358573\n",
      "Training Step:  6590 Loss:  0.7358168\n",
      "Training Step:  6591 Loss:  0.7357764\n",
      "Training Step:  6592 Loss:  0.73573637\n",
      "Training Step:  6593 Loss:  0.735696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  6594 Loss:  0.735656\n",
      "Training Step:  6595 Loss:  0.7356153\n",
      "Training Step:  6596 Loss:  0.73557556\n",
      "Training Step:  6597 Loss:  0.7355351\n",
      "Training Step:  6598 Loss:  0.735495\n",
      "Training Step:  6599 Loss:  0.73545444\n",
      "Training Step:  6600 Loss:  0.7354143\n",
      "Training Step:  6601 Loss:  0.7353741\n",
      "Training Step:  6602 Loss:  0.735335\n",
      "Training Step:  6603 Loss:  0.7352945\n",
      "Training Step:  6604 Loss:  0.7352542\n",
      "Training Step:  6605 Loss:  0.7352142\n",
      "Training Step:  6606 Loss:  0.73517406\n",
      "Training Step:  6607 Loss:  0.7351347\n",
      "Training Step:  6608 Loss:  0.7350943\n",
      "Training Step:  6609 Loss:  0.7350542\n",
      "Training Step:  6610 Loss:  0.73501486\n",
      "Training Step:  6611 Loss:  0.73497456\n",
      "Training Step:  6612 Loss:  0.734934\n",
      "Training Step:  6613 Loss:  0.73489445\n",
      "Training Step:  6614 Loss:  0.7348551\n",
      "Training Step:  6615 Loss:  0.73481524\n",
      "Training Step:  6616 Loss:  0.7347752\n",
      "Training Step:  6617 Loss:  0.73473567\n",
      "Training Step:  6618 Loss:  0.7346959\n",
      "Training Step:  6619 Loss:  0.7346556\n",
      "Training Step:  6620 Loss:  0.7346163\n",
      "Training Step:  6621 Loss:  0.73457617\n",
      "Training Step:  6622 Loss:  0.7345367\n",
      "Training Step:  6623 Loss:  0.7344968\n",
      "Training Step:  6624 Loss:  0.73445725\n",
      "Training Step:  6625 Loss:  0.7344172\n",
      "Training Step:  6626 Loss:  0.7343777\n",
      "Training Step:  6627 Loss:  0.7343387\n",
      "Training Step:  6628 Loss:  0.7342995\n",
      "Training Step:  6629 Loss:  0.7342595\n",
      "Training Step:  6630 Loss:  0.73422015\n",
      "Training Step:  6631 Loss:  0.7341795\n",
      "Training Step:  6632 Loss:  0.7341405\n",
      "Training Step:  6633 Loss:  0.7341016\n",
      "Training Step:  6634 Loss:  0.7340616\n",
      "Training Step:  6635 Loss:  0.73402286\n",
      "Training Step:  6636 Loss:  0.73398274\n",
      "Training Step:  6637 Loss:  0.73394376\n",
      "Training Step:  6638 Loss:  0.73390436\n",
      "Training Step:  6639 Loss:  0.73386526\n",
      "Training Step:  6640 Loss:  0.7338257\n",
      "Training Step:  6641 Loss:  0.73378557\n",
      "Training Step:  6642 Loss:  0.7337467\n",
      "Training Step:  6643 Loss:  0.7337079\n",
      "Training Step:  6644 Loss:  0.7336685\n",
      "Training Step:  6645 Loss:  0.73362947\n",
      "Training Step:  6646 Loss:  0.73358977\n",
      "Training Step:  6647 Loss:  0.7335509\n",
      "Training Step:  6648 Loss:  0.733511\n",
      "Training Step:  6649 Loss:  0.73347247\n",
      "Training Step:  6650 Loss:  0.73343366\n",
      "Training Step:  6651 Loss:  0.7333937\n",
      "Training Step:  6652 Loss:  0.7333549\n",
      "Training Step:  6653 Loss:  0.7333161\n",
      "Training Step:  6654 Loss:  0.73327655\n",
      "Training Step:  6655 Loss:  0.7332373\n",
      "Training Step:  6656 Loss:  0.7331988\n",
      "Training Step:  6657 Loss:  0.73316014\n",
      "Training Step:  6658 Loss:  0.7331206\n",
      "Training Step:  6659 Loss:  0.73308206\n",
      "Training Step:  6660 Loss:  0.7330426\n",
      "Training Step:  6661 Loss:  0.73300356\n",
      "Training Step:  6662 Loss:  0.732965\n",
      "Training Step:  6663 Loss:  0.7329261\n",
      "Training Step:  6664 Loss:  0.73288745\n",
      "Training Step:  6665 Loss:  0.73284954\n",
      "Training Step:  6666 Loss:  0.73280966\n",
      "Training Step:  6667 Loss:  0.73277116\n",
      "Training Step:  6668 Loss:  0.73273236\n",
      "Training Step:  6669 Loss:  0.73269355\n",
      "Training Step:  6670 Loss:  0.73265517\n",
      "Training Step:  6671 Loss:  0.73261607\n",
      "Training Step:  6672 Loss:  0.7325781\n",
      "Training Step:  6673 Loss:  0.73253924\n",
      "Training Step:  6674 Loss:  0.7324999\n",
      "Training Step:  6675 Loss:  0.7324625\n",
      "Training Step:  6676 Loss:  0.7324227\n",
      "Training Step:  6677 Loss:  0.73238456\n",
      "Training Step:  6678 Loss:  0.73234564\n",
      "Training Step:  6679 Loss:  0.73230773\n",
      "Training Step:  6680 Loss:  0.73226935\n",
      "Training Step:  6681 Loss:  0.73223037\n",
      "Training Step:  6682 Loss:  0.73219186\n",
      "Training Step:  6683 Loss:  0.732153\n",
      "Training Step:  6684 Loss:  0.7321148\n",
      "Training Step:  6685 Loss:  0.73207575\n",
      "Training Step:  6686 Loss:  0.73203814\n",
      "Training Step:  6687 Loss:  0.73199975\n",
      "Training Step:  6688 Loss:  0.7319615\n",
      "Training Step:  6689 Loss:  0.7319224\n",
      "Training Step:  6690 Loss:  0.73188496\n",
      "Training Step:  6691 Loss:  0.7318462\n",
      "Training Step:  6692 Loss:  0.73180795\n",
      "Training Step:  6693 Loss:  0.73176944\n",
      "Training Step:  6694 Loss:  0.73173136\n",
      "Training Step:  6695 Loss:  0.7316932\n",
      "Training Step:  6696 Loss:  0.73165506\n",
      "Training Step:  6697 Loss:  0.7316166\n",
      "Training Step:  6698 Loss:  0.73157895\n",
      "Training Step:  6699 Loss:  0.7315406\n",
      "Training Step:  6700 Loss:  0.7315029\n",
      "Training Step:  6701 Loss:  0.7314638\n",
      "Training Step:  6702 Loss:  0.73142624\n",
      "Training Step:  6703 Loss:  0.73138756\n",
      "Training Step:  6704 Loss:  0.7313496\n",
      "Training Step:  6705 Loss:  0.7313123\n",
      "Training Step:  6706 Loss:  0.73127407\n",
      "Training Step:  6707 Loss:  0.7312364\n",
      "Training Step:  6708 Loss:  0.7311976\n",
      "Training Step:  6709 Loss:  0.73115987\n",
      "Training Step:  6710 Loss:  0.7311222\n",
      "Training Step:  6711 Loss:  0.73108363\n",
      "Training Step:  6712 Loss:  0.7310462\n",
      "Training Step:  6713 Loss:  0.73100877\n",
      "Training Step:  6714 Loss:  0.7309706\n",
      "Training Step:  6715 Loss:  0.7309331\n",
      "Training Step:  6716 Loss:  0.7308945\n",
      "Training Step:  6717 Loss:  0.73085755\n",
      "Training Step:  6718 Loss:  0.7308187\n",
      "Training Step:  6719 Loss:  0.7307817\n",
      "Training Step:  6720 Loss:  0.7307441\n",
      "Training Step:  6721 Loss:  0.73070574\n",
      "Training Step:  6722 Loss:  0.7306685\n",
      "Training Step:  6723 Loss:  0.7306309\n",
      "Training Step:  6724 Loss:  0.7305929\n",
      "Training Step:  6725 Loss:  0.7305554\n",
      "Training Step:  6726 Loss:  0.7305175\n",
      "Training Step:  6727 Loss:  0.73047954\n",
      "Training Step:  6728 Loss:  0.7304427\n",
      "Training Step:  6729 Loss:  0.7304052\n",
      "Training Step:  6730 Loss:  0.7303669\n",
      "Training Step:  6731 Loss:  0.73032963\n",
      "Training Step:  6732 Loss:  0.7302927\n",
      "Training Step:  6733 Loss:  0.73025465\n",
      "Training Step:  6734 Loss:  0.7302171\n",
      "Training Step:  6735 Loss:  0.73018014\n",
      "Training Step:  6736 Loss:  0.7301422\n",
      "Training Step:  6737 Loss:  0.7301048\n",
      "Training Step:  6738 Loss:  0.73006666\n",
      "Training Step:  6739 Loss:  0.73002994\n",
      "Training Step:  6740 Loss:  0.7299929\n",
      "Training Step:  6741 Loss:  0.7299558\n",
      "Training Step:  6742 Loss:  0.72991776\n",
      "Training Step:  6743 Loss:  0.7298809\n",
      "Training Step:  6744 Loss:  0.7298432\n",
      "Training Step:  6745 Loss:  0.7298063\n",
      "Training Step:  6746 Loss:  0.7297686\n",
      "Training Step:  6747 Loss:  0.7297318\n",
      "Training Step:  6748 Loss:  0.72969466\n",
      "Training Step:  6749 Loss:  0.72965753\n",
      "Training Step:  6750 Loss:  0.72961974\n",
      "Training Step:  6751 Loss:  0.7295831\n",
      "Training Step:  6752 Loss:  0.72954583\n",
      "Training Step:  6753 Loss:  0.7295091\n",
      "Training Step:  6754 Loss:  0.72947145\n",
      "Training Step:  6755 Loss:  0.7294344\n",
      "Training Step:  6756 Loss:  0.7293968\n",
      "Training Step:  6757 Loss:  0.7293602\n",
      "Training Step:  6758 Loss:  0.7293237\n",
      "Training Step:  6759 Loss:  0.7292861\n",
      "Training Step:  6760 Loss:  0.7292493\n",
      "Training Step:  6761 Loss:  0.72921216\n",
      "Training Step:  6762 Loss:  0.7291752\n",
      "Training Step:  6763 Loss:  0.7291389\n",
      "Training Step:  6764 Loss:  0.72910184\n",
      "Training Step:  6765 Loss:  0.7290646\n",
      "Training Step:  6766 Loss:  0.72902787\n",
      "Training Step:  6767 Loss:  0.72899115\n",
      "Training Step:  6768 Loss:  0.7289532\n",
      "Training Step:  6769 Loss:  0.72891694\n",
      "Training Step:  6770 Loss:  0.7288799\n",
      "Training Step:  6771 Loss:  0.72884333\n",
      "Training Step:  6772 Loss:  0.72880745\n",
      "Training Step:  6773 Loss:  0.7287698\n",
      "Training Step:  6774 Loss:  0.7287334\n",
      "Training Step:  6775 Loss:  0.7286966\n",
      "Training Step:  6776 Loss:  0.72866076\n",
      "Training Step:  6777 Loss:  0.72862285\n",
      "Training Step:  6778 Loss:  0.72858614\n",
      "Training Step:  6779 Loss:  0.7285492\n",
      "Training Step:  6780 Loss:  0.7285131\n",
      "Training Step:  6781 Loss:  0.72847563\n",
      "Training Step:  6782 Loss:  0.72844017\n",
      "Training Step:  6783 Loss:  0.7284032\n",
      "Training Step:  6784 Loss:  0.72836673\n",
      "Training Step:  6785 Loss:  0.7283308\n",
      "Training Step:  6786 Loss:  0.72829366\n",
      "Training Step:  6787 Loss:  0.728257\n",
      "Training Step:  6788 Loss:  0.7282208\n",
      "Training Step:  6789 Loss:  0.72818476\n",
      "Training Step:  6790 Loss:  0.72814834\n",
      "Training Step:  6791 Loss:  0.72811186\n",
      "Training Step:  6792 Loss:  0.7280749\n",
      "Training Step:  6793 Loss:  0.728039\n",
      "Training Step:  6794 Loss:  0.72800267\n",
      "Training Step:  6795 Loss:  0.7279662\n",
      "Training Step:  6796 Loss:  0.7279297\n",
      "Training Step:  6797 Loss:  0.72789365\n",
      "Training Step:  6798 Loss:  0.72785664\n",
      "Training Step:  6799 Loss:  0.7278211\n",
      "Training Step:  6800 Loss:  0.72778475\n",
      "Training Step:  6801 Loss:  0.7277487\n",
      "Training Step:  6802 Loss:  0.7277118\n",
      "Training Step:  6803 Loss:  0.72767544\n",
      "Training Step:  6804 Loss:  0.72763956\n",
      "Training Step:  6805 Loss:  0.7276035\n",
      "Training Step:  6806 Loss:  0.72756743\n",
      "Training Step:  6807 Loss:  0.72753096\n",
      "Training Step:  6808 Loss:  0.7274955\n",
      "Training Step:  6809 Loss:  0.7274587\n",
      "Training Step:  6810 Loss:  0.7274233\n",
      "Training Step:  6811 Loss:  0.7273868\n",
      "Training Step:  6812 Loss:  0.7273507\n",
      "Training Step:  6813 Loss:  0.7273147\n",
      "Training Step:  6814 Loss:  0.72727937\n",
      "Training Step:  6815 Loss:  0.7272426\n",
      "Training Step:  6816 Loss:  0.7272064\n",
      "Training Step:  6817 Loss:  0.7271706\n",
      "Training Step:  6818 Loss:  0.72713405\n",
      "Training Step:  6819 Loss:  0.727099\n",
      "Training Step:  6820 Loss:  0.72706294\n",
      "Training Step:  6821 Loss:  0.7270273\n",
      "Training Step:  6822 Loss:  0.72699106\n",
      "Training Step:  6823 Loss:  0.7269562\n",
      "Training Step:  6824 Loss:  0.7269191\n",
      "Training Step:  6825 Loss:  0.726883\n",
      "Training Step:  6826 Loss:  0.726848\n",
      "Training Step:  6827 Loss:  0.7268119\n",
      "Training Step:  6828 Loss:  0.7267765\n",
      "Training Step:  6829 Loss:  0.726741\n",
      "Training Step:  6830 Loss:  0.72670454\n",
      "Training Step:  6831 Loss:  0.72666925\n",
      "Training Step:  6832 Loss:  0.7266333\n",
      "Training Step:  6833 Loss:  0.72659814\n",
      "Training Step:  6834 Loss:  0.7265618\n",
      "Training Step:  6835 Loss:  0.7265263\n",
      "Training Step:  6836 Loss:  0.72649074\n",
      "Training Step:  6837 Loss:  0.7264551\n",
      "Training Step:  6838 Loss:  0.7264199\n",
      "Training Step:  6839 Loss:  0.72638386\n",
      "Training Step:  6840 Loss:  0.7263489\n",
      "Training Step:  6841 Loss:  0.72631276\n",
      "Training Step:  6842 Loss:  0.7262773\n",
      "Training Step:  6843 Loss:  0.72624266\n",
      "Training Step:  6844 Loss:  0.7262064\n",
      "Training Step:  6845 Loss:  0.7261701\n",
      "Training Step:  6846 Loss:  0.7261351\n",
      "Training Step:  6847 Loss:  0.7260998\n",
      "Training Step:  6848 Loss:  0.7260645\n",
      "Training Step:  6849 Loss:  0.7260293\n",
      "Training Step:  6850 Loss:  0.7259933\n",
      "Training Step:  6851 Loss:  0.72595847\n",
      "Training Step:  6852 Loss:  0.7259226\n",
      "Training Step:  6853 Loss:  0.72588766\n",
      "Training Step:  6854 Loss:  0.72585285\n",
      "Training Step:  6855 Loss:  0.7258167\n",
      "Training Step:  6856 Loss:  0.72578126\n",
      "Training Step:  6857 Loss:  0.72574645\n",
      "Training Step:  6858 Loss:  0.72571164\n",
      "Training Step:  6859 Loss:  0.7256758\n",
      "Training Step:  6860 Loss:  0.72564054\n",
      "Training Step:  6861 Loss:  0.72560567\n",
      "Training Step:  6862 Loss:  0.7255709\n",
      "Training Step:  6863 Loss:  0.725535\n",
      "Training Step:  6864 Loss:  0.72550005\n",
      "Training Step:  6865 Loss:  0.7254652\n",
      "Training Step:  6866 Loss:  0.72543013\n",
      "Training Step:  6867 Loss:  0.7253946\n",
      "Training Step:  6868 Loss:  0.72536016\n",
      "Training Step:  6869 Loss:  0.72532445\n",
      "Training Step:  6870 Loss:  0.72528964\n",
      "Training Step:  6871 Loss:  0.7252545\n",
      "Training Step:  6872 Loss:  0.7252193\n",
      "Training Step:  6873 Loss:  0.7251851\n",
      "Training Step:  6874 Loss:  0.7251493\n",
      "Training Step:  6875 Loss:  0.725115\n",
      "Training Step:  6876 Loss:  0.7250798\n",
      "Training Step:  6877 Loss:  0.72504383\n",
      "Training Step:  6878 Loss:  0.7250102\n",
      "Training Step:  6879 Loss:  0.7249749\n",
      "Training Step:  6880 Loss:  0.7249399\n",
      "Training Step:  6881 Loss:  0.72490585\n",
      "Training Step:  6882 Loss:  0.72487044\n",
      "Training Step:  6883 Loss:  0.72483575\n",
      "Training Step:  6884 Loss:  0.7248005\n",
      "Training Step:  6885 Loss:  0.72476625\n",
      "Training Step:  6886 Loss:  0.72473097\n",
      "Training Step:  6887 Loss:  0.7246959\n",
      "Training Step:  6888 Loss:  0.724662\n",
      "Training Step:  6889 Loss:  0.72462684\n",
      "Training Step:  6890 Loss:  0.72459245\n",
      "Training Step:  6891 Loss:  0.7245573\n",
      "Training Step:  6892 Loss:  0.7245235\n",
      "Training Step:  6893 Loss:  0.7244878\n",
      "Training Step:  6894 Loss:  0.7244538\n",
      "Training Step:  6895 Loss:  0.72441834\n",
      "Training Step:  6896 Loss:  0.72438496\n",
      "Training Step:  6897 Loss:  0.72435\n",
      "Training Step:  6898 Loss:  0.72431517\n",
      "Training Step:  6899 Loss:  0.7242806\n",
      "Training Step:  6900 Loss:  0.7242464\n",
      "Training Step:  6901 Loss:  0.7242114\n",
      "Training Step:  6902 Loss:  0.72417724\n",
      "Training Step:  6903 Loss:  0.7241428\n",
      "Training Step:  6904 Loss:  0.7241082\n",
      "Training Step:  6905 Loss:  0.72407395\n",
      "Training Step:  6906 Loss:  0.7240395\n",
      "Training Step:  6907 Loss:  0.7240044\n",
      "Training Step:  6908 Loss:  0.7239703\n",
      "Training Step:  6909 Loss:  0.7239357\n",
      "Training Step:  6910 Loss:  0.7239018\n",
      "Training Step:  6911 Loss:  0.72386736\n",
      "Training Step:  6912 Loss:  0.72383326\n",
      "Training Step:  6913 Loss:  0.723799\n",
      "Training Step:  6914 Loss:  0.7237639\n",
      "Training Step:  6915 Loss:  0.7237304\n",
      "Training Step:  6916 Loss:  0.7236955\n",
      "Training Step:  6917 Loss:  0.72366154\n",
      "Training Step:  6918 Loss:  0.7236275\n",
      "Training Step:  6919 Loss:  0.7235931\n",
      "Training Step:  6920 Loss:  0.72355866\n",
      "Training Step:  6921 Loss:  0.72352415\n",
      "Training Step:  6922 Loss:  0.7234911\n",
      "Training Step:  6923 Loss:  0.72345597\n",
      "Training Step:  6924 Loss:  0.7234226\n",
      "Training Step:  6925 Loss:  0.7233875\n",
      "Training Step:  6926 Loss:  0.72335327\n",
      "Training Step:  6927 Loss:  0.7233199\n",
      "Training Step:  6928 Loss:  0.7232855\n",
      "Training Step:  6929 Loss:  0.7232519\n",
      "Training Step:  6930 Loss:  0.7232173\n",
      "Training Step:  6931 Loss:  0.7231829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  6932 Loss:  0.72315025\n",
      "Training Step:  6933 Loss:  0.72311586\n",
      "Training Step:  6934 Loss:  0.7230813\n",
      "Training Step:  6935 Loss:  0.7230476\n",
      "Training Step:  6936 Loss:  0.72301304\n",
      "Training Step:  6937 Loss:  0.72297984\n",
      "Training Step:  6938 Loss:  0.72294605\n",
      "Training Step:  6939 Loss:  0.7229121\n",
      "Training Step:  6940 Loss:  0.72287786\n",
      "Training Step:  6941 Loss:  0.7228443\n",
      "Training Step:  6942 Loss:  0.7228096\n",
      "Training Step:  6943 Loss:  0.72277606\n",
      "Training Step:  6944 Loss:  0.722743\n",
      "Training Step:  6945 Loss:  0.72270906\n",
      "Training Step:  6946 Loss:  0.7226751\n",
      "Training Step:  6947 Loss:  0.7226412\n",
      "Training Step:  6948 Loss:  0.72260714\n",
      "Training Step:  6949 Loss:  0.7225729\n",
      "Training Step:  6950 Loss:  0.7225399\n",
      "Training Step:  6951 Loss:  0.7225062\n",
      "Training Step:  6952 Loss:  0.72247326\n",
      "Training Step:  6953 Loss:  0.72243893\n",
      "Training Step:  6954 Loss:  0.7224053\n",
      "Training Step:  6955 Loss:  0.7223715\n",
      "Training Step:  6956 Loss:  0.722338\n",
      "Training Step:  6957 Loss:  0.72230446\n",
      "Training Step:  6958 Loss:  0.72227025\n",
      "Training Step:  6959 Loss:  0.72223735\n",
      "Training Step:  6960 Loss:  0.72220385\n",
      "Training Step:  6961 Loss:  0.72216964\n",
      "Training Step:  6962 Loss:  0.722136\n",
      "Training Step:  6963 Loss:  0.722103\n",
      "Training Step:  6964 Loss:  0.7220693\n",
      "Training Step:  6965 Loss:  0.722036\n",
      "Training Step:  6966 Loss:  0.72200185\n",
      "Training Step:  6967 Loss:  0.7219684\n",
      "Training Step:  6968 Loss:  0.7219359\n",
      "Training Step:  6969 Loss:  0.72190255\n",
      "Training Step:  6970 Loss:  0.7218683\n",
      "Training Step:  6971 Loss:  0.7218353\n",
      "Training Step:  6972 Loss:  0.721802\n",
      "Training Step:  6973 Loss:  0.7217689\n",
      "Training Step:  6974 Loss:  0.72173536\n",
      "Training Step:  6975 Loss:  0.72170186\n",
      "Training Step:  6976 Loss:  0.72166824\n",
      "Training Step:  6977 Loss:  0.72163475\n",
      "Training Step:  6978 Loss:  0.7216015\n",
      "Training Step:  6979 Loss:  0.72156894\n",
      "Training Step:  6980 Loss:  0.7215358\n",
      "Training Step:  6981 Loss:  0.7215023\n",
      "Training Step:  6982 Loss:  0.7214694\n",
      "Training Step:  6983 Loss:  0.7214358\n",
      "Training Step:  6984 Loss:  0.7214022\n",
      "Training Step:  6985 Loss:  0.72136945\n",
      "Training Step:  6986 Loss:  0.72133625\n",
      "Training Step:  6987 Loss:  0.72130346\n",
      "Training Step:  6988 Loss:  0.72127044\n",
      "Training Step:  6989 Loss:  0.7212366\n",
      "Training Step:  6990 Loss:  0.72120345\n",
      "Training Step:  6991 Loss:  0.7211707\n",
      "Training Step:  6992 Loss:  0.72113746\n",
      "Training Step:  6993 Loss:  0.7211044\n",
      "Training Step:  6994 Loss:  0.7210718\n",
      "Training Step:  6995 Loss:  0.72103775\n",
      "Training Step:  6996 Loss:  0.7210052\n",
      "Training Step:  6997 Loss:  0.72097224\n",
      "Training Step:  6998 Loss:  0.72093964\n",
      "Training Step:  6999 Loss:  0.7209066\n",
      "Training Step:  7000 Loss:  0.72087383\n",
      "Training Step:  7001 Loss:  0.72084117\n",
      "Training Step:  7002 Loss:  0.72080785\n",
      "Training Step:  7003 Loss:  0.720775\n",
      "Training Step:  7004 Loss:  0.7207426\n",
      "Training Step:  7005 Loss:  0.720709\n",
      "Training Step:  7006 Loss:  0.72067666\n",
      "Training Step:  7007 Loss:  0.7206439\n",
      "Training Step:  7008 Loss:  0.7206107\n",
      "Training Step:  7009 Loss:  0.7205776\n",
      "Training Step:  7010 Loss:  0.72054446\n",
      "Training Step:  7011 Loss:  0.72051203\n",
      "Training Step:  7012 Loss:  0.72047937\n",
      "Training Step:  7013 Loss:  0.7204466\n",
      "Training Step:  7014 Loss:  0.72041374\n",
      "Training Step:  7015 Loss:  0.72038066\n",
      "Training Step:  7016 Loss:  0.7203485\n",
      "Training Step:  7017 Loss:  0.720316\n",
      "Training Step:  7018 Loss:  0.72028315\n",
      "Training Step:  7019 Loss:  0.7202509\n",
      "Training Step:  7020 Loss:  0.7202182\n",
      "Training Step:  7021 Loss:  0.7201856\n",
      "Training Step:  7022 Loss:  0.7201526\n",
      "Training Step:  7023 Loss:  0.7201202\n",
      "Training Step:  7024 Loss:  0.7200879\n",
      "Training Step:  7025 Loss:  0.72005516\n",
      "Training Step:  7026 Loss:  0.7200227\n",
      "Training Step:  7027 Loss:  0.71998996\n",
      "Training Step:  7028 Loss:  0.7199569\n",
      "Training Step:  7029 Loss:  0.7199245\n",
      "Training Step:  7030 Loss:  0.7198914\n",
      "Training Step:  7031 Loss:  0.71985865\n",
      "Training Step:  7032 Loss:  0.7198268\n",
      "Training Step:  7033 Loss:  0.7197947\n",
      "Training Step:  7034 Loss:  0.7197615\n",
      "Training Step:  7035 Loss:  0.71973026\n",
      "Training Step:  7036 Loss:  0.71969706\n",
      "Training Step:  7037 Loss:  0.71966505\n",
      "Training Step:  7038 Loss:  0.71963257\n",
      "Training Step:  7039 Loss:  0.7196004\n",
      "Training Step:  7040 Loss:  0.71956795\n",
      "Training Step:  7041 Loss:  0.7195349\n",
      "Training Step:  7042 Loss:  0.7195035\n",
      "Training Step:  7043 Loss:  0.7194711\n",
      "Training Step:  7044 Loss:  0.71943927\n",
      "Training Step:  7045 Loss:  0.7194069\n",
      "Training Step:  7046 Loss:  0.71937436\n",
      "Training Step:  7047 Loss:  0.7193419\n",
      "Training Step:  7048 Loss:  0.71930945\n",
      "Training Step:  7049 Loss:  0.71927726\n",
      "Training Step:  7050 Loss:  0.7192449\n",
      "Training Step:  7051 Loss:  0.7192129\n",
      "Training Step:  7052 Loss:  0.71918124\n",
      "Training Step:  7053 Loss:  0.71914876\n",
      "Training Step:  7054 Loss:  0.71911603\n",
      "Training Step:  7055 Loss:  0.7190843\n",
      "Training Step:  7056 Loss:  0.71905214\n",
      "Training Step:  7057 Loss:  0.71902084\n",
      "Training Step:  7058 Loss:  0.7189885\n",
      "Training Step:  7059 Loss:  0.7189559\n",
      "Training Step:  7060 Loss:  0.7189238\n",
      "Training Step:  7061 Loss:  0.7188915\n",
      "Training Step:  7062 Loss:  0.7188603\n",
      "Training Step:  7063 Loss:  0.7188278\n",
      "Training Step:  7064 Loss:  0.71879584\n",
      "Training Step:  7065 Loss:  0.71876395\n",
      "Training Step:  7066 Loss:  0.7187326\n",
      "Training Step:  7067 Loss:  0.7187004\n",
      "Training Step:  7068 Loss:  0.718668\n",
      "Training Step:  7069 Loss:  0.71863604\n",
      "Training Step:  7070 Loss:  0.7186044\n",
      "Training Step:  7071 Loss:  0.7185722\n",
      "Training Step:  7072 Loss:  0.7185405\n",
      "Training Step:  7073 Loss:  0.7185087\n",
      "Training Step:  7074 Loss:  0.71847695\n",
      "Training Step:  7075 Loss:  0.7184451\n",
      "Training Step:  7076 Loss:  0.7184125\n",
      "Training Step:  7077 Loss:  0.7183813\n",
      "Training Step:  7078 Loss:  0.7183493\n",
      "Training Step:  7079 Loss:  0.7183179\n",
      "Training Step:  7080 Loss:  0.71828556\n",
      "Training Step:  7081 Loss:  0.71825397\n",
      "Training Step:  7082 Loss:  0.7182231\n",
      "Training Step:  7083 Loss:  0.71819055\n",
      "Training Step:  7084 Loss:  0.71815896\n",
      "Training Step:  7085 Loss:  0.7181272\n",
      "Training Step:  7086 Loss:  0.71809554\n",
      "Training Step:  7087 Loss:  0.71806383\n",
      "Training Step:  7088 Loss:  0.71803266\n",
      "Training Step:  7089 Loss:  0.7180012\n",
      "Training Step:  7090 Loss:  0.7179692\n",
      "Training Step:  7091 Loss:  0.71793735\n",
      "Training Step:  7092 Loss:  0.7179063\n",
      "Training Step:  7093 Loss:  0.7178745\n",
      "Training Step:  7094 Loss:  0.71784306\n",
      "Training Step:  7095 Loss:  0.7178113\n",
      "Training Step:  7096 Loss:  0.7177794\n",
      "Training Step:  7097 Loss:  0.7177484\n",
      "Training Step:  7098 Loss:  0.71771693\n",
      "Training Step:  7099 Loss:  0.71768504\n",
      "Training Step:  7100 Loss:  0.71765393\n",
      "Training Step:  7101 Loss:  0.7176219\n",
      "Training Step:  7102 Loss:  0.7175911\n",
      "Training Step:  7103 Loss:  0.7175596\n",
      "Training Step:  7104 Loss:  0.7175286\n",
      "Training Step:  7105 Loss:  0.7174965\n",
      "Training Step:  7106 Loss:  0.71746486\n",
      "Training Step:  7107 Loss:  0.71743387\n",
      "Training Step:  7108 Loss:  0.71740276\n",
      "Training Step:  7109 Loss:  0.7173709\n",
      "Training Step:  7110 Loss:  0.7173399\n",
      "Training Step:  7111 Loss:  0.7173078\n",
      "Training Step:  7112 Loss:  0.71727717\n",
      "Training Step:  7113 Loss:  0.7172454\n",
      "Training Step:  7114 Loss:  0.71721447\n",
      "Training Step:  7115 Loss:  0.7171834\n",
      "Training Step:  7116 Loss:  0.7171519\n",
      "Training Step:  7117 Loss:  0.71712023\n",
      "Training Step:  7118 Loss:  0.71708965\n",
      "Training Step:  7119 Loss:  0.7170581\n",
      "Training Step:  7120 Loss:  0.71702677\n",
      "Training Step:  7121 Loss:  0.71699566\n",
      "Training Step:  7122 Loss:  0.71696484\n",
      "Training Step:  7123 Loss:  0.7169335\n",
      "Training Step:  7124 Loss:  0.71690273\n",
      "Training Step:  7125 Loss:  0.71687114\n",
      "Training Step:  7126 Loss:  0.7168402\n",
      "Training Step:  7127 Loss:  0.71680945\n",
      "Training Step:  7128 Loss:  0.71677804\n",
      "Training Step:  7129 Loss:  0.71674675\n",
      "Training Step:  7130 Loss:  0.71671593\n",
      "Training Step:  7131 Loss:  0.7166849\n",
      "Training Step:  7132 Loss:  0.7166538\n",
      "Training Step:  7133 Loss:  0.7166226\n",
      "Training Step:  7134 Loss:  0.71659154\n",
      "Training Step:  7135 Loss:  0.71656084\n",
      "Training Step:  7136 Loss:  0.71653\n",
      "Training Step:  7137 Loss:  0.7164981\n",
      "Training Step:  7138 Loss:  0.716468\n",
      "Training Step:  7139 Loss:  0.7164372\n",
      "Training Step:  7140 Loss:  0.71640605\n",
      "Training Step:  7141 Loss:  0.7163751\n",
      "Training Step:  7142 Loss:  0.7163443\n",
      "Training Step:  7143 Loss:  0.7163138\n",
      "Training Step:  7144 Loss:  0.7162817\n",
      "Training Step:  7145 Loss:  0.7162516\n",
      "Training Step:  7146 Loss:  0.7162211\n",
      "Training Step:  7147 Loss:  0.7161904\n",
      "Training Step:  7148 Loss:  0.716159\n",
      "Training Step:  7149 Loss:  0.7161281\n",
      "Training Step:  7150 Loss:  0.71609753\n",
      "Training Step:  7151 Loss:  0.71606624\n",
      "Training Step:  7152 Loss:  0.7160359\n",
      "Training Step:  7153 Loss:  0.7160049\n",
      "Training Step:  7154 Loss:  0.7159747\n",
      "Training Step:  7155 Loss:  0.7159432\n",
      "Training Step:  7156 Loss:  0.7159131\n",
      "Training Step:  7157 Loss:  0.71588236\n",
      "Training Step:  7158 Loss:  0.7158517\n",
      "Training Step:  7159 Loss:  0.71582085\n",
      "Training Step:  7160 Loss:  0.71579045\n",
      "Training Step:  7161 Loss:  0.71575904\n",
      "Training Step:  7162 Loss:  0.71572936\n",
      "Training Step:  7163 Loss:  0.7156988\n",
      "Training Step:  7164 Loss:  0.7156676\n",
      "Training Step:  7165 Loss:  0.7156371\n",
      "Training Step:  7166 Loss:  0.7156069\n",
      "Training Step:  7167 Loss:  0.71557665\n",
      "Training Step:  7168 Loss:  0.7155455\n",
      "Training Step:  7169 Loss:  0.71551514\n",
      "Training Step:  7170 Loss:  0.71548516\n",
      "Training Step:  7171 Loss:  0.71545416\n",
      "Training Step:  7172 Loss:  0.71542346\n",
      "Training Step:  7173 Loss:  0.7153933\n",
      "Training Step:  7174 Loss:  0.71536237\n",
      "Training Step:  7175 Loss:  0.7153323\n",
      "Training Step:  7176 Loss:  0.71530175\n",
      "Training Step:  7177 Loss:  0.7152708\n",
      "Training Step:  7178 Loss:  0.71524096\n",
      "Training Step:  7179 Loss:  0.7152104\n",
      "Training Step:  7180 Loss:  0.7151805\n",
      "Training Step:  7181 Loss:  0.71514964\n",
      "Training Step:  7182 Loss:  0.71511877\n",
      "Training Step:  7183 Loss:  0.7150893\n",
      "Training Step:  7184 Loss:  0.7150588\n",
      "Training Step:  7185 Loss:  0.71502805\n",
      "Training Step:  7186 Loss:  0.7149984\n",
      "Training Step:  7187 Loss:  0.7149677\n",
      "Training Step:  7188 Loss:  0.71493685\n",
      "Training Step:  7189 Loss:  0.7149069\n",
      "Training Step:  7190 Loss:  0.7148772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  7191 Loss:  0.71484697\n",
      "Training Step:  7192 Loss:  0.71481615\n",
      "Training Step:  7193 Loss:  0.7147857\n",
      "Training Step:  7194 Loss:  0.7147562\n",
      "Training Step:  7195 Loss:  0.7147263\n",
      "Training Step:  7196 Loss:  0.71469545\n",
      "Training Step:  7197 Loss:  0.7146652\n",
      "Training Step:  7198 Loss:  0.71463555\n",
      "Training Step:  7199 Loss:  0.7146051\n",
      "Training Step:  7200 Loss:  0.71457565\n",
      "Training Step:  7201 Loss:  0.71454525\n",
      "Training Step:  7202 Loss:  0.71451503\n",
      "Training Step:  7203 Loss:  0.7144848\n",
      "Training Step:  7204 Loss:  0.7144546\n",
      "Training Step:  7205 Loss:  0.71442515\n",
      "Training Step:  7206 Loss:  0.7143947\n",
      "Training Step:  7207 Loss:  0.71436465\n",
      "Training Step:  7208 Loss:  0.71433496\n",
      "Training Step:  7209 Loss:  0.7143049\n",
      "Training Step:  7210 Loss:  0.71427464\n",
      "Training Step:  7211 Loss:  0.7142444\n",
      "Training Step:  7212 Loss:  0.7142147\n",
      "Training Step:  7213 Loss:  0.7141845\n",
      "Training Step:  7214 Loss:  0.7141547\n",
      "Training Step:  7215 Loss:  0.7141253\n",
      "Training Step:  7216 Loss:  0.7140953\n",
      "Training Step:  7217 Loss:  0.7140649\n",
      "Training Step:  7218 Loss:  0.71403515\n",
      "Training Step:  7219 Loss:  0.71400505\n",
      "Training Step:  7220 Loss:  0.713976\n",
      "Training Step:  7221 Loss:  0.71394616\n",
      "Training Step:  7222 Loss:  0.71391606\n",
      "Training Step:  7223 Loss:  0.71388596\n",
      "Training Step:  7224 Loss:  0.71385616\n",
      "Training Step:  7225 Loss:  0.71382624\n",
      "Training Step:  7226 Loss:  0.7137964\n",
      "Training Step:  7227 Loss:  0.7137662\n",
      "Training Step:  7228 Loss:  0.71373737\n",
      "Training Step:  7229 Loss:  0.71370757\n",
      "Training Step:  7230 Loss:  0.71367806\n",
      "Training Step:  7231 Loss:  0.71364784\n",
      "Training Step:  7232 Loss:  0.71361816\n",
      "Training Step:  7233 Loss:  0.71358883\n",
      "Training Step:  7234 Loss:  0.7135584\n",
      "Training Step:  7235 Loss:  0.7135294\n",
      "Training Step:  7236 Loss:  0.7134995\n",
      "Training Step:  7237 Loss:  0.7134696\n",
      "Training Step:  7238 Loss:  0.7134396\n",
      "Training Step:  7239 Loss:  0.71340984\n",
      "Training Step:  7240 Loss:  0.71338046\n",
      "Training Step:  7241 Loss:  0.7133511\n",
      "Training Step:  7242 Loss:  0.7133223\n",
      "Training Step:  7243 Loss:  0.71329224\n",
      "Training Step:  7244 Loss:  0.713263\n",
      "Training Step:  7245 Loss:  0.71323335\n",
      "Training Step:  7246 Loss:  0.71320385\n",
      "Training Step:  7247 Loss:  0.71317375\n",
      "Training Step:  7248 Loss:  0.7131444\n",
      "Training Step:  7249 Loss:  0.713115\n",
      "Training Step:  7250 Loss:  0.7130856\n",
      "Training Step:  7251 Loss:  0.713056\n",
      "Training Step:  7252 Loss:  0.7130266\n",
      "Training Step:  7253 Loss:  0.71299684\n",
      "Training Step:  7254 Loss:  0.7129684\n",
      "Training Step:  7255 Loss:  0.7129377\n",
      "Training Step:  7256 Loss:  0.7129093\n",
      "Training Step:  7257 Loss:  0.7128794\n",
      "Training Step:  7258 Loss:  0.71285015\n",
      "Training Step:  7259 Loss:  0.71282065\n",
      "Training Step:  7260 Loss:  0.71279144\n",
      "Training Step:  7261 Loss:  0.71276176\n",
      "Training Step:  7262 Loss:  0.712733\n",
      "Training Step:  7263 Loss:  0.712703\n",
      "Training Step:  7264 Loss:  0.7126742\n",
      "Training Step:  7265 Loss:  0.7126446\n",
      "Training Step:  7266 Loss:  0.71261555\n",
      "Training Step:  7267 Loss:  0.7125863\n",
      "Training Step:  7268 Loss:  0.7125572\n",
      "Training Step:  7269 Loss:  0.71252745\n",
      "Training Step:  7270 Loss:  0.71249783\n",
      "Training Step:  7271 Loss:  0.7124693\n",
      "Training Step:  7272 Loss:  0.7124395\n",
      "Training Step:  7273 Loss:  0.71241015\n",
      "Training Step:  7274 Loss:  0.712381\n",
      "Training Step:  7275 Loss:  0.71235216\n",
      "Training Step:  7276 Loss:  0.71232283\n",
      "Training Step:  7277 Loss:  0.71229446\n",
      "Training Step:  7278 Loss:  0.7122645\n",
      "Training Step:  7279 Loss:  0.7122358\n",
      "Training Step:  7280 Loss:  0.712207\n",
      "Training Step:  7281 Loss:  0.7121778\n",
      "Training Step:  7282 Loss:  0.71214837\n",
      "Training Step:  7283 Loss:  0.7121188\n",
      "Training Step:  7284 Loss:  0.71209\n",
      "Training Step:  7285 Loss:  0.71206105\n",
      "Training Step:  7286 Loss:  0.7120324\n",
      "Training Step:  7287 Loss:  0.71200335\n",
      "Training Step:  7288 Loss:  0.71197474\n",
      "Training Step:  7289 Loss:  0.71194524\n",
      "Training Step:  7290 Loss:  0.71191627\n",
      "Training Step:  7291 Loss:  0.711887\n",
      "Training Step:  7292 Loss:  0.7118577\n",
      "Training Step:  7293 Loss:  0.71182954\n",
      "Training Step:  7294 Loss:  0.7118005\n",
      "Training Step:  7295 Loss:  0.7117714\n",
      "Training Step:  7296 Loss:  0.7117425\n",
      "Training Step:  7297 Loss:  0.7117138\n",
      "Training Step:  7298 Loss:  0.7116847\n",
      "Training Step:  7299 Loss:  0.7116553\n",
      "Training Step:  7300 Loss:  0.71162695\n",
      "Training Step:  7301 Loss:  0.71159804\n",
      "Training Step:  7302 Loss:  0.7115691\n",
      "Training Step:  7303 Loss:  0.71154004\n",
      "Training Step:  7304 Loss:  0.71151096\n",
      "Training Step:  7305 Loss:  0.7114825\n",
      "Training Step:  7306 Loss:  0.711454\n",
      "Training Step:  7307 Loss:  0.7114259\n",
      "Training Step:  7308 Loss:  0.71139646\n",
      "Training Step:  7309 Loss:  0.7113674\n",
      "Training Step:  7310 Loss:  0.7113394\n",
      "Training Step:  7311 Loss:  0.71131057\n",
      "Training Step:  7312 Loss:  0.71128166\n",
      "Training Step:  7313 Loss:  0.71125257\n",
      "Training Step:  7314 Loss:  0.71122384\n",
      "Training Step:  7315 Loss:  0.7111956\n",
      "Training Step:  7316 Loss:  0.7111662\n",
      "Training Step:  7317 Loss:  0.71113783\n",
      "Training Step:  7318 Loss:  0.71110904\n",
      "Training Step:  7319 Loss:  0.7110803\n",
      "Training Step:  7320 Loss:  0.71105206\n",
      "Training Step:  7321 Loss:  0.71102315\n",
      "Training Step:  7322 Loss:  0.7109945\n",
      "Training Step:  7323 Loss:  0.71096605\n",
      "Training Step:  7324 Loss:  0.7109372\n",
      "Training Step:  7325 Loss:  0.71090853\n",
      "Training Step:  7326 Loss:  0.71088046\n",
      "Training Step:  7327 Loss:  0.71085197\n",
      "Training Step:  7328 Loss:  0.7108239\n",
      "Training Step:  7329 Loss:  0.71079516\n",
      "Training Step:  7330 Loss:  0.71076584\n",
      "Training Step:  7331 Loss:  0.7107381\n",
      "Training Step:  7332 Loss:  0.7107096\n",
      "Training Step:  7333 Loss:  0.710681\n",
      "Training Step:  7334 Loss:  0.7106528\n",
      "Training Step:  7335 Loss:  0.7106238\n",
      "Training Step:  7336 Loss:  0.7105951\n",
      "Training Step:  7337 Loss:  0.7105675\n",
      "Training Step:  7338 Loss:  0.71053857\n",
      "Training Step:  7339 Loss:  0.7105103\n",
      "Training Step:  7340 Loss:  0.71048176\n",
      "Training Step:  7341 Loss:  0.71045345\n",
      "Training Step:  7342 Loss:  0.71042466\n",
      "Training Step:  7343 Loss:  0.7103968\n",
      "Training Step:  7344 Loss:  0.7103687\n",
      "Training Step:  7345 Loss:  0.7103398\n",
      "Training Step:  7346 Loss:  0.7103118\n",
      "Training Step:  7347 Loss:  0.71028346\n",
      "Training Step:  7348 Loss:  0.71025455\n",
      "Training Step:  7349 Loss:  0.7102267\n",
      "Training Step:  7350 Loss:  0.7101984\n",
      "Training Step:  7351 Loss:  0.7101703\n",
      "Training Step:  7352 Loss:  0.7101422\n",
      "Training Step:  7353 Loss:  0.7101135\n",
      "Training Step:  7354 Loss:  0.71008503\n",
      "Training Step:  7355 Loss:  0.71005774\n",
      "Training Step:  7356 Loss:  0.71002877\n",
      "Training Step:  7357 Loss:  0.71000075\n",
      "Training Step:  7358 Loss:  0.7099731\n",
      "Training Step:  7359 Loss:  0.70994455\n",
      "Training Step:  7360 Loss:  0.7099165\n",
      "Training Step:  7361 Loss:  0.70988786\n",
      "Training Step:  7362 Loss:  0.7098605\n",
      "Training Step:  7363 Loss:  0.70983195\n",
      "Training Step:  7364 Loss:  0.7098036\n",
      "Training Step:  7365 Loss:  0.70977604\n",
      "Training Step:  7366 Loss:  0.70974743\n",
      "Training Step:  7367 Loss:  0.7097196\n",
      "Training Step:  7368 Loss:  0.7096905\n",
      "Training Step:  7369 Loss:  0.709663\n",
      "Training Step:  7370 Loss:  0.7096352\n",
      "Training Step:  7371 Loss:  0.7096071\n",
      "Training Step:  7372 Loss:  0.7095797\n",
      "Training Step:  7373 Loss:  0.70955086\n",
      "Training Step:  7374 Loss:  0.7095233\n",
      "Training Step:  7375 Loss:  0.70949495\n",
      "Training Step:  7376 Loss:  0.70946735\n",
      "Training Step:  7377 Loss:  0.7094391\n",
      "Training Step:  7378 Loss:  0.7094117\n",
      "Training Step:  7379 Loss:  0.70938337\n",
      "Training Step:  7380 Loss:  0.70935494\n",
      "Training Step:  7381 Loss:  0.70932734\n",
      "Training Step:  7382 Loss:  0.709299\n",
      "Training Step:  7383 Loss:  0.70927197\n",
      "Training Step:  7384 Loss:  0.7092436\n",
      "Training Step:  7385 Loss:  0.7092155\n",
      "Training Step:  7386 Loss:  0.70918775\n",
      "Training Step:  7387 Loss:  0.7091609\n",
      "Training Step:  7388 Loss:  0.7091323\n",
      "Training Step:  7389 Loss:  0.7091042\n",
      "Training Step:  7390 Loss:  0.7090769\n",
      "Training Step:  7391 Loss:  0.70904887\n",
      "Training Step:  7392 Loss:  0.70902073\n",
      "Training Step:  7393 Loss:  0.7089931\n",
      "Training Step:  7394 Loss:  0.70896536\n",
      "Training Step:  7395 Loss:  0.7089378\n",
      "Training Step:  7396 Loss:  0.70891064\n",
      "Training Step:  7397 Loss:  0.7088818\n",
      "Training Step:  7398 Loss:  0.70885473\n",
      "Training Step:  7399 Loss:  0.70882607\n",
      "Training Step:  7400 Loss:  0.70879924\n",
      "Training Step:  7401 Loss:  0.70877147\n",
      "Training Step:  7402 Loss:  0.70874363\n",
      "Training Step:  7403 Loss:  0.70871544\n",
      "Training Step:  7404 Loss:  0.70868844\n",
      "Training Step:  7405 Loss:  0.7086606\n",
      "Training Step:  7406 Loss:  0.7086328\n",
      "Training Step:  7407 Loss:  0.7086056\n",
      "Training Step:  7408 Loss:  0.7085768\n",
      "Training Step:  7409 Loss:  0.7085504\n",
      "Training Step:  7410 Loss:  0.7085223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  7411 Loss:  0.7084947\n",
      "Training Step:  7412 Loss:  0.70846736\n",
      "Training Step:  7413 Loss:  0.70843893\n",
      "Training Step:  7414 Loss:  0.708413\n",
      "Training Step:  7415 Loss:  0.70838475\n",
      "Training Step:  7416 Loss:  0.70835686\n",
      "Training Step:  7417 Loss:  0.70832986\n",
      "Training Step:  7418 Loss:  0.70830154\n",
      "Training Step:  7419 Loss:  0.70827496\n",
      "Training Step:  7420 Loss:  0.7082469\n",
      "Training Step:  7421 Loss:  0.7082196\n",
      "Training Step:  7422 Loss:  0.7081922\n",
      "Training Step:  7423 Loss:  0.7081645\n",
      "Training Step:  7424 Loss:  0.70813704\n",
      "Training Step:  7425 Loss:  0.7081099\n",
      "Training Step:  7426 Loss:  0.7080824\n",
      "Training Step:  7427 Loss:  0.7080545\n",
      "Training Step:  7428 Loss:  0.7080272\n",
      "Training Step:  7429 Loss:  0.7079992\n",
      "Training Step:  7430 Loss:  0.7079733\n",
      "Training Step:  7431 Loss:  0.7079454\n",
      "Training Step:  7432 Loss:  0.7079181\n",
      "Training Step:  7433 Loss:  0.70789057\n",
      "Training Step:  7434 Loss:  0.7078628\n",
      "Training Step:  7435 Loss:  0.70783556\n",
      "Training Step:  7436 Loss:  0.7078092\n",
      "Training Step:  7437 Loss:  0.70778203\n",
      "Training Step:  7438 Loss:  0.70775384\n",
      "Training Step:  7439 Loss:  0.7077271\n",
      "Training Step:  7440 Loss:  0.70769995\n",
      "Training Step:  7441 Loss:  0.7076715\n",
      "Training Step:  7442 Loss:  0.70764524\n",
      "Training Step:  7443 Loss:  0.70761776\n",
      "Training Step:  7444 Loss:  0.7075909\n",
      "Training Step:  7445 Loss:  0.7075631\n",
      "Training Step:  7446 Loss:  0.70753604\n",
      "Training Step:  7447 Loss:  0.7075092\n",
      "Training Step:  7448 Loss:  0.7074812\n",
      "Training Step:  7449 Loss:  0.7074543\n",
      "Training Step:  7450 Loss:  0.7074275\n",
      "Training Step:  7451 Loss:  0.70739985\n",
      "Training Step:  7452 Loss:  0.7073733\n",
      "Training Step:  7453 Loss:  0.7073456\n",
      "Training Step:  7454 Loss:  0.7073189\n",
      "Training Step:  7455 Loss:  0.7072912\n",
      "Training Step:  7456 Loss:  0.7072646\n",
      "Training Step:  7457 Loss:  0.7072371\n",
      "Training Step:  7458 Loss:  0.7072105\n",
      "Training Step:  7459 Loss:  0.7071836\n",
      "Training Step:  7460 Loss:  0.7071562\n",
      "Training Step:  7461 Loss:  0.7071288\n",
      "Training Step:  7462 Loss:  0.70710206\n",
      "Training Step:  7463 Loss:  0.70707494\n",
      "Training Step:  7464 Loss:  0.7070478\n",
      "Training Step:  7465 Loss:  0.7070208\n",
      "Training Step:  7466 Loss:  0.7069941\n",
      "Training Step:  7467 Loss:  0.7069675\n",
      "Training Step:  7468 Loss:  0.70694023\n",
      "Training Step:  7469 Loss:  0.7069129\n",
      "Training Step:  7470 Loss:  0.70688623\n",
      "Training Step:  7471 Loss:  0.7068591\n",
      "Training Step:  7472 Loss:  0.7068318\n",
      "Training Step:  7473 Loss:  0.70680463\n",
      "Training Step:  7474 Loss:  0.7067781\n",
      "Training Step:  7475 Loss:  0.7067513\n",
      "Training Step:  7476 Loss:  0.7067255\n",
      "Training Step:  7477 Loss:  0.70669794\n",
      "Training Step:  7478 Loss:  0.7066713\n",
      "Training Step:  7479 Loss:  0.7066439\n",
      "Training Step:  7480 Loss:  0.70661694\n",
      "Training Step:  7481 Loss:  0.70658976\n",
      "Training Step:  7482 Loss:  0.7065637\n",
      "Training Step:  7483 Loss:  0.7065369\n",
      "Training Step:  7484 Loss:  0.70650995\n",
      "Training Step:  7485 Loss:  0.7064828\n",
      "Training Step:  7486 Loss:  0.7064558\n",
      "Training Step:  7487 Loss:  0.70642865\n",
      "Training Step:  7488 Loss:  0.70640194\n",
      "Training Step:  7489 Loss:  0.7063762\n",
      "Training Step:  7490 Loss:  0.7063491\n",
      "Training Step:  7491 Loss:  0.7063231\n",
      "Training Step:  7492 Loss:  0.70629597\n",
      "Training Step:  7493 Loss:  0.70626855\n",
      "Training Step:  7494 Loss:  0.70624185\n",
      "Training Step:  7495 Loss:  0.7062154\n",
      "Training Step:  7496 Loss:  0.7061888\n",
      "Training Step:  7497 Loss:  0.70616245\n",
      "Training Step:  7498 Loss:  0.7061357\n",
      "Training Step:  7499 Loss:  0.7061092\n",
      "Training Step:  7500 Loss:  0.70608246\n",
      "Training Step:  7501 Loss:  0.706056\n",
      "Training Step:  7502 Loss:  0.7060293\n",
      "Training Step:  7503 Loss:  0.70600235\n",
      "Training Step:  7504 Loss:  0.70597553\n",
      "Training Step:  7505 Loss:  0.7059488\n",
      "Training Step:  7506 Loss:  0.70592195\n",
      "Training Step:  7507 Loss:  0.70589536\n",
      "Training Step:  7508 Loss:  0.70586944\n",
      "Training Step:  7509 Loss:  0.7058421\n",
      "Training Step:  7510 Loss:  0.7058168\n",
      "Training Step:  7511 Loss:  0.70579034\n",
      "Training Step:  7512 Loss:  0.70576346\n",
      "Training Step:  7513 Loss:  0.70573676\n",
      "Training Step:  7514 Loss:  0.7057107\n",
      "Training Step:  7515 Loss:  0.7056839\n",
      "Training Step:  7516 Loss:  0.70565736\n",
      "Training Step:  7517 Loss:  0.7056307\n",
      "Training Step:  7518 Loss:  0.7056049\n",
      "Training Step:  7519 Loss:  0.70557845\n",
      "Training Step:  7520 Loss:  0.7055523\n",
      "Training Step:  7521 Loss:  0.70552576\n",
      "Training Step:  7522 Loss:  0.70549864\n",
      "Training Step:  7523 Loss:  0.70547265\n",
      "Training Step:  7524 Loss:  0.7054458\n",
      "Training Step:  7525 Loss:  0.7054198\n",
      "Training Step:  7526 Loss:  0.7053931\n",
      "Training Step:  7527 Loss:  0.70536715\n",
      "Training Step:  7528 Loss:  0.7053404\n",
      "Training Step:  7529 Loss:  0.70531416\n",
      "Training Step:  7530 Loss:  0.70528775\n",
      "Training Step:  7531 Loss:  0.70526206\n",
      "Training Step:  7532 Loss:  0.7052354\n",
      "Training Step:  7533 Loss:  0.705209\n",
      "Training Step:  7534 Loss:  0.70518255\n",
      "Training Step:  7535 Loss:  0.70515627\n",
      "Training Step:  7536 Loss:  0.7051299\n",
      "Training Step:  7537 Loss:  0.70510405\n",
      "Training Step:  7538 Loss:  0.7050777\n",
      "Training Step:  7539 Loss:  0.70505154\n",
      "Training Step:  7540 Loss:  0.7050249\n",
      "Training Step:  7541 Loss:  0.70499873\n",
      "Training Step:  7542 Loss:  0.70497274\n",
      "Training Step:  7543 Loss:  0.7049463\n",
      "Training Step:  7544 Loss:  0.7049198\n",
      "Training Step:  7545 Loss:  0.70489323\n",
      "Training Step:  7546 Loss:  0.7048668\n",
      "Training Step:  7547 Loss:  0.70484126\n",
      "Training Step:  7548 Loss:  0.7048152\n",
      "Training Step:  7549 Loss:  0.7047887\n",
      "Training Step:  7550 Loss:  0.70476294\n",
      "Training Step:  7551 Loss:  0.7047367\n",
      "Training Step:  7552 Loss:  0.7047108\n",
      "Training Step:  7553 Loss:  0.7046848\n",
      "Training Step:  7554 Loss:  0.70465827\n",
      "Training Step:  7555 Loss:  0.7046323\n",
      "Training Step:  7556 Loss:  0.7046058\n",
      "Training Step:  7557 Loss:  0.70457995\n",
      "Training Step:  7558 Loss:  0.70455444\n",
      "Training Step:  7559 Loss:  0.7045283\n",
      "Training Step:  7560 Loss:  0.704502\n",
      "Training Step:  7561 Loss:  0.70447606\n",
      "Training Step:  7562 Loss:  0.7044505\n",
      "Training Step:  7563 Loss:  0.70442426\n",
      "Training Step:  7564 Loss:  0.70439816\n",
      "Training Step:  7565 Loss:  0.7043717\n",
      "Training Step:  7566 Loss:  0.70434594\n",
      "Training Step:  7567 Loss:  0.7043195\n",
      "Training Step:  7568 Loss:  0.7042934\n",
      "Training Step:  7569 Loss:  0.70426786\n",
      "Training Step:  7570 Loss:  0.7042422\n",
      "Training Step:  7571 Loss:  0.7042163\n",
      "Training Step:  7572 Loss:  0.7041906\n",
      "Training Step:  7573 Loss:  0.70416456\n",
      "Training Step:  7574 Loss:  0.7041382\n",
      "Training Step:  7575 Loss:  0.7041125\n",
      "Training Step:  7576 Loss:  0.70408773\n",
      "Training Step:  7577 Loss:  0.70406085\n",
      "Training Step:  7578 Loss:  0.70403475\n",
      "Training Step:  7579 Loss:  0.7040088\n",
      "Training Step:  7580 Loss:  0.7039832\n",
      "Training Step:  7581 Loss:  0.7039573\n",
      "Training Step:  7582 Loss:  0.70393133\n",
      "Training Step:  7583 Loss:  0.7039056\n",
      "Training Step:  7584 Loss:  0.70388025\n",
      "Training Step:  7585 Loss:  0.7038542\n",
      "Training Step:  7586 Loss:  0.7038289\n",
      "Training Step:  7587 Loss:  0.70380265\n",
      "Training Step:  7588 Loss:  0.7037768\n",
      "Training Step:  7589 Loss:  0.7037509\n",
      "Training Step:  7590 Loss:  0.7037249\n",
      "Training Step:  7591 Loss:  0.70369965\n",
      "Training Step:  7592 Loss:  0.7036741\n",
      "Training Step:  7593 Loss:  0.70364845\n",
      "Training Step:  7594 Loss:  0.70362216\n",
      "Training Step:  7595 Loss:  0.7035973\n",
      "Training Step:  7596 Loss:  0.703571\n",
      "Training Step:  7597 Loss:  0.7035449\n",
      "Training Step:  7598 Loss:  0.703519\n",
      "Training Step:  7599 Loss:  0.7034941\n",
      "Training Step:  7600 Loss:  0.7034681\n",
      "Training Step:  7601 Loss:  0.7034423\n",
      "Training Step:  7602 Loss:  0.70341676\n",
      "Training Step:  7603 Loss:  0.7033912\n",
      "Training Step:  7604 Loss:  0.703366\n",
      "Training Step:  7605 Loss:  0.7033399\n",
      "Training Step:  7606 Loss:  0.7033143\n",
      "Training Step:  7607 Loss:  0.7032883\n",
      "Training Step:  7608 Loss:  0.70326364\n",
      "Training Step:  7609 Loss:  0.7032378\n",
      "Training Step:  7610 Loss:  0.7032122\n",
      "Training Step:  7611 Loss:  0.70318675\n",
      "Training Step:  7612 Loss:  0.70316017\n",
      "Training Step:  7613 Loss:  0.7031354\n",
      "Training Step:  7614 Loss:  0.7031104\n",
      "Training Step:  7615 Loss:  0.7030845\n",
      "Training Step:  7616 Loss:  0.70305926\n",
      "Training Step:  7617 Loss:  0.70303386\n",
      "Training Step:  7618 Loss:  0.70300853\n",
      "Training Step:  7619 Loss:  0.7029826\n",
      "Training Step:  7620 Loss:  0.7029572\n",
      "Training Step:  7621 Loss:  0.70293176\n",
      "Training Step:  7622 Loss:  0.7029061\n",
      "Training Step:  7623 Loss:  0.70288116\n",
      "Training Step:  7624 Loss:  0.70285565\n",
      "Training Step:  7625 Loss:  0.7028294\n",
      "Training Step:  7626 Loss:  0.70280445\n",
      "Training Step:  7627 Loss:  0.7027792\n",
      "Training Step:  7628 Loss:  0.7027537\n",
      "Training Step:  7629 Loss:  0.7027282\n",
      "Training Step:  7630 Loss:  0.70270324\n",
      "Training Step:  7631 Loss:  0.7026767\n",
      "Training Step:  7632 Loss:  0.7026515\n",
      "Training Step:  7633 Loss:  0.70262635\n",
      "Training Step:  7634 Loss:  0.7026018\n",
      "Training Step:  7635 Loss:  0.70257664\n",
      "Training Step:  7636 Loss:  0.7025502\n",
      "Training Step:  7637 Loss:  0.7025259\n",
      "Training Step:  7638 Loss:  0.70250034\n",
      "Training Step:  7639 Loss:  0.7024753\n",
      "Training Step:  7640 Loss:  0.70245016\n",
      "Training Step:  7641 Loss:  0.70242405\n",
      "Training Step:  7642 Loss:  0.7023986\n",
      "Training Step:  7643 Loss:  0.7023735\n",
      "Training Step:  7644 Loss:  0.7023485\n",
      "Training Step:  7645 Loss:  0.70232385\n",
      "Training Step:  7646 Loss:  0.70229775\n",
      "Training Step:  7647 Loss:  0.7022725\n",
      "Training Step:  7648 Loss:  0.7022475\n",
      "Training Step:  7649 Loss:  0.70222294\n",
      "Training Step:  7650 Loss:  0.7021979\n",
      "Training Step:  7651 Loss:  0.7021713\n",
      "Training Step:  7652 Loss:  0.7021471\n",
      "Training Step:  7653 Loss:  0.7021216\n",
      "Training Step:  7654 Loss:  0.7020968\n",
      "Training Step:  7655 Loss:  0.7020716\n",
      "Training Step:  7656 Loss:  0.70204574\n",
      "Training Step:  7657 Loss:  0.702021\n",
      "Training Step:  7658 Loss:  0.7019961\n",
      "Training Step:  7659 Loss:  0.7019716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  7660 Loss:  0.7019455\n",
      "Training Step:  7661 Loss:  0.701921\n",
      "Training Step:  7662 Loss:  0.7018956\n",
      "Training Step:  7663 Loss:  0.7018708\n",
      "Training Step:  7664 Loss:  0.7018459\n",
      "Training Step:  7665 Loss:  0.70182025\n",
      "Training Step:  7666 Loss:  0.7017956\n",
      "Training Step:  7667 Loss:  0.7017709\n",
      "Training Step:  7668 Loss:  0.7017461\n",
      "Training Step:  7669 Loss:  0.70172006\n",
      "Training Step:  7670 Loss:  0.70169497\n",
      "Training Step:  7671 Loss:  0.7016703\n",
      "Training Step:  7672 Loss:  0.70164555\n",
      "Training Step:  7673 Loss:  0.7016203\n",
      "Training Step:  7674 Loss:  0.7015951\n",
      "Training Step:  7675 Loss:  0.7015702\n",
      "Training Step:  7676 Loss:  0.70154566\n",
      "Training Step:  7677 Loss:  0.70152\n",
      "Training Step:  7678 Loss:  0.70149565\n",
      "Training Step:  7679 Loss:  0.70147043\n",
      "Training Step:  7680 Loss:  0.701446\n",
      "Training Step:  7681 Loss:  0.7014204\n",
      "Training Step:  7682 Loss:  0.70139587\n",
      "Training Step:  7683 Loss:  0.70137113\n",
      "Training Step:  7684 Loss:  0.70134664\n",
      "Training Step:  7685 Loss:  0.7013209\n",
      "Training Step:  7686 Loss:  0.70129627\n",
      "Training Step:  7687 Loss:  0.7012718\n",
      "Training Step:  7688 Loss:  0.70124704\n",
      "Training Step:  7689 Loss:  0.70122176\n",
      "Training Step:  7690 Loss:  0.70119697\n",
      "Training Step:  7691 Loss:  0.70117235\n",
      "Training Step:  7692 Loss:  0.7011465\n",
      "Training Step:  7693 Loss:  0.7011222\n",
      "Training Step:  7694 Loss:  0.7010977\n",
      "Training Step:  7695 Loss:  0.70107335\n",
      "Training Step:  7696 Loss:  0.7010482\n",
      "Training Step:  7697 Loss:  0.70102316\n",
      "Training Step:  7698 Loss:  0.7009994\n",
      "Training Step:  7699 Loss:  0.70097363\n",
      "Training Step:  7700 Loss:  0.7009492\n",
      "Training Step:  7701 Loss:  0.7009243\n",
      "Training Step:  7702 Loss:  0.70089996\n",
      "Training Step:  7703 Loss:  0.7008748\n",
      "Training Step:  7704 Loss:  0.70085025\n",
      "Training Step:  7705 Loss:  0.7008257\n",
      "Training Step:  7706 Loss:  0.70080036\n",
      "Training Step:  7707 Loss:  0.7007761\n",
      "Training Step:  7708 Loss:  0.7007512\n",
      "Training Step:  7709 Loss:  0.70072645\n",
      "Training Step:  7710 Loss:  0.700702\n",
      "Training Step:  7711 Loss:  0.7006773\n",
      "Training Step:  7712 Loss:  0.70065266\n",
      "Training Step:  7713 Loss:  0.7006282\n",
      "Training Step:  7714 Loss:  0.70060354\n",
      "Training Step:  7715 Loss:  0.70057976\n",
      "Training Step:  7716 Loss:  0.70055366\n",
      "Training Step:  7717 Loss:  0.7005297\n",
      "Training Step:  7718 Loss:  0.7005055\n",
      "Training Step:  7719 Loss:  0.70048046\n",
      "Training Step:  7720 Loss:  0.70045567\n",
      "Training Step:  7721 Loss:  0.70043147\n",
      "Training Step:  7722 Loss:  0.7004066\n",
      "Training Step:  7723 Loss:  0.7003824\n",
      "Training Step:  7724 Loss:  0.70035815\n",
      "Training Step:  7725 Loss:  0.7003333\n",
      "Training Step:  7726 Loss:  0.70030844\n",
      "Training Step:  7727 Loss:  0.70028406\n",
      "Training Step:  7728 Loss:  0.70025975\n",
      "Training Step:  7729 Loss:  0.7002353\n",
      "Training Step:  7730 Loss:  0.700211\n",
      "Training Step:  7731 Loss:  0.7001857\n",
      "Training Step:  7732 Loss:  0.7001618\n",
      "Training Step:  7733 Loss:  0.7001375\n",
      "Training Step:  7734 Loss:  0.7001126\n",
      "Training Step:  7735 Loss:  0.7000886\n",
      "Training Step:  7736 Loss:  0.70006436\n",
      "Training Step:  7737 Loss:  0.70003927\n",
      "Training Step:  7738 Loss:  0.70001525\n",
      "Training Step:  7739 Loss:  0.69999105\n",
      "Training Step:  7740 Loss:  0.6999659\n",
      "Training Step:  7741 Loss:  0.6999422\n",
      "Training Step:  7742 Loss:  0.69991726\n",
      "Training Step:  7743 Loss:  0.6998933\n",
      "Training Step:  7744 Loss:  0.6998688\n",
      "Training Step:  7745 Loss:  0.6998443\n",
      "Training Step:  7746 Loss:  0.6998201\n",
      "Training Step:  7747 Loss:  0.69979584\n",
      "Training Step:  7748 Loss:  0.6997712\n",
      "Training Step:  7749 Loss:  0.69974715\n",
      "Training Step:  7750 Loss:  0.699723\n",
      "Training Step:  7751 Loss:  0.6996982\n",
      "Training Step:  7752 Loss:  0.69967437\n",
      "Training Step:  7753 Loss:  0.69964975\n",
      "Training Step:  7754 Loss:  0.69962573\n",
      "Training Step:  7755 Loss:  0.69960135\n",
      "Training Step:  7756 Loss:  0.6995766\n",
      "Training Step:  7757 Loss:  0.69955266\n",
      "Training Step:  7758 Loss:  0.69952863\n",
      "Training Step:  7759 Loss:  0.69950426\n",
      "Training Step:  7760 Loss:  0.6994801\n",
      "Training Step:  7761 Loss:  0.69945526\n",
      "Training Step:  7762 Loss:  0.69943184\n",
      "Training Step:  7763 Loss:  0.69940764\n",
      "Training Step:  7764 Loss:  0.69938344\n",
      "Training Step:  7765 Loss:  0.6993587\n",
      "Training Step:  7766 Loss:  0.69933474\n",
      "Training Step:  7767 Loss:  0.6993109\n",
      "Training Step:  7768 Loss:  0.6992863\n",
      "Training Step:  7769 Loss:  0.69926256\n",
      "Training Step:  7770 Loss:  0.6992384\n",
      "Training Step:  7771 Loss:  0.6992138\n",
      "Training Step:  7772 Loss:  0.6991901\n",
      "Training Step:  7773 Loss:  0.69916576\n",
      "Training Step:  7774 Loss:  0.69914186\n",
      "Training Step:  7775 Loss:  0.6991182\n",
      "Training Step:  7776 Loss:  0.69909346\n",
      "Training Step:  7777 Loss:  0.6990694\n",
      "Training Step:  7778 Loss:  0.6990456\n",
      "Training Step:  7779 Loss:  0.6990212\n",
      "Training Step:  7780 Loss:  0.69899726\n",
      "Training Step:  7781 Loss:  0.6989731\n",
      "Training Step:  7782 Loss:  0.69894886\n",
      "Training Step:  7783 Loss:  0.69892454\n",
      "Training Step:  7784 Loss:  0.6989012\n",
      "Training Step:  7785 Loss:  0.6988768\n",
      "Training Step:  7786 Loss:  0.69885314\n",
      "Training Step:  7787 Loss:  0.698829\n",
      "Training Step:  7788 Loss:  0.69880474\n",
      "Training Step:  7789 Loss:  0.69878054\n",
      "Training Step:  7790 Loss:  0.6987569\n",
      "Training Step:  7791 Loss:  0.6987332\n",
      "Training Step:  7792 Loss:  0.69870937\n",
      "Training Step:  7793 Loss:  0.69868493\n",
      "Training Step:  7794 Loss:  0.69866145\n",
      "Training Step:  7795 Loss:  0.6986368\n",
      "Training Step:  7796 Loss:  0.6986133\n",
      "Training Step:  7797 Loss:  0.6985891\n",
      "Training Step:  7798 Loss:  0.6985655\n",
      "Training Step:  7799 Loss:  0.698542\n",
      "Training Step:  7800 Loss:  0.6985179\n",
      "Training Step:  7801 Loss:  0.6984933\n",
      "Training Step:  7802 Loss:  0.6984699\n",
      "Training Step:  7803 Loss:  0.6984466\n",
      "Training Step:  7804 Loss:  0.6984212\n",
      "Training Step:  7805 Loss:  0.6983981\n",
      "Training Step:  7806 Loss:  0.698374\n",
      "Training Step:  7807 Loss:  0.69835013\n",
      "Training Step:  7808 Loss:  0.6983264\n",
      "Training Step:  7809 Loss:  0.6983027\n",
      "Training Step:  7810 Loss:  0.69827926\n",
      "Training Step:  7811 Loss:  0.69825554\n",
      "Training Step:  7812 Loss:  0.69823205\n",
      "Training Step:  7813 Loss:  0.6982074\n",
      "Training Step:  7814 Loss:  0.69818395\n",
      "Training Step:  7815 Loss:  0.6981599\n",
      "Training Step:  7816 Loss:  0.6981366\n",
      "Training Step:  7817 Loss:  0.6981123\n",
      "Training Step:  7818 Loss:  0.6980884\n",
      "Training Step:  7819 Loss:  0.6980647\n",
      "Training Step:  7820 Loss:  0.69804126\n",
      "Training Step:  7821 Loss:  0.6980173\n",
      "Training Step:  7822 Loss:  0.69799364\n",
      "Training Step:  7823 Loss:  0.6979699\n",
      "Training Step:  7824 Loss:  0.69794637\n",
      "Training Step:  7825 Loss:  0.69792217\n",
      "Training Step:  7826 Loss:  0.6978989\n",
      "Training Step:  7827 Loss:  0.6978745\n",
      "Training Step:  7828 Loss:  0.6978516\n",
      "Training Step:  7829 Loss:  0.69782746\n",
      "Training Step:  7830 Loss:  0.6978043\n",
      "Training Step:  7831 Loss:  0.6977802\n",
      "Training Step:  7832 Loss:  0.6977568\n",
      "Training Step:  7833 Loss:  0.6977333\n",
      "Training Step:  7834 Loss:  0.6977092\n",
      "Training Step:  7835 Loss:  0.69768566\n",
      "Training Step:  7836 Loss:  0.6976619\n",
      "Training Step:  7837 Loss:  0.69763815\n",
      "Training Step:  7838 Loss:  0.69761497\n",
      "Training Step:  7839 Loss:  0.6975912\n",
      "Training Step:  7840 Loss:  0.6975676\n",
      "Training Step:  7841 Loss:  0.69754416\n",
      "Training Step:  7842 Loss:  0.6975206\n",
      "Training Step:  7843 Loss:  0.6974974\n",
      "Training Step:  7844 Loss:  0.697474\n",
      "Training Step:  7845 Loss:  0.6974501\n",
      "Training Step:  7846 Loss:  0.6974268\n",
      "Training Step:  7847 Loss:  0.6974025\n",
      "Training Step:  7848 Loss:  0.6973799\n",
      "Training Step:  7849 Loss:  0.6973557\n",
      "Training Step:  7850 Loss:  0.69733286\n",
      "Training Step:  7851 Loss:  0.69730914\n",
      "Training Step:  7852 Loss:  0.69728535\n",
      "Training Step:  7853 Loss:  0.69726187\n",
      "Training Step:  7854 Loss:  0.69723904\n",
      "Training Step:  7855 Loss:  0.6972151\n",
      "Training Step:  7856 Loss:  0.69719213\n",
      "Training Step:  7857 Loss:  0.6971687\n",
      "Training Step:  7858 Loss:  0.69714475\n",
      "Training Step:  7859 Loss:  0.697121\n",
      "Training Step:  7860 Loss:  0.6970979\n",
      "Training Step:  7861 Loss:  0.69707483\n",
      "Training Step:  7862 Loss:  0.69705105\n",
      "Training Step:  7863 Loss:  0.6970278\n",
      "Training Step:  7864 Loss:  0.69700414\n",
      "Training Step:  7865 Loss:  0.6969813\n",
      "Training Step:  7866 Loss:  0.6969576\n",
      "Training Step:  7867 Loss:  0.6969345\n",
      "Training Step:  7868 Loss:  0.6969107\n",
      "Training Step:  7869 Loss:  0.6968874\n",
      "Training Step:  7870 Loss:  0.6968643\n",
      "Training Step:  7871 Loss:  0.69684047\n",
      "Training Step:  7872 Loss:  0.69681734\n",
      "Training Step:  7873 Loss:  0.69679403\n",
      "Training Step:  7874 Loss:  0.69677097\n",
      "Training Step:  7875 Loss:  0.69674766\n",
      "Training Step:  7876 Loss:  0.69672436\n",
      "Training Step:  7877 Loss:  0.6967013\n",
      "Training Step:  7878 Loss:  0.6966774\n",
      "Training Step:  7879 Loss:  0.69665426\n",
      "Training Step:  7880 Loss:  0.69663095\n",
      "Training Step:  7881 Loss:  0.69660795\n",
      "Training Step:  7882 Loss:  0.6965843\n",
      "Training Step:  7883 Loss:  0.6965618\n",
      "Training Step:  7884 Loss:  0.69653803\n",
      "Training Step:  7885 Loss:  0.6965145\n",
      "Training Step:  7886 Loss:  0.6964914\n",
      "Training Step:  7887 Loss:  0.69646794\n",
      "Training Step:  7888 Loss:  0.6964451\n",
      "Training Step:  7889 Loss:  0.6964221\n",
      "Training Step:  7890 Loss:  0.69639903\n",
      "Training Step:  7891 Loss:  0.6963758\n",
      "Training Step:  7892 Loss:  0.6963523\n",
      "Training Step:  7893 Loss:  0.69632906\n",
      "Training Step:  7894 Loss:  0.69630593\n",
      "Training Step:  7895 Loss:  0.69628316\n",
      "Training Step:  7896 Loss:  0.6962597\n",
      "Training Step:  7897 Loss:  0.69623625\n",
      "Training Step:  7898 Loss:  0.6962136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  7899 Loss:  0.69619036\n",
      "Training Step:  7900 Loss:  0.6961676\n",
      "Training Step:  7901 Loss:  0.6961441\n",
      "Training Step:  7902 Loss:  0.6961206\n",
      "Training Step:  7903 Loss:  0.6960979\n",
      "Training Step:  7904 Loss:  0.6960746\n",
      "Training Step:  7905 Loss:  0.6960522\n",
      "Training Step:  7906 Loss:  0.69602835\n",
      "Training Step:  7907 Loss:  0.696005\n",
      "Training Step:  7908 Loss:  0.6959823\n",
      "Training Step:  7909 Loss:  0.6959596\n",
      "Training Step:  7910 Loss:  0.69593674\n",
      "Training Step:  7911 Loss:  0.695913\n",
      "Training Step:  7912 Loss:  0.6958899\n",
      "Training Step:  7913 Loss:  0.6958674\n",
      "Training Step:  7914 Loss:  0.6958443\n",
      "Training Step:  7915 Loss:  0.69582033\n",
      "Training Step:  7916 Loss:  0.6957979\n",
      "Training Step:  7917 Loss:  0.6957752\n",
      "Training Step:  7918 Loss:  0.6957524\n",
      "Training Step:  7919 Loss:  0.6957288\n",
      "Training Step:  7920 Loss:  0.69570583\n",
      "Training Step:  7921 Loss:  0.6956836\n",
      "Training Step:  7922 Loss:  0.6956608\n",
      "Training Step:  7923 Loss:  0.6956372\n",
      "Training Step:  7924 Loss:  0.6956143\n",
      "Training Step:  7925 Loss:  0.6955914\n",
      "Training Step:  7926 Loss:  0.69556874\n",
      "Training Step:  7927 Loss:  0.69554496\n",
      "Training Step:  7928 Loss:  0.69552255\n",
      "Training Step:  7929 Loss:  0.69550025\n",
      "Training Step:  7930 Loss:  0.6954768\n",
      "Training Step:  7931 Loss:  0.6954539\n",
      "Training Step:  7932 Loss:  0.6954313\n",
      "Training Step:  7933 Loss:  0.69540787\n",
      "Training Step:  7934 Loss:  0.69538534\n",
      "Training Step:  7935 Loss:  0.6953627\n",
      "Training Step:  7936 Loss:  0.69533956\n",
      "Training Step:  7937 Loss:  0.69531626\n",
      "Training Step:  7938 Loss:  0.69529325\n",
      "Training Step:  7939 Loss:  0.69527155\n",
      "Training Step:  7940 Loss:  0.69524777\n",
      "Training Step:  7941 Loss:  0.69522494\n",
      "Training Step:  7942 Loss:  0.69520223\n",
      "Training Step:  7943 Loss:  0.69517964\n",
      "Training Step:  7944 Loss:  0.6951573\n",
      "Training Step:  7945 Loss:  0.69513386\n",
      "Training Step:  7946 Loss:  0.69511116\n",
      "Training Step:  7947 Loss:  0.695089\n",
      "Training Step:  7948 Loss:  0.6950655\n",
      "Training Step:  7949 Loss:  0.6950429\n",
      "Training Step:  7950 Loss:  0.69502014\n",
      "Training Step:  7951 Loss:  0.69499743\n",
      "Training Step:  7952 Loss:  0.6949743\n",
      "Training Step:  7953 Loss:  0.69495136\n",
      "Training Step:  7954 Loss:  0.69492954\n",
      "Training Step:  7955 Loss:  0.6949065\n",
      "Training Step:  7956 Loss:  0.6948841\n",
      "Training Step:  7957 Loss:  0.6948612\n",
      "Training Step:  7958 Loss:  0.69483817\n",
      "Training Step:  7959 Loss:  0.69481647\n",
      "Training Step:  7960 Loss:  0.6947932\n",
      "Training Step:  7961 Loss:  0.6947707\n",
      "Training Step:  7962 Loss:  0.69474804\n",
      "Training Step:  7963 Loss:  0.6947254\n",
      "Training Step:  7964 Loss:  0.69470257\n",
      "Training Step:  7965 Loss:  0.69467986\n",
      "Training Step:  7966 Loss:  0.69465697\n",
      "Training Step:  7967 Loss:  0.6946345\n",
      "Training Step:  7968 Loss:  0.6946116\n",
      "Training Step:  7969 Loss:  0.69458866\n",
      "Training Step:  7970 Loss:  0.6945667\n",
      "Training Step:  7971 Loss:  0.6945435\n",
      "Training Step:  7972 Loss:  0.6945214\n",
      "Training Step:  7973 Loss:  0.6944985\n",
      "Training Step:  7974 Loss:  0.694476\n",
      "Training Step:  7975 Loss:  0.6944537\n",
      "Training Step:  7976 Loss:  0.6944313\n",
      "Training Step:  7977 Loss:  0.6944091\n",
      "Training Step:  7978 Loss:  0.6943857\n",
      "Training Step:  7979 Loss:  0.6943636\n",
      "Training Step:  7980 Loss:  0.69434065\n",
      "Training Step:  7981 Loss:  0.69431794\n",
      "Training Step:  7982 Loss:  0.6942963\n",
      "Training Step:  7983 Loss:  0.6942731\n",
      "Training Step:  7984 Loss:  0.6942513\n",
      "Training Step:  7985 Loss:  0.69422793\n",
      "Training Step:  7986 Loss:  0.6942064\n",
      "Training Step:  7987 Loss:  0.69418293\n",
      "Training Step:  7988 Loss:  0.69416046\n",
      "Training Step:  7989 Loss:  0.6941376\n",
      "Training Step:  7990 Loss:  0.6941164\n",
      "Training Step:  7991 Loss:  0.6940932\n",
      "Training Step:  7992 Loss:  0.69407064\n",
      "Training Step:  7993 Loss:  0.69404894\n",
      "Training Step:  7994 Loss:  0.694026\n",
      "Training Step:  7995 Loss:  0.6940034\n",
      "Training Step:  7996 Loss:  0.693981\n",
      "Training Step:  7997 Loss:  0.69395965\n",
      "Training Step:  7998 Loss:  0.69393647\n",
      "Training Step:  7999 Loss:  0.6939138\n",
      "Training Step:  8000 Loss:  0.6938915\n",
      "Training Step:  8001 Loss:  0.69386935\n",
      "Training Step:  8002 Loss:  0.6938463\n",
      "Training Step:  8003 Loss:  0.69382405\n",
      "Training Step:  8004 Loss:  0.6938013\n",
      "Training Step:  8005 Loss:  0.69377965\n",
      "Training Step:  8006 Loss:  0.6937567\n",
      "Training Step:  8007 Loss:  0.6937346\n",
      "Training Step:  8008 Loss:  0.6937122\n",
      "Training Step:  8009 Loss:  0.69369036\n",
      "Training Step:  8010 Loss:  0.69366795\n",
      "Training Step:  8011 Loss:  0.6936454\n",
      "Training Step:  8012 Loss:  0.6936229\n",
      "Training Step:  8013 Loss:  0.6936006\n",
      "Training Step:  8014 Loss:  0.6935788\n",
      "Training Step:  8015 Loss:  0.6935563\n",
      "Training Step:  8016 Loss:  0.6935336\n",
      "Training Step:  8017 Loss:  0.6935121\n",
      "Training Step:  8018 Loss:  0.6934893\n",
      "Training Step:  8019 Loss:  0.6934671\n",
      "Training Step:  8020 Loss:  0.6934447\n",
      "Training Step:  8021 Loss:  0.69342226\n",
      "Training Step:  8022 Loss:  0.6934007\n",
      "Training Step:  8023 Loss:  0.69337773\n",
      "Training Step:  8024 Loss:  0.69335586\n",
      "Training Step:  8025 Loss:  0.6933333\n",
      "Training Step:  8026 Loss:  0.69331187\n",
      "Training Step:  8027 Loss:  0.69328886\n",
      "Training Step:  8028 Loss:  0.69326687\n",
      "Training Step:  8029 Loss:  0.6932448\n",
      "Training Step:  8030 Loss:  0.6932219\n",
      "Training Step:  8031 Loss:  0.6932009\n",
      "Training Step:  8032 Loss:  0.69317824\n",
      "Training Step:  8033 Loss:  0.69315606\n",
      "Training Step:  8034 Loss:  0.6931336\n",
      "Training Step:  8035 Loss:  0.69311124\n",
      "Training Step:  8036 Loss:  0.69308954\n",
      "Training Step:  8037 Loss:  0.6930673\n",
      "Training Step:  8038 Loss:  0.6930455\n",
      "Training Step:  8039 Loss:  0.693023\n",
      "Training Step:  8040 Loss:  0.6930008\n",
      "Training Step:  8041 Loss:  0.69297856\n",
      "Training Step:  8042 Loss:  0.6929566\n",
      "Training Step:  8043 Loss:  0.6929345\n",
      "Training Step:  8044 Loss:  0.69291204\n",
      "Training Step:  8045 Loss:  0.6928898\n",
      "Training Step:  8046 Loss:  0.6928687\n",
      "Training Step:  8047 Loss:  0.69284624\n",
      "Training Step:  8048 Loss:  0.69282454\n",
      "Training Step:  8049 Loss:  0.6928022\n",
      "Training Step:  8050 Loss:  0.6927799\n",
      "Training Step:  8051 Loss:  0.69275844\n",
      "Training Step:  8052 Loss:  0.692736\n",
      "Training Step:  8053 Loss:  0.6927139\n",
      "Training Step:  8054 Loss:  0.6926917\n",
      "Training Step:  8055 Loss:  0.6926694\n",
      "Training Step:  8056 Loss:  0.6926477\n",
      "Training Step:  8057 Loss:  0.6926263\n",
      "Training Step:  8058 Loss:  0.6926036\n",
      "Training Step:  8059 Loss:  0.69258124\n",
      "Training Step:  8060 Loss:  0.69256\n",
      "Training Step:  8061 Loss:  0.692537\n",
      "Training Step:  8062 Loss:  0.692516\n",
      "Training Step:  8063 Loss:  0.6924943\n",
      "Training Step:  8064 Loss:  0.6924716\n",
      "Training Step:  8065 Loss:  0.69244975\n",
      "Training Step:  8066 Loss:  0.6924276\n",
      "Training Step:  8067 Loss:  0.6924051\n",
      "Training Step:  8068 Loss:  0.69238424\n",
      "Training Step:  8069 Loss:  0.6923625\n",
      "Training Step:  8070 Loss:  0.69234014\n",
      "Training Step:  8071 Loss:  0.6923182\n",
      "Training Step:  8072 Loss:  0.692296\n",
      "Training Step:  8073 Loss:  0.6922742\n",
      "Training Step:  8074 Loss:  0.6922523\n",
      "Training Step:  8075 Loss:  0.69223064\n",
      "Training Step:  8076 Loss:  0.6922083\n",
      "Training Step:  8077 Loss:  0.6921865\n",
      "Training Step:  8078 Loss:  0.6921649\n",
      "Training Step:  8079 Loss:  0.692143\n",
      "Training Step:  8080 Loss:  0.69212043\n",
      "Training Step:  8081 Loss:  0.6920996\n",
      "Training Step:  8082 Loss:  0.69207776\n",
      "Training Step:  8083 Loss:  0.69205576\n",
      "Training Step:  8084 Loss:  0.69203377\n",
      "Training Step:  8085 Loss:  0.69201195\n",
      "Training Step:  8086 Loss:  0.69198966\n",
      "Training Step:  8087 Loss:  0.6919678\n",
      "Training Step:  8088 Loss:  0.6919471\n",
      "Training Step:  8089 Loss:  0.6919246\n",
      "Training Step:  8090 Loss:  0.6919027\n",
      "Training Step:  8091 Loss:  0.69188076\n",
      "Training Step:  8092 Loss:  0.691859\n",
      "Training Step:  8093 Loss:  0.6918373\n",
      "Training Step:  8094 Loss:  0.6918157\n",
      "Training Step:  8095 Loss:  0.6917932\n",
      "Training Step:  8096 Loss:  0.6917725\n",
      "Training Step:  8097 Loss:  0.69175035\n",
      "Training Step:  8098 Loss:  0.69172806\n",
      "Training Step:  8099 Loss:  0.69170684\n",
      "Training Step:  8100 Loss:  0.69168484\n",
      "Training Step:  8101 Loss:  0.6916635\n",
      "Training Step:  8102 Loss:  0.6916419\n",
      "Training Step:  8103 Loss:  0.69161975\n",
      "Training Step:  8104 Loss:  0.6915981\n",
      "Training Step:  8105 Loss:  0.6915773\n",
      "Training Step:  8106 Loss:  0.6915549\n",
      "Training Step:  8107 Loss:  0.69153285\n",
      "Training Step:  8108 Loss:  0.69151163\n",
      "Training Step:  8109 Loss:  0.6914899\n",
      "Training Step:  8110 Loss:  0.6914677\n",
      "Training Step:  8111 Loss:  0.6914464\n",
      "Training Step:  8112 Loss:  0.69142485\n",
      "Training Step:  8113 Loss:  0.69140273\n",
      "Training Step:  8114 Loss:  0.6913814\n",
      "Training Step:  8115 Loss:  0.6913602\n",
      "Training Step:  8116 Loss:  0.69133866\n",
      "Training Step:  8117 Loss:  0.6913167\n",
      "Training Step:  8118 Loss:  0.6912948\n",
      "Training Step:  8119 Loss:  0.6912735\n",
      "Training Step:  8120 Loss:  0.6912515\n",
      "Training Step:  8121 Loss:  0.69123006\n",
      "Training Step:  8122 Loss:  0.6912084\n",
      "Training Step:  8123 Loss:  0.6911869\n",
      "Training Step:  8124 Loss:  0.6911653\n",
      "Training Step:  8125 Loss:  0.6911439\n",
      "Training Step:  8126 Loss:  0.69112265\n",
      "Training Step:  8127 Loss:  0.69110125\n",
      "Training Step:  8128 Loss:  0.6910789\n",
      "Training Step:  8129 Loss:  0.69105715\n",
      "Training Step:  8130 Loss:  0.69103557\n",
      "Training Step:  8131 Loss:  0.6910141\n",
      "Training Step:  8132 Loss:  0.6909931\n",
      "Training Step:  8133 Loss:  0.69097114\n",
      "Training Step:  8134 Loss:  0.6909498\n",
      "Training Step:  8135 Loss:  0.6909281\n",
      "Training Step:  8136 Loss:  0.6909064\n",
      "Training Step:  8137 Loss:  0.69088525\n",
      "Training Step:  8138 Loss:  0.6908636\n",
      "Training Step:  8139 Loss:  0.69084245\n",
      "Training Step:  8140 Loss:  0.6908209\n",
      "Training Step:  8141 Loss:  0.69079983\n",
      "Training Step:  8142 Loss:  0.69077754\n",
      "Training Step:  8143 Loss:  0.69075626\n",
      "Training Step:  8144 Loss:  0.6907344\n",
      "Training Step:  8145 Loss:  0.69071317\n",
      "Training Step:  8146 Loss:  0.69069153\n",
      "Training Step:  8147 Loss:  0.69067055\n",
      "Training Step:  8148 Loss:  0.6906494\n",
      "Training Step:  8149 Loss:  0.6906278\n",
      "Training Step:  8150 Loss:  0.6906059\n",
      "Training Step:  8151 Loss:  0.6905844\n",
      "Training Step:  8152 Loss:  0.69056284\n",
      "Training Step:  8153 Loss:  0.6905415\n",
      "Training Step:  8154 Loss:  0.6905205\n",
      "Training Step:  8155 Loss:  0.6904989\n",
      "Training Step:  8156 Loss:  0.6904777\n",
      "Training Step:  8157 Loss:  0.6904565\n",
      "Training Step:  8158 Loss:  0.6904352\n",
      "Training Step:  8159 Loss:  0.69041383\n",
      "Training Step:  8160 Loss:  0.6903926\n",
      "Training Step:  8161 Loss:  0.6903715\n",
      "Training Step:  8162 Loss:  0.6903498\n",
      "Training Step:  8163 Loss:  0.6903281\n",
      "Training Step:  8164 Loss:  0.6903068\n",
      "Training Step:  8165 Loss:  0.6902859\n",
      "Training Step:  8166 Loss:  0.69026387\n",
      "Training Step:  8167 Loss:  0.6902425\n",
      "Training Step:  8168 Loss:  0.6902214\n",
      "Training Step:  8169 Loss:  0.69019985\n",
      "Training Step:  8170 Loss:  0.690179\n",
      "Training Step:  8171 Loss:  0.69015765\n",
      "Training Step:  8172 Loss:  0.6901362\n",
      "Training Step:  8173 Loss:  0.6901151\n",
      "Training Step:  8174 Loss:  0.6900937\n",
      "Training Step:  8175 Loss:  0.6900727\n",
      "Training Step:  8176 Loss:  0.6900515\n",
      "Training Step:  8177 Loss:  0.6900304\n",
      "Training Step:  8178 Loss:  0.69000876\n",
      "Training Step:  8179 Loss:  0.68998784\n",
      "Training Step:  8180 Loss:  0.6899663\n",
      "Training Step:  8181 Loss:  0.68994534\n",
      "Training Step:  8182 Loss:  0.6899237\n",
      "Training Step:  8183 Loss:  0.68990266\n",
      "Training Step:  8184 Loss:  0.68988127\n",
      "Training Step:  8185 Loss:  0.68986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  8186 Loss:  0.6898384\n",
      "Training Step:  8187 Loss:  0.6898175\n",
      "Training Step:  8188 Loss:  0.68979657\n",
      "Training Step:  8189 Loss:  0.6897752\n",
      "Training Step:  8190 Loss:  0.6897538\n",
      "Training Step:  8191 Loss:  0.68973297\n",
      "Training Step:  8192 Loss:  0.6897117\n",
      "Training Step:  8193 Loss:  0.6896908\n",
      "Training Step:  8194 Loss:  0.6896699\n",
      "Training Step:  8195 Loss:  0.68964845\n",
      "Training Step:  8196 Loss:  0.6896275\n",
      "Training Step:  8197 Loss:  0.68960625\n",
      "Training Step:  8198 Loss:  0.6895849\n",
      "Training Step:  8199 Loss:  0.68956375\n",
      "Training Step:  8200 Loss:  0.68954253\n",
      "Training Step:  8201 Loss:  0.6895216\n",
      "Training Step:  8202 Loss:  0.68950063\n",
      "Training Step:  8203 Loss:  0.6894794\n",
      "Training Step:  8204 Loss:  0.6894582\n",
      "Training Step:  8205 Loss:  0.6894374\n",
      "Training Step:  8206 Loss:  0.68941605\n",
      "Training Step:  8207 Loss:  0.6893948\n",
      "Training Step:  8208 Loss:  0.68937415\n",
      "Training Step:  8209 Loss:  0.68935305\n",
      "Training Step:  8210 Loss:  0.68933153\n",
      "Training Step:  8211 Loss:  0.6893106\n",
      "Training Step:  8212 Loss:  0.68929005\n",
      "Training Step:  8213 Loss:  0.68926895\n",
      "Training Step:  8214 Loss:  0.68924767\n",
      "Training Step:  8215 Loss:  0.6892264\n",
      "Training Step:  8216 Loss:  0.68920565\n",
      "Training Step:  8217 Loss:  0.68918484\n",
      "Training Step:  8218 Loss:  0.68916345\n",
      "Training Step:  8219 Loss:  0.68914306\n",
      "Training Step:  8220 Loss:  0.6891221\n",
      "Training Step:  8221 Loss:  0.6891006\n",
      "Training Step:  8222 Loss:  0.6890795\n",
      "Training Step:  8223 Loss:  0.68905777\n",
      "Training Step:  8224 Loss:  0.6890373\n",
      "Training Step:  8225 Loss:  0.68901646\n",
      "Training Step:  8226 Loss:  0.68899506\n",
      "Training Step:  8227 Loss:  0.68897384\n",
      "Training Step:  8228 Loss:  0.68895334\n",
      "Training Step:  8229 Loss:  0.68893236\n",
      "Training Step:  8230 Loss:  0.6889117\n",
      "Training Step:  8231 Loss:  0.68889105\n",
      "Training Step:  8232 Loss:  0.68886995\n",
      "Training Step:  8233 Loss:  0.6888491\n",
      "Training Step:  8234 Loss:  0.68882805\n",
      "Training Step:  8235 Loss:  0.68880683\n",
      "Training Step:  8236 Loss:  0.68878573\n",
      "Training Step:  8237 Loss:  0.68876547\n",
      "Training Step:  8238 Loss:  0.68874466\n",
      "Training Step:  8239 Loss:  0.6887238\n",
      "Training Step:  8240 Loss:  0.6887025\n",
      "Training Step:  8241 Loss:  0.6886815\n",
      "Training Step:  8242 Loss:  0.6886604\n",
      "Training Step:  8243 Loss:  0.68863976\n",
      "Training Step:  8244 Loss:  0.68861884\n",
      "Training Step:  8245 Loss:  0.6885986\n",
      "Training Step:  8246 Loss:  0.688577\n",
      "Training Step:  8247 Loss:  0.6885564\n",
      "Training Step:  8248 Loss:  0.68853545\n",
      "Training Step:  8249 Loss:  0.6885146\n",
      "Training Step:  8250 Loss:  0.68849367\n",
      "Training Step:  8251 Loss:  0.6884735\n",
      "Training Step:  8252 Loss:  0.6884531\n",
      "Training Step:  8253 Loss:  0.6884321\n",
      "Training Step:  8254 Loss:  0.6884105\n",
      "Training Step:  8255 Loss:  0.6883895\n",
      "Training Step:  8256 Loss:  0.68836915\n",
      "Training Step:  8257 Loss:  0.6883486\n",
      "Training Step:  8258 Loss:  0.68832755\n",
      "Training Step:  8259 Loss:  0.688307\n",
      "Training Step:  8260 Loss:  0.68828577\n",
      "Training Step:  8261 Loss:  0.6882652\n",
      "Training Step:  8262 Loss:  0.68824416\n",
      "Training Step:  8263 Loss:  0.688224\n",
      "Training Step:  8264 Loss:  0.6882032\n",
      "Training Step:  8265 Loss:  0.6881816\n",
      "Training Step:  8266 Loss:  0.6881613\n",
      "Training Step:  8267 Loss:  0.68814075\n",
      "Training Step:  8268 Loss:  0.6881199\n",
      "Training Step:  8269 Loss:  0.68809885\n",
      "Training Step:  8270 Loss:  0.6880788\n",
      "Training Step:  8271 Loss:  0.68805784\n",
      "Training Step:  8272 Loss:  0.68803746\n",
      "Training Step:  8273 Loss:  0.68801653\n",
      "Training Step:  8274 Loss:  0.6879961\n",
      "Training Step:  8275 Loss:  0.687975\n",
      "Training Step:  8276 Loss:  0.6879541\n",
      "Training Step:  8277 Loss:  0.6879338\n",
      "Training Step:  8278 Loss:  0.68791306\n",
      "Training Step:  8279 Loss:  0.68789256\n",
      "Training Step:  8280 Loss:  0.6878716\n",
      "Training Step:  8281 Loss:  0.6878511\n",
      "Training Step:  8282 Loss:  0.68783027\n",
      "Training Step:  8283 Loss:  0.68781006\n",
      "Training Step:  8284 Loss:  0.6877887\n",
      "Training Step:  8285 Loss:  0.68776816\n",
      "Training Step:  8286 Loss:  0.68774784\n",
      "Training Step:  8287 Loss:  0.68772745\n",
      "Training Step:  8288 Loss:  0.68770653\n",
      "Training Step:  8289 Loss:  0.68768597\n",
      "Training Step:  8290 Loss:  0.68766534\n",
      "Training Step:  8291 Loss:  0.68764466\n",
      "Training Step:  8292 Loss:  0.68762386\n",
      "Training Step:  8293 Loss:  0.6876036\n",
      "Training Step:  8294 Loss:  0.68758243\n",
      "Training Step:  8295 Loss:  0.6875627\n",
      "Training Step:  8296 Loss:  0.68754214\n",
      "Training Step:  8297 Loss:  0.68752146\n",
      "Training Step:  8298 Loss:  0.68750113\n",
      "Training Step:  8299 Loss:  0.6874799\n",
      "Training Step:  8300 Loss:  0.68745947\n",
      "Training Step:  8301 Loss:  0.6874392\n",
      "Training Step:  8302 Loss:  0.6874187\n",
      "Training Step:  8303 Loss:  0.6873981\n",
      "Training Step:  8304 Loss:  0.68737775\n",
      "Training Step:  8305 Loss:  0.687357\n",
      "Training Step:  8306 Loss:  0.68733585\n",
      "Training Step:  8307 Loss:  0.6873159\n",
      "Training Step:  8308 Loss:  0.68729514\n",
      "Training Step:  8309 Loss:  0.6872753\n",
      "Training Step:  8310 Loss:  0.6872545\n",
      "Training Step:  8311 Loss:  0.68723416\n",
      "Training Step:  8312 Loss:  0.6872138\n",
      "Training Step:  8313 Loss:  0.6871926\n",
      "Training Step:  8314 Loss:  0.68717253\n",
      "Training Step:  8315 Loss:  0.68715256\n",
      "Training Step:  8316 Loss:  0.6871314\n",
      "Training Step:  8317 Loss:  0.6871118\n",
      "Training Step:  8318 Loss:  0.6870908\n",
      "Training Step:  8319 Loss:  0.68707025\n",
      "Training Step:  8320 Loss:  0.68704927\n",
      "Training Step:  8321 Loss:  0.6870297\n",
      "Training Step:  8322 Loss:  0.68700856\n",
      "Training Step:  8323 Loss:  0.6869888\n",
      "Training Step:  8324 Loss:  0.6869681\n",
      "Training Step:  8325 Loss:  0.6869474\n",
      "Training Step:  8326 Loss:  0.686927\n",
      "Training Step:  8327 Loss:  0.68690664\n",
      "Training Step:  8328 Loss:  0.6868861\n",
      "Training Step:  8329 Loss:  0.68686604\n",
      "Training Step:  8330 Loss:  0.686846\n",
      "Training Step:  8331 Loss:  0.6868252\n",
      "Training Step:  8332 Loss:  0.68680483\n",
      "Training Step:  8333 Loss:  0.68678457\n",
      "Training Step:  8334 Loss:  0.6867638\n",
      "Training Step:  8335 Loss:  0.68674403\n",
      "Training Step:  8336 Loss:  0.6867227\n",
      "Training Step:  8337 Loss:  0.68670267\n",
      "Training Step:  8338 Loss:  0.68668264\n",
      "Training Step:  8339 Loss:  0.68666196\n",
      "Training Step:  8340 Loss:  0.6866425\n",
      "Training Step:  8341 Loss:  0.6866216\n",
      "Training Step:  8342 Loss:  0.686601\n",
      "Training Step:  8343 Loss:  0.6865806\n",
      "Training Step:  8344 Loss:  0.6865603\n",
      "Training Step:  8345 Loss:  0.6865407\n",
      "Training Step:  8346 Loss:  0.68652004\n",
      "Training Step:  8347 Loss:  0.6864995\n",
      "Training Step:  8348 Loss:  0.6864791\n",
      "Training Step:  8349 Loss:  0.6864587\n",
      "Training Step:  8350 Loss:  0.6864388\n",
      "Training Step:  8351 Loss:  0.68641907\n",
      "Training Step:  8352 Loss:  0.68639815\n",
      "Training Step:  8353 Loss:  0.6863782\n",
      "Training Step:  8354 Loss:  0.6863576\n",
      "Training Step:  8355 Loss:  0.68633807\n",
      "Training Step:  8356 Loss:  0.6863171\n",
      "Training Step:  8357 Loss:  0.6862966\n",
      "Training Step:  8358 Loss:  0.6862768\n",
      "Training Step:  8359 Loss:  0.68625647\n",
      "Training Step:  8360 Loss:  0.6862365\n",
      "Training Step:  8361 Loss:  0.6862159\n",
      "Training Step:  8362 Loss:  0.6861955\n",
      "Training Step:  8363 Loss:  0.6861755\n",
      "Training Step:  8364 Loss:  0.6861552\n",
      "Training Step:  8365 Loss:  0.68613523\n",
      "Training Step:  8366 Loss:  0.6861149\n",
      "Training Step:  8367 Loss:  0.6860946\n",
      "Training Step:  8368 Loss:  0.68607426\n",
      "Training Step:  8369 Loss:  0.68605494\n",
      "Training Step:  8370 Loss:  0.6860342\n",
      "Training Step:  8371 Loss:  0.686014\n",
      "Training Step:  8372 Loss:  0.6859938\n",
      "Training Step:  8373 Loss:  0.68597376\n",
      "Training Step:  8374 Loss:  0.6859538\n",
      "Training Step:  8375 Loss:  0.6859335\n",
      "Training Step:  8376 Loss:  0.6859133\n",
      "Training Step:  8377 Loss:  0.6858934\n",
      "Training Step:  8378 Loss:  0.68587375\n",
      "Training Step:  8379 Loss:  0.6858522\n",
      "Training Step:  8380 Loss:  0.6858323\n",
      "Training Step:  8381 Loss:  0.68581265\n",
      "Training Step:  8382 Loss:  0.68579304\n",
      "Training Step:  8383 Loss:  0.6857722\n",
      "Training Step:  8384 Loss:  0.68575203\n",
      "Training Step:  8385 Loss:  0.68573207\n",
      "Training Step:  8386 Loss:  0.68571216\n",
      "Training Step:  8387 Loss:  0.6856915\n",
      "Training Step:  8388 Loss:  0.68567216\n",
      "Training Step:  8389 Loss:  0.6856519\n",
      "Training Step:  8390 Loss:  0.68563193\n",
      "Training Step:  8391 Loss:  0.68561155\n",
      "Training Step:  8392 Loss:  0.68559206\n",
      "Training Step:  8393 Loss:  0.68557155\n",
      "Training Step:  8394 Loss:  0.6855517\n",
      "Training Step:  8395 Loss:  0.68553126\n",
      "Training Step:  8396 Loss:  0.68551177\n",
      "Training Step:  8397 Loss:  0.6854913\n",
      "Training Step:  8398 Loss:  0.68547124\n",
      "Training Step:  8399 Loss:  0.6854507\n",
      "Training Step:  8400 Loss:  0.6854311\n",
      "Training Step:  8401 Loss:  0.68541145\n",
      "Training Step:  8402 Loss:  0.6853916\n",
      "Training Step:  8403 Loss:  0.6853711\n",
      "Training Step:  8404 Loss:  0.6853514\n",
      "Training Step:  8405 Loss:  0.6853313\n",
      "Training Step:  8406 Loss:  0.68531173\n",
      "Training Step:  8407 Loss:  0.685291\n",
      "Training Step:  8408 Loss:  0.6852713\n",
      "Training Step:  8409 Loss:  0.6852515\n",
      "Training Step:  8410 Loss:  0.6852308\n",
      "Training Step:  8411 Loss:  0.6852113\n",
      "Training Step:  8412 Loss:  0.6851915\n",
      "Training Step:  8413 Loss:  0.68517125\n",
      "Training Step:  8414 Loss:  0.6851512\n",
      "Training Step:  8415 Loss:  0.68513185\n",
      "Training Step:  8416 Loss:  0.68511134\n",
      "Training Step:  8417 Loss:  0.68509185\n",
      "Training Step:  8418 Loss:  0.6850718\n",
      "Training Step:  8419 Loss:  0.68505156\n",
      "Training Step:  8420 Loss:  0.6850321\n",
      "Training Step:  8421 Loss:  0.68501145\n",
      "Training Step:  8422 Loss:  0.68499213\n",
      "Training Step:  8423 Loss:  0.6849723\n",
      "Training Step:  8424 Loss:  0.68495214\n",
      "Training Step:  8425 Loss:  0.6849323\n",
      "Training Step:  8426 Loss:  0.6849123\n",
      "Training Step:  8427 Loss:  0.68489265\n",
      "Training Step:  8428 Loss:  0.6848722\n",
      "Training Step:  8429 Loss:  0.68485296\n",
      "Training Step:  8430 Loss:  0.68483275\n",
      "Training Step:  8431 Loss:  0.6848124\n",
      "Training Step:  8432 Loss:  0.68479323\n",
      "Training Step:  8433 Loss:  0.6847738\n",
      "Training Step:  8434 Loss:  0.68475366\n",
      "Training Step:  8435 Loss:  0.68473345\n",
      "Training Step:  8436 Loss:  0.6847137\n",
      "Training Step:  8437 Loss:  0.68469405\n",
      "Training Step:  8438 Loss:  0.68467426\n",
      "Training Step:  8439 Loss:  0.6846546\n",
      "Training Step:  8440 Loss:  0.6846348\n",
      "Training Step:  8441 Loss:  0.6846148\n",
      "Training Step:  8442 Loss:  0.6845953\n",
      "Training Step:  8443 Loss:  0.68457484\n",
      "Training Step:  8444 Loss:  0.684555\n",
      "Training Step:  8445 Loss:  0.6845354\n",
      "Training Step:  8446 Loss:  0.6845154\n",
      "Training Step:  8447 Loss:  0.6844961\n",
      "Training Step:  8448 Loss:  0.684476\n",
      "Training Step:  8449 Loss:  0.68445593\n",
      "Training Step:  8450 Loss:  0.684437\n",
      "Training Step:  8451 Loss:  0.6844165\n",
      "Training Step:  8452 Loss:  0.6843966\n",
      "Training Step:  8453 Loss:  0.68437725\n",
      "Training Step:  8454 Loss:  0.6843569\n",
      "Training Step:  8455 Loss:  0.68433774\n",
      "Training Step:  8456 Loss:  0.68431836\n",
      "Training Step:  8457 Loss:  0.68429816\n",
      "Training Step:  8458 Loss:  0.6842785\n",
      "Training Step:  8459 Loss:  0.6842594\n",
      "Training Step:  8460 Loss:  0.6842389\n",
      "Training Step:  8461 Loss:  0.684219\n",
      "Training Step:  8462 Loss:  0.6841998\n",
      "Training Step:  8463 Loss:  0.68417996\n",
      "Training Step:  8464 Loss:  0.6841601\n",
      "Training Step:  8465 Loss:  0.6841409\n",
      "Training Step:  8466 Loss:  0.6841205\n",
      "Training Step:  8467 Loss:  0.68410087\n",
      "Training Step:  8468 Loss:  0.68408144\n",
      "Training Step:  8469 Loss:  0.684061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  8470 Loss:  0.684042\n",
      "Training Step:  8471 Loss:  0.68402314\n",
      "Training Step:  8472 Loss:  0.68400216\n",
      "Training Step:  8473 Loss:  0.68398273\n",
      "Training Step:  8474 Loss:  0.6839639\n",
      "Training Step:  8475 Loss:  0.68394345\n",
      "Training Step:  8476 Loss:  0.6839236\n",
      "Training Step:  8477 Loss:  0.6839047\n",
      "Training Step:  8478 Loss:  0.6838851\n",
      "Training Step:  8479 Loss:  0.68386525\n",
      "Training Step:  8480 Loss:  0.68384534\n",
      "Training Step:  8481 Loss:  0.6838261\n",
      "Training Step:  8482 Loss:  0.6838061\n",
      "Training Step:  8483 Loss:  0.68378603\n",
      "Training Step:  8484 Loss:  0.68376684\n",
      "Training Step:  8485 Loss:  0.68374765\n",
      "Training Step:  8486 Loss:  0.6837272\n",
      "Training Step:  8487 Loss:  0.6837085\n",
      "Training Step:  8488 Loss:  0.68368894\n",
      "Training Step:  8489 Loss:  0.683669\n",
      "Training Step:  8490 Loss:  0.6836492\n",
      "Training Step:  8491 Loss:  0.68362963\n",
      "Training Step:  8492 Loss:  0.6836103\n",
      "Training Step:  8493 Loss:  0.6835906\n",
      "Training Step:  8494 Loss:  0.68357086\n",
      "Training Step:  8495 Loss:  0.6835516\n",
      "Training Step:  8496 Loss:  0.683532\n",
      "Training Step:  8497 Loss:  0.68351203\n",
      "Training Step:  8498 Loss:  0.6834924\n",
      "Training Step:  8499 Loss:  0.6834739\n",
      "Training Step:  8500 Loss:  0.6834537\n",
      "Training Step:  8501 Loss:  0.68343437\n",
      "Training Step:  8502 Loss:  0.68341434\n",
      "Training Step:  8503 Loss:  0.683395\n",
      "Training Step:  8504 Loss:  0.68337536\n",
      "Training Step:  8505 Loss:  0.6833561\n",
      "Training Step:  8506 Loss:  0.6833371\n",
      "Training Step:  8507 Loss:  0.683317\n",
      "Training Step:  8508 Loss:  0.68329793\n",
      "Training Step:  8509 Loss:  0.683278\n",
      "Training Step:  8510 Loss:  0.6832586\n",
      "Training Step:  8511 Loss:  0.6832389\n",
      "Training Step:  8512 Loss:  0.6832204\n",
      "Training Step:  8513 Loss:  0.6832001\n",
      "Training Step:  8514 Loss:  0.6831807\n",
      "Training Step:  8515 Loss:  0.683161\n",
      "Training Step:  8516 Loss:  0.6831416\n",
      "Training Step:  8517 Loss:  0.6831229\n",
      "Training Step:  8518 Loss:  0.6831027\n",
      "Training Step:  8519 Loss:  0.6830838\n",
      "Training Step:  8520 Loss:  0.68306386\n",
      "Training Step:  8521 Loss:  0.6830449\n",
      "Training Step:  8522 Loss:  0.6830257\n",
      "Training Step:  8523 Loss:  0.6830054\n",
      "Training Step:  8524 Loss:  0.6829864\n",
      "Training Step:  8525 Loss:  0.6829667\n",
      "Training Step:  8526 Loss:  0.6829475\n",
      "Training Step:  8527 Loss:  0.68292844\n",
      "Training Step:  8528 Loss:  0.6829084\n",
      "Training Step:  8529 Loss:  0.68288964\n",
      "Training Step:  8530 Loss:  0.6828695\n",
      "Training Step:  8531 Loss:  0.6828502\n",
      "Training Step:  8532 Loss:  0.6828313\n",
      "Training Step:  8533 Loss:  0.6828113\n",
      "Training Step:  8534 Loss:  0.68279254\n",
      "Training Step:  8535 Loss:  0.6827724\n",
      "Training Step:  8536 Loss:  0.68275344\n",
      "Training Step:  8537 Loss:  0.6827337\n",
      "Training Step:  8538 Loss:  0.6827144\n",
      "Training Step:  8539 Loss:  0.6826954\n",
      "Training Step:  8540 Loss:  0.6826762\n",
      "Training Step:  8541 Loss:  0.68265694\n",
      "Training Step:  8542 Loss:  0.6826376\n",
      "Training Step:  8543 Loss:  0.68261814\n",
      "Training Step:  8544 Loss:  0.68259865\n",
      "Training Step:  8545 Loss:  0.6825798\n",
      "Training Step:  8546 Loss:  0.68256086\n",
      "Training Step:  8547 Loss:  0.6825407\n",
      "Training Step:  8548 Loss:  0.68252164\n",
      "Training Step:  8549 Loss:  0.682502\n",
      "Training Step:  8550 Loss:  0.6824827\n",
      "Training Step:  8551 Loss:  0.68246275\n",
      "Training Step:  8552 Loss:  0.68244404\n",
      "Training Step:  8553 Loss:  0.6824253\n",
      "Training Step:  8554 Loss:  0.6824052\n",
      "Training Step:  8555 Loss:  0.6823869\n",
      "Training Step:  8556 Loss:  0.682367\n",
      "Training Step:  8557 Loss:  0.6823475\n",
      "Training Step:  8558 Loss:  0.68232834\n",
      "Training Step:  8559 Loss:  0.6823089\n",
      "Training Step:  8560 Loss:  0.6822902\n",
      "Training Step:  8561 Loss:  0.6822711\n",
      "Training Step:  8562 Loss:  0.68225163\n",
      "Training Step:  8563 Loss:  0.6822324\n",
      "Training Step:  8564 Loss:  0.68221354\n",
      "Training Step:  8565 Loss:  0.6821932\n",
      "Training Step:  8566 Loss:  0.682175\n",
      "Training Step:  8567 Loss:  0.68215525\n",
      "Training Step:  8568 Loss:  0.6821362\n",
      "Training Step:  8569 Loss:  0.6821167\n",
      "Training Step:  8570 Loss:  0.6820974\n",
      "Training Step:  8571 Loss:  0.6820787\n",
      "Training Step:  8572 Loss:  0.68205917\n",
      "Training Step:  8573 Loss:  0.6820398\n",
      "Training Step:  8574 Loss:  0.68202126\n",
      "Training Step:  8575 Loss:  0.6820015\n",
      "Training Step:  8576 Loss:  0.6819829\n",
      "Training Step:  8577 Loss:  0.68196326\n",
      "Training Step:  8578 Loss:  0.6819438\n",
      "Training Step:  8579 Loss:  0.68192506\n",
      "Training Step:  8580 Loss:  0.6819052\n",
      "Training Step:  8581 Loss:  0.6818865\n",
      "Training Step:  8582 Loss:  0.68186665\n",
      "Training Step:  8583 Loss:  0.6818487\n",
      "Training Step:  8584 Loss:  0.68182915\n",
      "Training Step:  8585 Loss:  0.68181\n",
      "Training Step:  8586 Loss:  0.6817912\n",
      "Training Step:  8587 Loss:  0.6817718\n",
      "Training Step:  8588 Loss:  0.6817529\n",
      "Training Step:  8589 Loss:  0.6817338\n",
      "Training Step:  8590 Loss:  0.6817149\n",
      "Training Step:  8591 Loss:  0.6816952\n",
      "Training Step:  8592 Loss:  0.68167675\n",
      "Training Step:  8593 Loss:  0.68165684\n",
      "Training Step:  8594 Loss:  0.68163824\n",
      "Training Step:  8595 Loss:  0.6816186\n",
      "Training Step:  8596 Loss:  0.6815994\n",
      "Training Step:  8597 Loss:  0.6815803\n",
      "Training Step:  8598 Loss:  0.6815609\n",
      "Training Step:  8599 Loss:  0.6815422\n",
      "Training Step:  8600 Loss:  0.6815237\n",
      "Training Step:  8601 Loss:  0.6815042\n",
      "Training Step:  8602 Loss:  0.68148464\n",
      "Training Step:  8603 Loss:  0.6814655\n",
      "Training Step:  8604 Loss:  0.6814474\n",
      "Training Step:  8605 Loss:  0.68142736\n",
      "Training Step:  8606 Loss:  0.6814084\n",
      "Training Step:  8607 Loss:  0.68138933\n",
      "Training Step:  8608 Loss:  0.6813706\n",
      "Training Step:  8609 Loss:  0.6813514\n",
      "Training Step:  8610 Loss:  0.68133265\n",
      "Training Step:  8611 Loss:  0.6813135\n",
      "Training Step:  8612 Loss:  0.68129474\n",
      "Training Step:  8613 Loss:  0.6812754\n",
      "Training Step:  8614 Loss:  0.68125653\n",
      "Training Step:  8615 Loss:  0.6812371\n",
      "Training Step:  8616 Loss:  0.68121874\n",
      "Training Step:  8617 Loss:  0.6811991\n",
      "Training Step:  8618 Loss:  0.68118006\n",
      "Training Step:  8619 Loss:  0.681161\n",
      "Training Step:  8620 Loss:  0.6811421\n",
      "Training Step:  8621 Loss:  0.68112266\n",
      "Training Step:  8622 Loss:  0.68110454\n",
      "Training Step:  8623 Loss:  0.681085\n",
      "Training Step:  8624 Loss:  0.68106633\n",
      "Training Step:  8625 Loss:  0.6810475\n",
      "Training Step:  8626 Loss:  0.68102866\n",
      "Training Step:  8627 Loss:  0.6810094\n",
      "Training Step:  8628 Loss:  0.68099004\n",
      "Training Step:  8629 Loss:  0.6809715\n",
      "Training Step:  8630 Loss:  0.6809529\n",
      "Training Step:  8631 Loss:  0.68093365\n",
      "Training Step:  8632 Loss:  0.68091506\n",
      "Training Step:  8633 Loss:  0.6808959\n",
      "Training Step:  8634 Loss:  0.6808773\n",
      "Training Step:  8635 Loss:  0.68085825\n",
      "Training Step:  8636 Loss:  0.68083954\n",
      "Training Step:  8637 Loss:  0.6808206\n",
      "Training Step:  8638 Loss:  0.68080133\n",
      "Training Step:  8639 Loss:  0.6807823\n",
      "Training Step:  8640 Loss:  0.6807635\n",
      "Training Step:  8641 Loss:  0.68074423\n",
      "Training Step:  8642 Loss:  0.6807254\n",
      "Training Step:  8643 Loss:  0.6807067\n",
      "Training Step:  8644 Loss:  0.6806876\n",
      "Training Step:  8645 Loss:  0.68066883\n",
      "Training Step:  8646 Loss:  0.6806498\n",
      "Training Step:  8647 Loss:  0.6806313\n",
      "Training Step:  8648 Loss:  0.68061244\n",
      "Training Step:  8649 Loss:  0.6805932\n",
      "Training Step:  8650 Loss:  0.6805744\n",
      "Training Step:  8651 Loss:  0.6805558\n",
      "Training Step:  8652 Loss:  0.6805371\n",
      "Training Step:  8653 Loss:  0.68051773\n",
      "Training Step:  8654 Loss:  0.68049884\n",
      "Training Step:  8655 Loss:  0.68047965\n",
      "Training Step:  8656 Loss:  0.68046165\n",
      "Training Step:  8657 Loss:  0.68044245\n",
      "Training Step:  8658 Loss:  0.6804235\n",
      "Training Step:  8659 Loss:  0.6804051\n",
      "Training Step:  8660 Loss:  0.6803863\n",
      "Training Step:  8661 Loss:  0.68036723\n",
      "Training Step:  8662 Loss:  0.68034893\n",
      "Training Step:  8663 Loss:  0.68032974\n",
      "Training Step:  8664 Loss:  0.6803105\n",
      "Training Step:  8665 Loss:  0.68029183\n",
      "Training Step:  8666 Loss:  0.68027264\n",
      "Training Step:  8667 Loss:  0.6802544\n",
      "Training Step:  8668 Loss:  0.6802354\n",
      "Training Step:  8669 Loss:  0.68021715\n",
      "Training Step:  8670 Loss:  0.68019813\n",
      "Training Step:  8671 Loss:  0.68017966\n",
      "Training Step:  8672 Loss:  0.6801605\n",
      "Training Step:  8673 Loss:  0.6801414\n",
      "Training Step:  8674 Loss:  0.68012273\n",
      "Training Step:  8675 Loss:  0.68010396\n",
      "Training Step:  8676 Loss:  0.6800859\n",
      "Training Step:  8677 Loss:  0.6800664\n",
      "Training Step:  8678 Loss:  0.68004787\n",
      "Training Step:  8679 Loss:  0.6800292\n",
      "Training Step:  8680 Loss:  0.6800106\n",
      "Training Step:  8681 Loss:  0.679992\n",
      "Training Step:  8682 Loss:  0.67997295\n",
      "Training Step:  8683 Loss:  0.6799545\n",
      "Training Step:  8684 Loss:  0.6799355\n",
      "Training Step:  8685 Loss:  0.6799171\n",
      "Training Step:  8686 Loss:  0.6798977\n",
      "Training Step:  8687 Loss:  0.6798791\n",
      "Training Step:  8688 Loss:  0.67986083\n",
      "Training Step:  8689 Loss:  0.6798418\n",
      "Training Step:  8690 Loss:  0.67982286\n",
      "Training Step:  8691 Loss:  0.6798044\n",
      "Training Step:  8692 Loss:  0.679785\n",
      "Training Step:  8693 Loss:  0.6797669\n",
      "Training Step:  8694 Loss:  0.679748\n",
      "Training Step:  8695 Loss:  0.6797296\n",
      "Training Step:  8696 Loss:  0.679711\n",
      "Training Step:  8697 Loss:  0.67969286\n",
      "Training Step:  8698 Loss:  0.6796738\n",
      "Training Step:  8699 Loss:  0.6796547\n",
      "Training Step:  8700 Loss:  0.67963606\n",
      "Training Step:  8701 Loss:  0.6796179\n",
      "Training Step:  8702 Loss:  0.6795995\n",
      "Training Step:  8703 Loss:  0.67958057\n",
      "Training Step:  8704 Loss:  0.67956126\n",
      "Training Step:  8705 Loss:  0.67954266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  8706 Loss:  0.6795243\n",
      "Training Step:  8707 Loss:  0.67950636\n",
      "Training Step:  8708 Loss:  0.6794869\n",
      "Training Step:  8709 Loss:  0.67946863\n",
      "Training Step:  8710 Loss:  0.67944926\n",
      "Training Step:  8711 Loss:  0.6794313\n",
      "Training Step:  8712 Loss:  0.6794129\n",
      "Training Step:  8713 Loss:  0.67939425\n",
      "Training Step:  8714 Loss:  0.6793751\n",
      "Training Step:  8715 Loss:  0.6793572\n",
      "Training Step:  8716 Loss:  0.6793386\n",
      "Training Step:  8717 Loss:  0.6793194\n",
      "Training Step:  8718 Loss:  0.67930055\n",
      "Training Step:  8719 Loss:  0.6792819\n",
      "Training Step:  8720 Loss:  0.67926437\n",
      "Training Step:  8721 Loss:  0.67924535\n",
      "Training Step:  8722 Loss:  0.67922634\n",
      "Training Step:  8723 Loss:  0.67920816\n",
      "Training Step:  8724 Loss:  0.67918986\n",
      "Training Step:  8725 Loss:  0.679171\n",
      "Training Step:  8726 Loss:  0.67915255\n",
      "Training Step:  8727 Loss:  0.6791339\n",
      "Training Step:  8728 Loss:  0.6791159\n",
      "Training Step:  8729 Loss:  0.67909724\n",
      "Training Step:  8730 Loss:  0.6790779\n",
      "Training Step:  8731 Loss:  0.67905957\n",
      "Training Step:  8732 Loss:  0.67904115\n",
      "Training Step:  8733 Loss:  0.67902297\n",
      "Training Step:  8734 Loss:  0.6790039\n",
      "Training Step:  8735 Loss:  0.6789853\n",
      "Training Step:  8736 Loss:  0.6789672\n",
      "Training Step:  8737 Loss:  0.67894846\n",
      "Training Step:  8738 Loss:  0.67892975\n",
      "Training Step:  8739 Loss:  0.6789117\n",
      "Training Step:  8740 Loss:  0.6788928\n",
      "Training Step:  8741 Loss:  0.67887497\n",
      "Training Step:  8742 Loss:  0.67885625\n",
      "Training Step:  8743 Loss:  0.6788372\n",
      "Training Step:  8744 Loss:  0.6788191\n",
      "Training Step:  8745 Loss:  0.67880017\n",
      "Training Step:  8746 Loss:  0.67878145\n",
      "Training Step:  8747 Loss:  0.6787636\n",
      "Training Step:  8748 Loss:  0.6787448\n",
      "Training Step:  8749 Loss:  0.67872584\n",
      "Training Step:  8750 Loss:  0.678708\n",
      "Training Step:  8751 Loss:  0.6786896\n",
      "Training Step:  8752 Loss:  0.67867136\n",
      "Training Step:  8753 Loss:  0.67865264\n",
      "Training Step:  8754 Loss:  0.6786344\n",
      "Training Step:  8755 Loss:  0.6786163\n",
      "Training Step:  8756 Loss:  0.67859757\n",
      "Training Step:  8757 Loss:  0.6785787\n",
      "Training Step:  8758 Loss:  0.67856085\n",
      "Training Step:  8759 Loss:  0.6785421\n",
      "Training Step:  8760 Loss:  0.6785233\n",
      "Training Step:  8761 Loss:  0.67850536\n",
      "Training Step:  8762 Loss:  0.67848665\n",
      "Training Step:  8763 Loss:  0.67846787\n",
      "Training Step:  8764 Loss:  0.6784499\n",
      "Training Step:  8765 Loss:  0.67843175\n",
      "Training Step:  8766 Loss:  0.67841315\n",
      "Training Step:  8767 Loss:  0.67839533\n",
      "Training Step:  8768 Loss:  0.67837644\n",
      "Training Step:  8769 Loss:  0.67835766\n",
      "Training Step:  8770 Loss:  0.67834\n",
      "Training Step:  8771 Loss:  0.67832124\n",
      "Training Step:  8772 Loss:  0.67830247\n",
      "Training Step:  8773 Loss:  0.67828494\n",
      "Training Step:  8774 Loss:  0.67826587\n",
      "Training Step:  8775 Loss:  0.6782472\n",
      "Training Step:  8776 Loss:  0.6782299\n",
      "Training Step:  8777 Loss:  0.6782111\n",
      "Training Step:  8778 Loss:  0.67819273\n",
      "Training Step:  8779 Loss:  0.6781746\n",
      "Training Step:  8780 Loss:  0.6781559\n",
      "Training Step:  8781 Loss:  0.6781372\n",
      "Training Step:  8782 Loss:  0.6781193\n",
      "Training Step:  8783 Loss:  0.67810106\n",
      "Training Step:  8784 Loss:  0.67808205\n",
      "Training Step:  8785 Loss:  0.67806447\n",
      "Training Step:  8786 Loss:  0.6780453\n",
      "Training Step:  8787 Loss:  0.67802733\n",
      "Training Step:  8788 Loss:  0.6780098\n",
      "Training Step:  8789 Loss:  0.6779907\n",
      "Training Step:  8790 Loss:  0.67797273\n",
      "Training Step:  8791 Loss:  0.67795473\n",
      "Training Step:  8792 Loss:  0.6779359\n",
      "Training Step:  8793 Loss:  0.6779174\n",
      "Training Step:  8794 Loss:  0.6778991\n",
      "Training Step:  8795 Loss:  0.6778813\n",
      "Training Step:  8796 Loss:  0.677863\n",
      "Training Step:  8797 Loss:  0.6778442\n",
      "Training Step:  8798 Loss:  0.67782634\n",
      "Training Step:  8799 Loss:  0.6778077\n",
      "Training Step:  8800 Loss:  0.6777893\n",
      "Training Step:  8801 Loss:  0.67777157\n",
      "Training Step:  8802 Loss:  0.67775315\n",
      "Training Step:  8803 Loss:  0.67773515\n",
      "Training Step:  8804 Loss:  0.6777169\n",
      "Training Step:  8805 Loss:  0.6776986\n",
      "Training Step:  8806 Loss:  0.67767996\n",
      "Training Step:  8807 Loss:  0.67766136\n",
      "Training Step:  8808 Loss:  0.6776439\n",
      "Training Step:  8809 Loss:  0.67762524\n",
      "Training Step:  8810 Loss:  0.6776069\n",
      "Training Step:  8811 Loss:  0.6775891\n",
      "Training Step:  8812 Loss:  0.67757046\n",
      "Training Step:  8813 Loss:  0.6775522\n",
      "Training Step:  8814 Loss:  0.6775342\n",
      "Training Step:  8815 Loss:  0.6775155\n",
      "Training Step:  8816 Loss:  0.677498\n",
      "Training Step:  8817 Loss:  0.6774792\n",
      "Training Step:  8818 Loss:  0.6774618\n",
      "Training Step:  8819 Loss:  0.67744327\n",
      "Training Step:  8820 Loss:  0.67742497\n",
      "Training Step:  8821 Loss:  0.677407\n",
      "Training Step:  8822 Loss:  0.67738813\n",
      "Training Step:  8823 Loss:  0.6773704\n",
      "Training Step:  8824 Loss:  0.6773518\n",
      "Training Step:  8825 Loss:  0.6773341\n",
      "Training Step:  8826 Loss:  0.67731553\n",
      "Training Step:  8827 Loss:  0.6772975\n",
      "Training Step:  8828 Loss:  0.67727935\n",
      "Training Step:  8829 Loss:  0.6772612\n",
      "Training Step:  8830 Loss:  0.6772433\n",
      "Training Step:  8831 Loss:  0.67722476\n",
      "Training Step:  8832 Loss:  0.67720705\n",
      "Training Step:  8833 Loss:  0.6771888\n",
      "Training Step:  8834 Loss:  0.6771704\n",
      "Training Step:  8835 Loss:  0.6771522\n",
      "Training Step:  8836 Loss:  0.67713434\n",
      "Training Step:  8837 Loss:  0.67711633\n",
      "Training Step:  8838 Loss:  0.67709786\n",
      "Training Step:  8839 Loss:  0.6770795\n",
      "Training Step:  8840 Loss:  0.67706174\n",
      "Training Step:  8841 Loss:  0.67704356\n",
      "Training Step:  8842 Loss:  0.6770252\n",
      "Training Step:  8843 Loss:  0.6770077\n",
      "Training Step:  8844 Loss:  0.676989\n",
      "Training Step:  8845 Loss:  0.67697144\n",
      "Training Step:  8846 Loss:  0.67695254\n",
      "Training Step:  8847 Loss:  0.67693514\n",
      "Training Step:  8848 Loss:  0.6769167\n",
      "Training Step:  8849 Loss:  0.67689884\n",
      "Training Step:  8850 Loss:  0.6768801\n",
      "Training Step:  8851 Loss:  0.67686254\n",
      "Training Step:  8852 Loss:  0.67684424\n",
      "Training Step:  8853 Loss:  0.6768265\n",
      "Training Step:  8854 Loss:  0.6768079\n",
      "Training Step:  8855 Loss:  0.67679024\n",
      "Training Step:  8856 Loss:  0.6767718\n",
      "Training Step:  8857 Loss:  0.67675394\n",
      "Training Step:  8858 Loss:  0.6767352\n",
      "Training Step:  8859 Loss:  0.676718\n",
      "Training Step:  8860 Loss:  0.6766999\n",
      "Training Step:  8861 Loss:  0.6766814\n",
      "Training Step:  8862 Loss:  0.6766633\n",
      "Training Step:  8863 Loss:  0.67664564\n",
      "Training Step:  8864 Loss:  0.67662746\n",
      "Training Step:  8865 Loss:  0.67660964\n",
      "Training Step:  8866 Loss:  0.6765913\n",
      "Training Step:  8867 Loss:  0.6765741\n",
      "Training Step:  8868 Loss:  0.67655563\n",
      "Training Step:  8869 Loss:  0.6765375\n",
      "Training Step:  8870 Loss:  0.67651933\n",
      "Training Step:  8871 Loss:  0.6765015\n",
      "Training Step:  8872 Loss:  0.6764833\n",
      "Training Step:  8873 Loss:  0.6764655\n",
      "Training Step:  8874 Loss:  0.6764472\n",
      "Training Step:  8875 Loss:  0.6764293\n",
      "Training Step:  8876 Loss:  0.67641217\n",
      "Training Step:  8877 Loss:  0.6763935\n",
      "Training Step:  8878 Loss:  0.67637545\n",
      "Training Step:  8879 Loss:  0.6763569\n",
      "Training Step:  8880 Loss:  0.67633986\n",
      "Training Step:  8881 Loss:  0.6763213\n",
      "Training Step:  8882 Loss:  0.67630374\n",
      "Training Step:  8883 Loss:  0.67628527\n",
      "Training Step:  8884 Loss:  0.67626745\n",
      "Training Step:  8885 Loss:  0.6762494\n",
      "Training Step:  8886 Loss:  0.67623144\n",
      "Training Step:  8887 Loss:  0.6762138\n",
      "Training Step:  8888 Loss:  0.6761955\n",
      "Training Step:  8889 Loss:  0.676178\n",
      "Training Step:  8890 Loss:  0.6761597\n",
      "Training Step:  8891 Loss:  0.6761424\n",
      "Training Step:  8892 Loss:  0.6761236\n",
      "Training Step:  8893 Loss:  0.67610586\n",
      "Training Step:  8894 Loss:  0.67608845\n",
      "Training Step:  8895 Loss:  0.6760696\n",
      "Training Step:  8896 Loss:  0.6760523\n",
      "Training Step:  8897 Loss:  0.6760339\n",
      "Training Step:  8898 Loss:  0.6760168\n",
      "Training Step:  8899 Loss:  0.6759987\n",
      "Training Step:  8900 Loss:  0.6759803\n",
      "Training Step:  8901 Loss:  0.6759621\n",
      "Training Step:  8902 Loss:  0.67594445\n",
      "Training Step:  8903 Loss:  0.67592716\n",
      "Training Step:  8904 Loss:  0.6759091\n",
      "Training Step:  8905 Loss:  0.6758911\n",
      "Training Step:  8906 Loss:  0.6758724\n",
      "Training Step:  8907 Loss:  0.67585486\n",
      "Training Step:  8908 Loss:  0.6758375\n",
      "Training Step:  8909 Loss:  0.67581886\n",
      "Training Step:  8910 Loss:  0.67580146\n",
      "Training Step:  8911 Loss:  0.67578334\n",
      "Training Step:  8912 Loss:  0.6757653\n",
      "Training Step:  8913 Loss:  0.6757479\n",
      "Training Step:  8914 Loss:  0.67573035\n",
      "Training Step:  8915 Loss:  0.67571187\n",
      "Training Step:  8916 Loss:  0.6756946\n",
      "Training Step:  8917 Loss:  0.67567587\n",
      "Training Step:  8918 Loss:  0.6756588\n",
      "Training Step:  8919 Loss:  0.67564106\n",
      "Training Step:  8920 Loss:  0.67562246\n",
      "Training Step:  8921 Loss:  0.6756053\n",
      "Training Step:  8922 Loss:  0.6755868\n",
      "Training Step:  8923 Loss:  0.6755699\n",
      "Training Step:  8924 Loss:  0.6755512\n",
      "Training Step:  8925 Loss:  0.6755333\n",
      "Training Step:  8926 Loss:  0.6755159\n",
      "Training Step:  8927 Loss:  0.67549837\n",
      "Training Step:  8928 Loss:  0.6754805\n",
      "Training Step:  8929 Loss:  0.675462\n",
      "Training Step:  8930 Loss:  0.6754448\n",
      "Training Step:  8931 Loss:  0.67542696\n",
      "Training Step:  8932 Loss:  0.67540914\n",
      "Training Step:  8933 Loss:  0.6753906\n",
      "Training Step:  8934 Loss:  0.67537385\n",
      "Training Step:  8935 Loss:  0.6753552\n",
      "Training Step:  8936 Loss:  0.6753373\n",
      "Training Step:  8937 Loss:  0.67532027\n",
      "Training Step:  8938 Loss:  0.67530215\n",
      "Training Step:  8939 Loss:  0.67528343\n",
      "Training Step:  8940 Loss:  0.6752668\n",
      "Training Step:  8941 Loss:  0.67524904\n",
      "Training Step:  8942 Loss:  0.6752313\n",
      "Training Step:  8943 Loss:  0.67521316\n",
      "Training Step:  8944 Loss:  0.6751955\n",
      "Training Step:  8945 Loss:  0.6751773\n",
      "Training Step:  8946 Loss:  0.6751603\n",
      "Training Step:  8947 Loss:  0.6751421\n",
      "Training Step:  8948 Loss:  0.67512476\n",
      "Training Step:  8949 Loss:  0.67510676\n",
      "Training Step:  8950 Loss:  0.6750885\n",
      "Training Step:  8951 Loss:  0.6750706\n",
      "Training Step:  8952 Loss:  0.67505383\n",
      "Training Step:  8953 Loss:  0.6750361\n",
      "Training Step:  8954 Loss:  0.6750179\n",
      "Training Step:  8955 Loss:  0.67500055\n",
      "Training Step:  8956 Loss:  0.674982\n",
      "Training Step:  8957 Loss:  0.6749646\n",
      "Training Step:  8958 Loss:  0.67494726\n",
      "Training Step:  8959 Loss:  0.6749296\n",
      "Training Step:  8960 Loss:  0.6749115\n",
      "Training Step:  8961 Loss:  0.6748935\n",
      "Training Step:  8962 Loss:  0.6748762\n",
      "Training Step:  8963 Loss:  0.6748578\n",
      "Training Step:  8964 Loss:  0.6748406\n",
      "Training Step:  8965 Loss:  0.6748229\n",
      "Training Step:  8966 Loss:  0.6748056\n",
      "Training Step:  8967 Loss:  0.6747879\n",
      "Training Step:  8968 Loss:  0.67477024\n",
      "Training Step:  8969 Loss:  0.6747525\n",
      "Training Step:  8970 Loss:  0.67473435\n",
      "Training Step:  8971 Loss:  0.6747175\n",
      "Training Step:  8972 Loss:  0.6746995\n",
      "Training Step:  8973 Loss:  0.674682\n",
      "Training Step:  8974 Loss:  0.6746637\n",
      "Training Step:  8975 Loss:  0.67464685\n",
      "Training Step:  8976 Loss:  0.67462873\n",
      "Training Step:  8977 Loss:  0.67461056\n",
      "Training Step:  8978 Loss:  0.67459357\n",
      "Training Step:  8979 Loss:  0.674576\n",
      "Training Step:  8980 Loss:  0.67455804\n",
      "Training Step:  8981 Loss:  0.67454034\n",
      "Training Step:  8982 Loss:  0.67452276\n",
      "Training Step:  8983 Loss:  0.6745055\n",
      "Training Step:  8984 Loss:  0.6744873\n",
      "Training Step:  8985 Loss:  0.6744694\n",
      "Training Step:  8986 Loss:  0.6744527\n",
      "Training Step:  8987 Loss:  0.6744351\n",
      "Training Step:  8988 Loss:  0.6744168\n",
      "Training Step:  8989 Loss:  0.6743996\n",
      "Training Step:  8990 Loss:  0.67438173\n",
      "Training Step:  8991 Loss:  0.6743642\n",
      "Training Step:  8992 Loss:  0.6743464\n",
      "Training Step:  8993 Loss:  0.67432857\n",
      "Training Step:  8994 Loss:  0.67431194\n",
      "Training Step:  8995 Loss:  0.6742942\n",
      "Training Step:  8996 Loss:  0.6742761\n",
      "Training Step:  8997 Loss:  0.67425877\n",
      "Training Step:  8998 Loss:  0.6742412\n",
      "Training Step:  8999 Loss:  0.67422336\n",
      "Training Step:  9000 Loss:  0.6742059\n",
      "Training Step:  9001 Loss:  0.6741881\n",
      "Training Step:  9002 Loss:  0.67417073\n",
      "Training Step:  9003 Loss:  0.67415345\n",
      "Training Step:  9004 Loss:  0.67413557\n",
      "Training Step:  9005 Loss:  0.67411864\n",
      "Training Step:  9006 Loss:  0.6741005\n",
      "Training Step:  9007 Loss:  0.67408293\n",
      "Training Step:  9008 Loss:  0.67406535\n",
      "Training Step:  9009 Loss:  0.6740471\n",
      "Training Step:  9010 Loss:  0.6740297\n",
      "Training Step:  9011 Loss:  0.67401284\n",
      "Training Step:  9012 Loss:  0.67399555\n",
      "Training Step:  9013 Loss:  0.6739779\n",
      "Training Step:  9014 Loss:  0.6739599\n",
      "Training Step:  9015 Loss:  0.673943\n",
      "Training Step:  9016 Loss:  0.6739253\n",
      "Training Step:  9017 Loss:  0.6739076\n",
      "Training Step:  9018 Loss:  0.6738895\n",
      "Training Step:  9019 Loss:  0.67387223\n",
      "Training Step:  9020 Loss:  0.6738547\n",
      "Training Step:  9021 Loss:  0.67383736\n",
      "Training Step:  9022 Loss:  0.67382\n",
      "Training Step:  9023 Loss:  0.67380273\n",
      "Training Step:  9024 Loss:  0.6737849\n",
      "Training Step:  9025 Loss:  0.6737672\n",
      "Training Step:  9026 Loss:  0.6737501\n",
      "Training Step:  9027 Loss:  0.6737326\n",
      "Training Step:  9028 Loss:  0.6737151\n",
      "Training Step:  9029 Loss:  0.6736973\n",
      "Training Step:  9030 Loss:  0.67367977\n",
      "Training Step:  9031 Loss:  0.6736624\n",
      "Training Step:  9032 Loss:  0.67364526\n",
      "Training Step:  9033 Loss:  0.6736275\n",
      "Training Step:  9034 Loss:  0.67361\n",
      "Training Step:  9035 Loss:  0.67359245\n",
      "Training Step:  9036 Loss:  0.6735749\n",
      "Training Step:  9037 Loss:  0.6735575\n",
      "Training Step:  9038 Loss:  0.6735403\n",
      "Training Step:  9039 Loss:  0.6735228\n",
      "Training Step:  9040 Loss:  0.6735049\n",
      "Training Step:  9041 Loss:  0.6734881\n",
      "Training Step:  9042 Loss:  0.67346966\n",
      "Training Step:  9043 Loss:  0.6734528\n",
      "Training Step:  9044 Loss:  0.673436\n",
      "Training Step:  9045 Loss:  0.6734184\n",
      "Training Step:  9046 Loss:  0.67340076\n",
      "Training Step:  9047 Loss:  0.67338306\n",
      "Training Step:  9048 Loss:  0.6733661\n",
      "Training Step:  9049 Loss:  0.67334837\n",
      "Training Step:  9050 Loss:  0.6733312\n",
      "Training Step:  9051 Loss:  0.6733135\n",
      "Training Step:  9052 Loss:  0.67329586\n",
      "Training Step:  9053 Loss:  0.6732787\n",
      "Training Step:  9054 Loss:  0.67326105\n",
      "Training Step:  9055 Loss:  0.67324346\n",
      "Training Step:  9056 Loss:  0.673226\n",
      "Training Step:  9057 Loss:  0.673209\n",
      "Training Step:  9058 Loss:  0.6731923\n",
      "Training Step:  9059 Loss:  0.6731748\n",
      "Training Step:  9060 Loss:  0.6731574\n",
      "Training Step:  9061 Loss:  0.67313987\n",
      "Training Step:  9062 Loss:  0.6731219\n",
      "Training Step:  9063 Loss:  0.67310464\n",
      "Training Step:  9064 Loss:  0.673087\n",
      "Training Step:  9065 Loss:  0.6730697\n",
      "Training Step:  9066 Loss:  0.67305255\n",
      "Training Step:  9067 Loss:  0.6730346\n",
      "Training Step:  9068 Loss:  0.67301804\n",
      "Training Step:  9069 Loss:  0.67300034\n",
      "Training Step:  9070 Loss:  0.67298293\n",
      "Training Step:  9071 Loss:  0.67296565\n",
      "Training Step:  9072 Loss:  0.6729479\n",
      "Training Step:  9073 Loss:  0.6729308\n",
      "Training Step:  9074 Loss:  0.67291397\n",
      "Training Step:  9075 Loss:  0.672897\n",
      "Training Step:  9076 Loss:  0.67287946\n",
      "Training Step:  9077 Loss:  0.6728619\n",
      "Training Step:  9078 Loss:  0.67284447\n",
      "Training Step:  9079 Loss:  0.67282665\n",
      "Training Step:  9080 Loss:  0.67280984\n",
      "Training Step:  9081 Loss:  0.6727927\n",
      "Training Step:  9082 Loss:  0.672775\n",
      "Training Step:  9083 Loss:  0.67275745\n",
      "Training Step:  9084 Loss:  0.6727404\n",
      "Training Step:  9085 Loss:  0.6727232\n",
      "Training Step:  9086 Loss:  0.67270577\n",
      "Training Step:  9087 Loss:  0.67268866\n",
      "Training Step:  9088 Loss:  0.67267084\n",
      "Training Step:  9089 Loss:  0.6726542\n",
      "Training Step:  9090 Loss:  0.6726364\n",
      "Training Step:  9091 Loss:  0.6726189\n",
      "Training Step:  9092 Loss:  0.6726011\n",
      "Training Step:  9093 Loss:  0.67258394\n",
      "Training Step:  9094 Loss:  0.672567\n",
      "Training Step:  9095 Loss:  0.6725497\n",
      "Training Step:  9096 Loss:  0.67253196\n",
      "Training Step:  9097 Loss:  0.6725152\n",
      "Training Step:  9098 Loss:  0.6724983\n",
      "Training Step:  9099 Loss:  0.6724815\n",
      "Training Step:  9100 Loss:  0.67246395\n",
      "Training Step:  9101 Loss:  0.67244685\n",
      "Training Step:  9102 Loss:  0.6724294\n",
      "Training Step:  9103 Loss:  0.6724119\n",
      "Training Step:  9104 Loss:  0.672395\n",
      "Training Step:  9105 Loss:  0.6723779\n",
      "Training Step:  9106 Loss:  0.6723604\n",
      "Training Step:  9107 Loss:  0.67234296\n",
      "Training Step:  9108 Loss:  0.67232543\n",
      "Training Step:  9109 Loss:  0.67230827\n",
      "Training Step:  9110 Loss:  0.67229116\n",
      "Training Step:  9111 Loss:  0.67227376\n",
      "Training Step:  9112 Loss:  0.672257\n",
      "Training Step:  9113 Loss:  0.6722395\n",
      "Training Step:  9114 Loss:  0.6722222\n",
      "Training Step:  9115 Loss:  0.67220485\n",
      "Training Step:  9116 Loss:  0.67218786\n",
      "Training Step:  9117 Loss:  0.67217016\n",
      "Training Step:  9118 Loss:  0.67215335\n",
      "Training Step:  9119 Loss:  0.6721358\n",
      "Training Step:  9120 Loss:  0.67211866\n",
      "Training Step:  9121 Loss:  0.6721013\n",
      "Training Step:  9122 Loss:  0.67208433\n",
      "Training Step:  9123 Loss:  0.6720674\n",
      "Training Step:  9124 Loss:  0.67204994\n",
      "Training Step:  9125 Loss:  0.6720332\n",
      "Training Step:  9126 Loss:  0.67201567\n",
      "Training Step:  9127 Loss:  0.67199826\n",
      "Training Step:  9128 Loss:  0.6719809\n",
      "Training Step:  9129 Loss:  0.6719638\n",
      "Training Step:  9130 Loss:  0.67194724\n",
      "Training Step:  9131 Loss:  0.6719297\n",
      "Training Step:  9132 Loss:  0.6719126\n",
      "Training Step:  9133 Loss:  0.67189586\n",
      "Training Step:  9134 Loss:  0.6718784\n",
      "Training Step:  9135 Loss:  0.6718613\n",
      "Training Step:  9136 Loss:  0.671844\n",
      "Training Step:  9137 Loss:  0.671827\n",
      "Training Step:  9138 Loss:  0.67180985\n",
      "Training Step:  9139 Loss:  0.67179215\n",
      "Training Step:  9140 Loss:  0.6717756\n",
      "Training Step:  9141 Loss:  0.67175835\n",
      "Training Step:  9142 Loss:  0.6717413\n",
      "Training Step:  9143 Loss:  0.671724\n",
      "Training Step:  9144 Loss:  0.6717063\n",
      "Training Step:  9145 Loss:  0.67168945\n",
      "Training Step:  9146 Loss:  0.67167246\n",
      "Training Step:  9147 Loss:  0.6716553\n",
      "Training Step:  9148 Loss:  0.671638\n",
      "Training Step:  9149 Loss:  0.6716211\n",
      "Training Step:  9150 Loss:  0.67160374\n",
      "Training Step:  9151 Loss:  0.6715866\n",
      "Training Step:  9152 Loss:  0.6715695\n",
      "Training Step:  9153 Loss:  0.67155284\n",
      "Training Step:  9154 Loss:  0.6715353\n",
      "Training Step:  9155 Loss:  0.67151827\n",
      "Training Step:  9156 Loss:  0.67150146\n",
      "Training Step:  9157 Loss:  0.6714845\n",
      "Training Step:  9158 Loss:  0.67146695\n",
      "Training Step:  9159 Loss:  0.67144954\n",
      "Training Step:  9160 Loss:  0.6714326\n",
      "Training Step:  9161 Loss:  0.6714156\n",
      "Training Step:  9162 Loss:  0.67139894\n",
      "Training Step:  9163 Loss:  0.6713816\n",
      "Training Step:  9164 Loss:  0.6713647\n",
      "Training Step:  9165 Loss:  0.67134774\n",
      "Training Step:  9166 Loss:  0.67133033\n",
      "Training Step:  9167 Loss:  0.6713136\n",
      "Training Step:  9168 Loss:  0.67129624\n",
      "Training Step:  9169 Loss:  0.6712796\n",
      "Training Step:  9170 Loss:  0.6712622\n",
      "Training Step:  9171 Loss:  0.6712446\n",
      "Training Step:  9172 Loss:  0.6712273\n",
      "Training Step:  9173 Loss:  0.6712108\n",
      "Training Step:  9174 Loss:  0.6711933\n",
      "Training Step:  9175 Loss:  0.67117596\n",
      "Training Step:  9176 Loss:  0.67115927\n",
      "Training Step:  9177 Loss:  0.6711419\n",
      "Training Step:  9178 Loss:  0.671125\n",
      "Training Step:  9179 Loss:  0.6711083\n",
      "Training Step:  9180 Loss:  0.67109096\n",
      "Training Step:  9181 Loss:  0.6710745\n",
      "Training Step:  9182 Loss:  0.6710576\n",
      "Training Step:  9183 Loss:  0.6710403\n",
      "Training Step:  9184 Loss:  0.67102355\n",
      "Training Step:  9185 Loss:  0.67100674\n",
      "Training Step:  9186 Loss:  0.6709893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  9187 Loss:  0.6709722\n",
      "Training Step:  9188 Loss:  0.67095596\n",
      "Training Step:  9189 Loss:  0.67093813\n",
      "Training Step:  9190 Loss:  0.67092127\n",
      "Training Step:  9191 Loss:  0.67090476\n",
      "Training Step:  9192 Loss:  0.67088795\n",
      "Training Step:  9193 Loss:  0.67087066\n",
      "Training Step:  9194 Loss:  0.6708536\n",
      "Training Step:  9195 Loss:  0.67083603\n",
      "Training Step:  9196 Loss:  0.670819\n",
      "Training Step:  9197 Loss:  0.670802\n",
      "Training Step:  9198 Loss:  0.67078507\n",
      "Training Step:  9199 Loss:  0.67076784\n",
      "Training Step:  9200 Loss:  0.6707511\n",
      "Training Step:  9201 Loss:  0.6707342\n",
      "Training Step:  9202 Loss:  0.6707174\n",
      "Training Step:  9203 Loss:  0.6707004\n",
      "Training Step:  9204 Loss:  0.6706834\n",
      "Training Step:  9205 Loss:  0.67066634\n",
      "Training Step:  9206 Loss:  0.6706494\n",
      "Training Step:  9207 Loss:  0.67063266\n",
      "Training Step:  9208 Loss:  0.6706155\n",
      "Training Step:  9209 Loss:  0.67059857\n",
      "Training Step:  9210 Loss:  0.6705816\n",
      "Training Step:  9211 Loss:  0.6705652\n",
      "Training Step:  9212 Loss:  0.67054814\n",
      "Training Step:  9213 Loss:  0.67053056\n",
      "Training Step:  9214 Loss:  0.67051333\n",
      "Training Step:  9215 Loss:  0.67049664\n",
      "Training Step:  9216 Loss:  0.6704798\n",
      "Training Step:  9217 Loss:  0.6704632\n",
      "Training Step:  9218 Loss:  0.6704461\n",
      "Training Step:  9219 Loss:  0.67042905\n",
      "Training Step:  9220 Loss:  0.67041266\n",
      "Training Step:  9221 Loss:  0.6703955\n",
      "Training Step:  9222 Loss:  0.67037845\n",
      "Training Step:  9223 Loss:  0.6703614\n",
      "Training Step:  9224 Loss:  0.67034435\n",
      "Training Step:  9225 Loss:  0.6703284\n",
      "Training Step:  9226 Loss:  0.67031115\n",
      "Training Step:  9227 Loss:  0.67029333\n",
      "Training Step:  9228 Loss:  0.6702762\n",
      "Training Step:  9229 Loss:  0.67025965\n",
      "Training Step:  9230 Loss:  0.67024314\n",
      "Training Step:  9231 Loss:  0.67022586\n",
      "Training Step:  9232 Loss:  0.67020893\n",
      "Training Step:  9233 Loss:  0.6701928\n",
      "Training Step:  9234 Loss:  0.6701753\n",
      "Training Step:  9235 Loss:  0.67015845\n",
      "Training Step:  9236 Loss:  0.6701423\n",
      "Training Step:  9237 Loss:  0.670125\n",
      "Training Step:  9238 Loss:  0.6701084\n",
      "Training Step:  9239 Loss:  0.67009103\n",
      "Training Step:  9240 Loss:  0.6700742\n",
      "Training Step:  9241 Loss:  0.6700572\n",
      "Training Step:  9242 Loss:  0.67004025\n",
      "Training Step:  9243 Loss:  0.6700238\n",
      "Training Step:  9244 Loss:  0.6700071\n",
      "Training Step:  9245 Loss:  0.6699899\n",
      "Training Step:  9246 Loss:  0.6699727\n",
      "Training Step:  9247 Loss:  0.6699561\n",
      "Training Step:  9248 Loss:  0.66993964\n",
      "Training Step:  9249 Loss:  0.6699229\n",
      "Training Step:  9250 Loss:  0.6699058\n",
      "Training Step:  9251 Loss:  0.6698886\n",
      "Training Step:  9252 Loss:  0.66987145\n",
      "Training Step:  9253 Loss:  0.66985506\n",
      "Training Step:  9254 Loss:  0.6698385\n",
      "Training Step:  9255 Loss:  0.6698213\n",
      "Training Step:  9256 Loss:  0.6698045\n",
      "Training Step:  9257 Loss:  0.66978794\n",
      "Training Step:  9258 Loss:  0.6697711\n",
      "Training Step:  9259 Loss:  0.66975456\n",
      "Training Step:  9260 Loss:  0.66973674\n",
      "Training Step:  9261 Loss:  0.6697201\n",
      "Training Step:  9262 Loss:  0.6697028\n",
      "Training Step:  9263 Loss:  0.66968644\n",
      "Training Step:  9264 Loss:  0.6696696\n",
      "Training Step:  9265 Loss:  0.6696534\n",
      "Training Step:  9266 Loss:  0.66963667\n",
      "Training Step:  9267 Loss:  0.66961974\n",
      "Training Step:  9268 Loss:  0.66960293\n",
      "Training Step:  9269 Loss:  0.66958636\n",
      "Training Step:  9270 Loss:  0.6695688\n",
      "Training Step:  9271 Loss:  0.6695521\n",
      "Training Step:  9272 Loss:  0.6695359\n",
      "Training Step:  9273 Loss:  0.6695186\n",
      "Training Step:  9274 Loss:  0.6695027\n",
      "Training Step:  9275 Loss:  0.66948545\n",
      "Training Step:  9276 Loss:  0.6694691\n",
      "Training Step:  9277 Loss:  0.6694522\n",
      "Training Step:  9278 Loss:  0.66943467\n",
      "Training Step:  9279 Loss:  0.6694176\n",
      "Training Step:  9280 Loss:  0.6694013\n",
      "Training Step:  9281 Loss:  0.6693846\n",
      "Training Step:  9282 Loss:  0.6693678\n",
      "Training Step:  9283 Loss:  0.669351\n",
      "Training Step:  9284 Loss:  0.66933423\n",
      "Training Step:  9285 Loss:  0.6693176\n",
      "Training Step:  9286 Loss:  0.6693012\n",
      "Training Step:  9287 Loss:  0.6692835\n",
      "Training Step:  9288 Loss:  0.6692674\n",
      "Training Step:  9289 Loss:  0.66925037\n",
      "Training Step:  9290 Loss:  0.66923356\n",
      "Training Step:  9291 Loss:  0.6692171\n",
      "Training Step:  9292 Loss:  0.66920024\n",
      "Training Step:  9293 Loss:  0.6691836\n",
      "Training Step:  9294 Loss:  0.66916716\n",
      "Training Step:  9295 Loss:  0.66915\n",
      "Training Step:  9296 Loss:  0.66913366\n",
      "Training Step:  9297 Loss:  0.66911733\n",
      "Training Step:  9298 Loss:  0.66910017\n",
      "Training Step:  9299 Loss:  0.6690836\n",
      "Training Step:  9300 Loss:  0.6690674\n",
      "Training Step:  9301 Loss:  0.6690502\n",
      "Training Step:  9302 Loss:  0.669033\n",
      "Training Step:  9303 Loss:  0.6690161\n",
      "Training Step:  9304 Loss:  0.66899985\n",
      "Training Step:  9305 Loss:  0.66898316\n",
      "Training Step:  9306 Loss:  0.66896623\n",
      "Training Step:  9307 Loss:  0.6689501\n",
      "Training Step:  9308 Loss:  0.6689334\n",
      "Training Step:  9309 Loss:  0.66891617\n",
      "Training Step:  9310 Loss:  0.66889936\n",
      "Training Step:  9311 Loss:  0.6688832\n",
      "Training Step:  9312 Loss:  0.66886634\n",
      "Training Step:  9313 Loss:  0.6688498\n",
      "Training Step:  9314 Loss:  0.6688335\n",
      "Training Step:  9315 Loss:  0.6688164\n",
      "Training Step:  9316 Loss:  0.6687995\n",
      "Training Step:  9317 Loss:  0.6687828\n",
      "Training Step:  9318 Loss:  0.6687666\n",
      "Training Step:  9319 Loss:  0.6687496\n",
      "Training Step:  9320 Loss:  0.66873336\n",
      "Training Step:  9321 Loss:  0.66871625\n",
      "Training Step:  9322 Loss:  0.66869944\n",
      "Training Step:  9323 Loss:  0.6686824\n",
      "Training Step:  9324 Loss:  0.6686663\n",
      "Training Step:  9325 Loss:  0.6686497\n",
      "Training Step:  9326 Loss:  0.66863304\n",
      "Training Step:  9327 Loss:  0.66861606\n",
      "Training Step:  9328 Loss:  0.6686003\n",
      "Training Step:  9329 Loss:  0.6685827\n",
      "Training Step:  9330 Loss:  0.66856617\n",
      "Training Step:  9331 Loss:  0.66854984\n",
      "Training Step:  9332 Loss:  0.66853356\n",
      "Training Step:  9333 Loss:  0.6685164\n",
      "Training Step:  9334 Loss:  0.6685002\n",
      "Training Step:  9335 Loss:  0.6684834\n",
      "Training Step:  9336 Loss:  0.66846675\n",
      "Training Step:  9337 Loss:  0.6684502\n",
      "Training Step:  9338 Loss:  0.66843367\n",
      "Training Step:  9339 Loss:  0.668417\n",
      "Training Step:  9340 Loss:  0.6684005\n",
      "Training Step:  9341 Loss:  0.66838396\n",
      "Training Step:  9342 Loss:  0.6683668\n",
      "Training Step:  9343 Loss:  0.6683503\n",
      "Training Step:  9344 Loss:  0.66833377\n",
      "Training Step:  9345 Loss:  0.66831726\n",
      "Training Step:  9346 Loss:  0.66830087\n",
      "Training Step:  9347 Loss:  0.6682841\n",
      "Training Step:  9348 Loss:  0.66826737\n",
      "Training Step:  9349 Loss:  0.6682506\n",
      "Training Step:  9350 Loss:  0.6682342\n",
      "Training Step:  9351 Loss:  0.6682175\n",
      "Training Step:  9352 Loss:  0.66820127\n",
      "Training Step:  9353 Loss:  0.6681841\n",
      "Training Step:  9354 Loss:  0.66816765\n",
      "Training Step:  9355 Loss:  0.6681514\n",
      "Training Step:  9356 Loss:  0.66813487\n",
      "Training Step:  9357 Loss:  0.66811806\n",
      "Training Step:  9358 Loss:  0.66810226\n",
      "Training Step:  9359 Loss:  0.66808444\n",
      "Training Step:  9360 Loss:  0.6680689\n",
      "Training Step:  9361 Loss:  0.66805184\n",
      "Training Step:  9362 Loss:  0.66803557\n",
      "Training Step:  9363 Loss:  0.6680194\n",
      "Training Step:  9364 Loss:  0.668002\n",
      "Training Step:  9365 Loss:  0.6679855\n",
      "Training Step:  9366 Loss:  0.6679697\n",
      "Training Step:  9367 Loss:  0.6679526\n",
      "Training Step:  9368 Loss:  0.66793644\n",
      "Training Step:  9369 Loss:  0.66792023\n",
      "Training Step:  9370 Loss:  0.66790295\n",
      "Training Step:  9371 Loss:  0.6678865\n",
      "Training Step:  9372 Loss:  0.66786975\n",
      "Training Step:  9373 Loss:  0.66785336\n",
      "Training Step:  9374 Loss:  0.66783744\n",
      "Training Step:  9375 Loss:  0.66782004\n",
      "Training Step:  9376 Loss:  0.6678037\n",
      "Training Step:  9377 Loss:  0.66778725\n",
      "Training Step:  9378 Loss:  0.6677706\n",
      "Training Step:  9379 Loss:  0.66775465\n",
      "Training Step:  9380 Loss:  0.6677372\n",
      "Training Step:  9381 Loss:  0.66772133\n",
      "Training Step:  9382 Loss:  0.66770464\n",
      "Training Step:  9383 Loss:  0.6676883\n",
      "Training Step:  9384 Loss:  0.66767156\n",
      "Training Step:  9385 Loss:  0.66765517\n",
      "Training Step:  9386 Loss:  0.66763884\n",
      "Training Step:  9387 Loss:  0.6676222\n",
      "Training Step:  9388 Loss:  0.6676054\n",
      "Training Step:  9389 Loss:  0.6675897\n",
      "Training Step:  9390 Loss:  0.6675725\n",
      "Training Step:  9391 Loss:  0.66755617\n",
      "Training Step:  9392 Loss:  0.66753966\n",
      "Training Step:  9393 Loss:  0.6675236\n",
      "Training Step:  9394 Loss:  0.66750705\n",
      "Training Step:  9395 Loss:  0.66748995\n",
      "Training Step:  9396 Loss:  0.66747373\n",
      "Training Step:  9397 Loss:  0.66745764\n",
      "Training Step:  9398 Loss:  0.6674409\n",
      "Training Step:  9399 Loss:  0.6674253\n",
      "Training Step:  9400 Loss:  0.6674076\n",
      "Training Step:  9401 Loss:  0.66739166\n",
      "Training Step:  9402 Loss:  0.66737497\n",
      "Training Step:  9403 Loss:  0.6673588\n",
      "Training Step:  9404 Loss:  0.66734266\n",
      "Training Step:  9405 Loss:  0.66732585\n",
      "Training Step:  9406 Loss:  0.6673092\n",
      "Training Step:  9407 Loss:  0.66729295\n",
      "Training Step:  9408 Loss:  0.6672765\n",
      "Training Step:  9409 Loss:  0.6672599\n",
      "Training Step:  9410 Loss:  0.6672437\n",
      "Training Step:  9411 Loss:  0.66722673\n",
      "Training Step:  9412 Loss:  0.6672102\n",
      "Training Step:  9413 Loss:  0.6671947\n",
      "Training Step:  9414 Loss:  0.66717744\n",
      "Training Step:  9415 Loss:  0.6671615\n",
      "Training Step:  9416 Loss:  0.66714483\n",
      "Training Step:  9417 Loss:  0.66712874\n",
      "Training Step:  9418 Loss:  0.6671115\n",
      "Training Step:  9419 Loss:  0.6670955\n",
      "Training Step:  9420 Loss:  0.6670794\n",
      "Training Step:  9421 Loss:  0.66706336\n",
      "Training Step:  9422 Loss:  0.6670462\n",
      "Training Step:  9423 Loss:  0.66702974\n",
      "Training Step:  9424 Loss:  0.6670139\n",
      "Training Step:  9425 Loss:  0.6669972\n",
      "Training Step:  9426 Loss:  0.66698104\n",
      "Training Step:  9427 Loss:  0.6669643\n",
      "Training Step:  9428 Loss:  0.6669477\n",
      "Training Step:  9429 Loss:  0.6669315\n",
      "Training Step:  9430 Loss:  0.66691524\n",
      "Training Step:  9431 Loss:  0.66689813\n",
      "Training Step:  9432 Loss:  0.6668829\n",
      "Training Step:  9433 Loss:  0.666866\n",
      "Training Step:  9434 Loss:  0.6668496\n",
      "Training Step:  9435 Loss:  0.6668333\n",
      "Training Step:  9436 Loss:  0.6668164\n",
      "Training Step:  9437 Loss:  0.6668004\n",
      "Training Step:  9438 Loss:  0.66678375\n",
      "Training Step:  9439 Loss:  0.66676813\n",
      "Training Step:  9440 Loss:  0.6667509\n",
      "Training Step:  9441 Loss:  0.66673493\n",
      "Training Step:  9442 Loss:  0.6667185\n",
      "Training Step:  9443 Loss:  0.6667022\n",
      "Training Step:  9444 Loss:  0.6666856\n",
      "Training Step:  9445 Loss:  0.6666699\n",
      "Training Step:  9446 Loss:  0.666653\n",
      "Training Step:  9447 Loss:  0.6666373\n",
      "Training Step:  9448 Loss:  0.66662014\n",
      "Training Step:  9449 Loss:  0.6666039\n",
      "Training Step:  9450 Loss:  0.66658777\n",
      "Training Step:  9451 Loss:  0.66657203\n",
      "Training Step:  9452 Loss:  0.666555\n",
      "Training Step:  9453 Loss:  0.6665391\n",
      "Training Step:  9454 Loss:  0.6665221\n",
      "Training Step:  9455 Loss:  0.6665066\n",
      "Training Step:  9456 Loss:  0.66648936\n",
      "Training Step:  9457 Loss:  0.66647357\n",
      "Training Step:  9458 Loss:  0.6664572\n",
      "Training Step:  9459 Loss:  0.666441\n",
      "Training Step:  9460 Loss:  0.6664242\n",
      "Training Step:  9461 Loss:  0.66640854\n",
      "Training Step:  9462 Loss:  0.6663923\n",
      "Training Step:  9463 Loss:  0.6663757\n",
      "Training Step:  9464 Loss:  0.6663588\n",
      "Training Step:  9465 Loss:  0.6663428\n",
      "Training Step:  9466 Loss:  0.6663266\n",
      "Training Step:  9467 Loss:  0.6663105\n",
      "Training Step:  9468 Loss:  0.6662931\n",
      "Training Step:  9469 Loss:  0.66627765\n",
      "Training Step:  9470 Loss:  0.6662614\n",
      "Training Step:  9471 Loss:  0.66624475\n",
      "Training Step:  9472 Loss:  0.6662285\n",
      "Training Step:  9473 Loss:  0.66621196\n",
      "Training Step:  9474 Loss:  0.6661965\n",
      "Training Step:  9475 Loss:  0.66617936\n",
      "Training Step:  9476 Loss:  0.6661633\n",
      "Training Step:  9477 Loss:  0.6661472\n",
      "Training Step:  9478 Loss:  0.66613156\n",
      "Training Step:  9479 Loss:  0.6661148\n",
      "Training Step:  9480 Loss:  0.66609854\n",
      "Training Step:  9481 Loss:  0.6660817\n",
      "Training Step:  9482 Loss:  0.666066\n",
      "Training Step:  9483 Loss:  0.6660495\n",
      "Training Step:  9484 Loss:  0.66603297\n",
      "Training Step:  9485 Loss:  0.666017\n",
      "Training Step:  9486 Loss:  0.6660005\n",
      "Training Step:  9487 Loss:  0.66598463\n",
      "Training Step:  9488 Loss:  0.66596854\n",
      "Training Step:  9489 Loss:  0.6659525\n",
      "Training Step:  9490 Loss:  0.66593575\n",
      "Training Step:  9491 Loss:  0.6659199\n",
      "Training Step:  9492 Loss:  0.6659029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  9493 Loss:  0.6658865\n",
      "Training Step:  9494 Loss:  0.6658708\n",
      "Training Step:  9495 Loss:  0.6658547\n",
      "Training Step:  9496 Loss:  0.66583836\n",
      "Training Step:  9497 Loss:  0.66582197\n",
      "Training Step:  9498 Loss:  0.66580606\n",
      "Training Step:  9499 Loss:  0.6657898\n",
      "Training Step:  9500 Loss:  0.6657735\n",
      "Training Step:  9501 Loss:  0.66575694\n",
      "Training Step:  9502 Loss:  0.66574055\n",
      "Training Step:  9503 Loss:  0.6657244\n",
      "Training Step:  9504 Loss:  0.6657079\n",
      "Training Step:  9505 Loss:  0.66569227\n",
      "Training Step:  9506 Loss:  0.6656761\n",
      "Training Step:  9507 Loss:  0.66565955\n",
      "Training Step:  9508 Loss:  0.6656433\n",
      "Training Step:  9509 Loss:  0.6656272\n",
      "Training Step:  9510 Loss:  0.6656113\n",
      "Training Step:  9511 Loss:  0.6655946\n",
      "Training Step:  9512 Loss:  0.6655786\n",
      "Training Step:  9513 Loss:  0.66556233\n",
      "Training Step:  9514 Loss:  0.6655457\n",
      "Training Step:  9515 Loss:  0.66552997\n",
      "Training Step:  9516 Loss:  0.6655135\n",
      "Training Step:  9517 Loss:  0.665498\n",
      "Training Step:  9518 Loss:  0.6654813\n",
      "Training Step:  9519 Loss:  0.66546553\n",
      "Training Step:  9520 Loss:  0.66544974\n",
      "Training Step:  9521 Loss:  0.66543263\n",
      "Training Step:  9522 Loss:  0.66541684\n",
      "Training Step:  9523 Loss:  0.6654007\n",
      "Training Step:  9524 Loss:  0.66538435\n",
      "Training Step:  9525 Loss:  0.6653682\n",
      "Training Step:  9526 Loss:  0.6653515\n",
      "Training Step:  9527 Loss:  0.66533595\n",
      "Training Step:  9528 Loss:  0.6653191\n",
      "Training Step:  9529 Loss:  0.6653034\n",
      "Training Step:  9530 Loss:  0.66528773\n",
      "Training Step:  9531 Loss:  0.66527104\n",
      "Training Step:  9532 Loss:  0.6652552\n",
      "Training Step:  9533 Loss:  0.66523916\n",
      "Training Step:  9534 Loss:  0.6652224\n",
      "Training Step:  9535 Loss:  0.665206\n",
      "Training Step:  9536 Loss:  0.66519016\n",
      "Training Step:  9537 Loss:  0.66517425\n",
      "Training Step:  9538 Loss:  0.6651581\n",
      "Training Step:  9539 Loss:  0.6651416\n",
      "Training Step:  9540 Loss:  0.6651256\n",
      "Training Step:  9541 Loss:  0.6651092\n",
      "Training Step:  9542 Loss:  0.66509336\n",
      "Training Step:  9543 Loss:  0.6650773\n",
      "Training Step:  9544 Loss:  0.66506106\n",
      "Training Step:  9545 Loss:  0.6650451\n",
      "Training Step:  9546 Loss:  0.6650292\n",
      "Training Step:  9547 Loss:  0.6650126\n",
      "Training Step:  9548 Loss:  0.6649967\n",
      "Training Step:  9549 Loss:  0.6649809\n",
      "Training Step:  9550 Loss:  0.6649642\n",
      "Training Step:  9551 Loss:  0.66494846\n",
      "Training Step:  9552 Loss:  0.6649322\n",
      "Training Step:  9553 Loss:  0.6649155\n",
      "Training Step:  9554 Loss:  0.66489995\n",
      "Training Step:  9555 Loss:  0.66488385\n",
      "Training Step:  9556 Loss:  0.6648676\n",
      "Training Step:  9557 Loss:  0.66485167\n",
      "Training Step:  9558 Loss:  0.6648359\n",
      "Training Step:  9559 Loss:  0.66481954\n",
      "Training Step:  9560 Loss:  0.6648031\n",
      "Training Step:  9561 Loss:  0.6647873\n",
      "Training Step:  9562 Loss:  0.6647711\n",
      "Training Step:  9563 Loss:  0.664755\n",
      "Training Step:  9564 Loss:  0.66473883\n",
      "Training Step:  9565 Loss:  0.664723\n",
      "Training Step:  9566 Loss:  0.66470665\n",
      "Training Step:  9567 Loss:  0.6646908\n",
      "Training Step:  9568 Loss:  0.66467464\n",
      "Training Step:  9569 Loss:  0.6646583\n",
      "Training Step:  9570 Loss:  0.6646425\n",
      "Training Step:  9571 Loss:  0.6646262\n",
      "Training Step:  9572 Loss:  0.66460997\n",
      "Training Step:  9573 Loss:  0.6645939\n",
      "Training Step:  9574 Loss:  0.6645781\n",
      "Training Step:  9575 Loss:  0.66456175\n",
      "Training Step:  9576 Loss:  0.66454566\n",
      "Training Step:  9577 Loss:  0.6645303\n",
      "Training Step:  9578 Loss:  0.66451365\n",
      "Training Step:  9579 Loss:  0.6644974\n",
      "Training Step:  9580 Loss:  0.6644821\n",
      "Training Step:  9581 Loss:  0.66446596\n",
      "Training Step:  9582 Loss:  0.66444933\n",
      "Training Step:  9583 Loss:  0.6644334\n",
      "Training Step:  9584 Loss:  0.6644173\n",
      "Training Step:  9585 Loss:  0.6644014\n",
      "Training Step:  9586 Loss:  0.66438496\n",
      "Training Step:  9587 Loss:  0.6643697\n",
      "Training Step:  9588 Loss:  0.6643535\n",
      "Training Step:  9589 Loss:  0.66433704\n",
      "Training Step:  9590 Loss:  0.66432136\n",
      "Training Step:  9591 Loss:  0.66430557\n",
      "Training Step:  9592 Loss:  0.6642892\n",
      "Training Step:  9593 Loss:  0.6642733\n",
      "Training Step:  9594 Loss:  0.66425735\n",
      "Training Step:  9595 Loss:  0.66424096\n",
      "Training Step:  9596 Loss:  0.66422534\n",
      "Training Step:  9597 Loss:  0.66420954\n",
      "Training Step:  9598 Loss:  0.6641926\n",
      "Training Step:  9599 Loss:  0.66417706\n",
      "Training Step:  9600 Loss:  0.66416097\n",
      "Training Step:  9601 Loss:  0.6641449\n",
      "Training Step:  9602 Loss:  0.6641288\n",
      "Training Step:  9603 Loss:  0.6641128\n",
      "Training Step:  9604 Loss:  0.6640968\n",
      "Training Step:  9605 Loss:  0.6640804\n",
      "Training Step:  9606 Loss:  0.66406465\n",
      "Training Step:  9607 Loss:  0.66404927\n",
      "Training Step:  9608 Loss:  0.6640328\n",
      "Training Step:  9609 Loss:  0.66401684\n",
      "Training Step:  9610 Loss:  0.66400087\n",
      "Training Step:  9611 Loss:  0.66398495\n",
      "Training Step:  9612 Loss:  0.6639688\n",
      "Training Step:  9613 Loss:  0.6639526\n",
      "Training Step:  9614 Loss:  0.663937\n",
      "Training Step:  9615 Loss:  0.66392094\n",
      "Training Step:  9616 Loss:  0.66390544\n",
      "Training Step:  9617 Loss:  0.66388905\n",
      "Training Step:  9618 Loss:  0.6638731\n",
      "Training Step:  9619 Loss:  0.6638573\n",
      "Training Step:  9620 Loss:  0.663841\n",
      "Training Step:  9621 Loss:  0.6638254\n",
      "Training Step:  9622 Loss:  0.6638097\n",
      "Training Step:  9623 Loss:  0.66379374\n",
      "Training Step:  9624 Loss:  0.6637774\n",
      "Training Step:  9625 Loss:  0.66376156\n",
      "Training Step:  9626 Loss:  0.6637452\n",
      "Training Step:  9627 Loss:  0.6637291\n",
      "Training Step:  9628 Loss:  0.66371274\n",
      "Training Step:  9629 Loss:  0.6636972\n",
      "Training Step:  9630 Loss:  0.66368186\n",
      "Training Step:  9631 Loss:  0.6636654\n",
      "Training Step:  9632 Loss:  0.66364944\n",
      "Training Step:  9633 Loss:  0.66363347\n",
      "Training Step:  9634 Loss:  0.66361684\n",
      "Training Step:  9635 Loss:  0.6636015\n",
      "Training Step:  9636 Loss:  0.6635852\n",
      "Training Step:  9637 Loss:  0.66356987\n",
      "Training Step:  9638 Loss:  0.66355383\n",
      "Training Step:  9639 Loss:  0.66353774\n",
      "Training Step:  9640 Loss:  0.66352206\n",
      "Training Step:  9641 Loss:  0.6635059\n",
      "Training Step:  9642 Loss:  0.66349024\n",
      "Training Step:  9643 Loss:  0.6634743\n",
      "Training Step:  9644 Loss:  0.6634577\n",
      "Training Step:  9645 Loss:  0.6634424\n",
      "Training Step:  9646 Loss:  0.66342664\n",
      "Training Step:  9647 Loss:  0.6634107\n",
      "Training Step:  9648 Loss:  0.6633949\n",
      "Training Step:  9649 Loss:  0.66337824\n",
      "Training Step:  9650 Loss:  0.6633623\n",
      "Training Step:  9651 Loss:  0.663347\n",
      "Training Step:  9652 Loss:  0.6633312\n",
      "Training Step:  9653 Loss:  0.663315\n",
      "Training Step:  9654 Loss:  0.66329867\n",
      "Training Step:  9655 Loss:  0.66328347\n",
      "Training Step:  9656 Loss:  0.66326725\n",
      "Training Step:  9657 Loss:  0.6632514\n",
      "Training Step:  9658 Loss:  0.66323507\n",
      "Training Step:  9659 Loss:  0.6632191\n",
      "Training Step:  9660 Loss:  0.6632038\n",
      "Training Step:  9661 Loss:  0.66318816\n",
      "Training Step:  9662 Loss:  0.6631719\n",
      "Training Step:  9663 Loss:  0.66315585\n",
      "Training Step:  9664 Loss:  0.6631402\n",
      "Training Step:  9665 Loss:  0.6631247\n",
      "Training Step:  9666 Loss:  0.6631087\n",
      "Training Step:  9667 Loss:  0.6630921\n",
      "Training Step:  9668 Loss:  0.6630761\n",
      "Training Step:  9669 Loss:  0.66306096\n",
      "Training Step:  9670 Loss:  0.6630444\n",
      "Training Step:  9671 Loss:  0.663029\n",
      "Training Step:  9672 Loss:  0.6630126\n",
      "Training Step:  9673 Loss:  0.66299707\n",
      "Training Step:  9674 Loss:  0.66298103\n",
      "Training Step:  9675 Loss:  0.66296524\n",
      "Training Step:  9676 Loss:  0.6629497\n",
      "Training Step:  9677 Loss:  0.6629332\n",
      "Training Step:  9678 Loss:  0.6629178\n",
      "Training Step:  9679 Loss:  0.66290164\n",
      "Training Step:  9680 Loss:  0.66288626\n",
      "Training Step:  9681 Loss:  0.6628707\n",
      "Training Step:  9682 Loss:  0.66285414\n",
      "Training Step:  9683 Loss:  0.6628384\n",
      "Training Step:  9684 Loss:  0.66282237\n",
      "Training Step:  9685 Loss:  0.662807\n",
      "Training Step:  9686 Loss:  0.6627914\n",
      "Training Step:  9687 Loss:  0.6627749\n",
      "Training Step:  9688 Loss:  0.66275895\n",
      "Training Step:  9689 Loss:  0.66274315\n",
      "Training Step:  9690 Loss:  0.66272765\n",
      "Training Step:  9691 Loss:  0.66271216\n",
      "Training Step:  9692 Loss:  0.6626964\n",
      "Training Step:  9693 Loss:  0.6626807\n",
      "Training Step:  9694 Loss:  0.66266423\n",
      "Training Step:  9695 Loss:  0.66264844\n",
      "Training Step:  9696 Loss:  0.6626327\n",
      "Training Step:  9697 Loss:  0.6626171\n",
      "Training Step:  9698 Loss:  0.6626008\n",
      "Training Step:  9699 Loss:  0.6625851\n",
      "Training Step:  9700 Loss:  0.6625694\n",
      "Training Step:  9701 Loss:  0.6625531\n",
      "Training Step:  9702 Loss:  0.6625378\n",
      "Training Step:  9703 Loss:  0.66252226\n",
      "Training Step:  9704 Loss:  0.66250587\n",
      "Training Step:  9705 Loss:  0.6624903\n",
      "Training Step:  9706 Loss:  0.66247404\n",
      "Training Step:  9707 Loss:  0.6624586\n",
      "Training Step:  9708 Loss:  0.66244256\n",
      "Training Step:  9709 Loss:  0.66242677\n",
      "Training Step:  9710 Loss:  0.66241133\n",
      "Training Step:  9711 Loss:  0.66239524\n",
      "Training Step:  9712 Loss:  0.6623803\n",
      "Training Step:  9713 Loss:  0.66236347\n",
      "Training Step:  9714 Loss:  0.6623478\n",
      "Training Step:  9715 Loss:  0.6623323\n",
      "Training Step:  9716 Loss:  0.6623167\n",
      "Training Step:  9717 Loss:  0.6623012\n",
      "Training Step:  9718 Loss:  0.6622846\n",
      "Training Step:  9719 Loss:  0.6622693\n",
      "Training Step:  9720 Loss:  0.66225326\n",
      "Training Step:  9721 Loss:  0.6622381\n",
      "Training Step:  9722 Loss:  0.6622215\n",
      "Training Step:  9723 Loss:  0.6622065\n",
      "Training Step:  9724 Loss:  0.6621911\n",
      "Training Step:  9725 Loss:  0.66217464\n",
      "Training Step:  9726 Loss:  0.66215914\n",
      "Training Step:  9727 Loss:  0.6621429\n",
      "Training Step:  9728 Loss:  0.6621275\n",
      "Training Step:  9729 Loss:  0.66211104\n",
      "Training Step:  9730 Loss:  0.6620958\n",
      "Training Step:  9731 Loss:  0.66208005\n",
      "Training Step:  9732 Loss:  0.66206425\n",
      "Training Step:  9733 Loss:  0.66204876\n",
      "Training Step:  9734 Loss:  0.6620327\n",
      "Training Step:  9735 Loss:  0.66201735\n",
      "Training Step:  9736 Loss:  0.66200095\n",
      "Training Step:  9737 Loss:  0.6619854\n",
      "Training Step:  9738 Loss:  0.66196954\n",
      "Training Step:  9739 Loss:  0.6619537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  9740 Loss:  0.6619383\n",
      "Training Step:  9741 Loss:  0.66192245\n",
      "Training Step:  9742 Loss:  0.6619068\n",
      "Training Step:  9743 Loss:  0.6618909\n",
      "Training Step:  9744 Loss:  0.6618757\n",
      "Training Step:  9745 Loss:  0.66185975\n",
      "Training Step:  9746 Loss:  0.6618439\n",
      "Training Step:  9747 Loss:  0.66182774\n",
      "Training Step:  9748 Loss:  0.6618124\n",
      "Training Step:  9749 Loss:  0.6617962\n",
      "Training Step:  9750 Loss:  0.66178095\n",
      "Training Step:  9751 Loss:  0.66176605\n",
      "Training Step:  9752 Loss:  0.66174954\n",
      "Training Step:  9753 Loss:  0.6617342\n",
      "Training Step:  9754 Loss:  0.66171837\n",
      "Training Step:  9755 Loss:  0.66170293\n",
      "Training Step:  9756 Loss:  0.6616864\n",
      "Training Step:  9757 Loss:  0.66167086\n",
      "Training Step:  9758 Loss:  0.6616553\n",
      "Training Step:  9759 Loss:  0.6616392\n",
      "Training Step:  9760 Loss:  0.6616234\n",
      "Training Step:  9761 Loss:  0.6616084\n",
      "Training Step:  9762 Loss:  0.66159296\n",
      "Training Step:  9763 Loss:  0.6615769\n",
      "Training Step:  9764 Loss:  0.6615616\n",
      "Training Step:  9765 Loss:  0.6615452\n",
      "Training Step:  9766 Loss:  0.6615301\n",
      "Training Step:  9767 Loss:  0.6615139\n",
      "Training Step:  9768 Loss:  0.6614986\n",
      "Training Step:  9769 Loss:  0.6614827\n",
      "Training Step:  9770 Loss:  0.66146684\n",
      "Training Step:  9771 Loss:  0.66145104\n",
      "Training Step:  9772 Loss:  0.66143525\n",
      "Training Step:  9773 Loss:  0.6614197\n",
      "Training Step:  9774 Loss:  0.6614043\n",
      "Training Step:  9775 Loss:  0.66138774\n",
      "Training Step:  9776 Loss:  0.6613731\n",
      "Training Step:  9777 Loss:  0.6613575\n",
      "Training Step:  9778 Loss:  0.66134137\n",
      "Training Step:  9779 Loss:  0.6613262\n",
      "Training Step:  9780 Loss:  0.66131014\n",
      "Training Step:  9781 Loss:  0.66129506\n",
      "Training Step:  9782 Loss:  0.6612792\n",
      "Training Step:  9783 Loss:  0.6612633\n",
      "Training Step:  9784 Loss:  0.66124797\n",
      "Training Step:  9785 Loss:  0.66123176\n",
      "Training Step:  9786 Loss:  0.66121596\n",
      "Training Step:  9787 Loss:  0.66120076\n",
      "Training Step:  9788 Loss:  0.6611853\n",
      "Training Step:  9789 Loss:  0.66116935\n",
      "Training Step:  9790 Loss:  0.66115355\n",
      "Training Step:  9791 Loss:  0.6611379\n",
      "Training Step:  9792 Loss:  0.661122\n",
      "Training Step:  9793 Loss:  0.66110677\n",
      "Training Step:  9794 Loss:  0.66109097\n",
      "Training Step:  9795 Loss:  0.6610754\n",
      "Training Step:  9796 Loss:  0.66106033\n",
      "Training Step:  9797 Loss:  0.6610445\n",
      "Training Step:  9798 Loss:  0.6610288\n",
      "Training Step:  9799 Loss:  0.6610131\n",
      "Training Step:  9800 Loss:  0.66099775\n",
      "Training Step:  9801 Loss:  0.66098183\n",
      "Training Step:  9802 Loss:  0.6609665\n",
      "Training Step:  9803 Loss:  0.6609505\n",
      "Training Step:  9804 Loss:  0.6609349\n",
      "Training Step:  9805 Loss:  0.66091925\n",
      "Training Step:  9806 Loss:  0.66090417\n",
      "Training Step:  9807 Loss:  0.6608881\n",
      "Training Step:  9808 Loss:  0.66087276\n",
      "Training Step:  9809 Loss:  0.6608565\n",
      "Training Step:  9810 Loss:  0.66084164\n",
      "Training Step:  9811 Loss:  0.66082543\n",
      "Training Step:  9812 Loss:  0.6608101\n",
      "Training Step:  9813 Loss:  0.6607942\n",
      "Training Step:  9814 Loss:  0.6607784\n",
      "Training Step:  9815 Loss:  0.6607627\n",
      "Training Step:  9816 Loss:  0.66074723\n",
      "Training Step:  9817 Loss:  0.66073143\n",
      "Training Step:  9818 Loss:  0.66071624\n",
      "Training Step:  9819 Loss:  0.6607001\n",
      "Training Step:  9820 Loss:  0.66068506\n",
      "Training Step:  9821 Loss:  0.66066897\n",
      "Training Step:  9822 Loss:  0.66065395\n",
      "Training Step:  9823 Loss:  0.66063815\n",
      "Training Step:  9824 Loss:  0.66062266\n",
      "Training Step:  9825 Loss:  0.66060734\n",
      "Training Step:  9826 Loss:  0.6605916\n",
      "Training Step:  9827 Loss:  0.66057557\n",
      "Training Step:  9828 Loss:  0.66056025\n",
      "Training Step:  9829 Loss:  0.6605449\n",
      "Training Step:  9830 Loss:  0.6605296\n",
      "Training Step:  9831 Loss:  0.6605135\n",
      "Training Step:  9832 Loss:  0.6604984\n",
      "Training Step:  9833 Loss:  0.6604823\n",
      "Training Step:  9834 Loss:  0.660467\n",
      "Training Step:  9835 Loss:  0.6604509\n",
      "Training Step:  9836 Loss:  0.66043603\n",
      "Training Step:  9837 Loss:  0.6604196\n",
      "Training Step:  9838 Loss:  0.6604049\n",
      "Training Step:  9839 Loss:  0.6603888\n",
      "Training Step:  9840 Loss:  0.66037387\n",
      "Training Step:  9841 Loss:  0.660358\n",
      "Training Step:  9842 Loss:  0.66034275\n",
      "Training Step:  9843 Loss:  0.6603271\n",
      "Training Step:  9844 Loss:  0.6603117\n",
      "Training Step:  9845 Loss:  0.66029596\n",
      "Training Step:  9846 Loss:  0.6602807\n",
      "Training Step:  9847 Loss:  0.6602644\n",
      "Training Step:  9848 Loss:  0.66024905\n",
      "Training Step:  9849 Loss:  0.6602331\n",
      "Training Step:  9850 Loss:  0.6602181\n",
      "Training Step:  9851 Loss:  0.66020215\n",
      "Training Step:  9852 Loss:  0.6601875\n",
      "Training Step:  9853 Loss:  0.6601715\n",
      "Training Step:  9854 Loss:  0.6601564\n",
      "Training Step:  9855 Loss:  0.6601402\n",
      "Training Step:  9856 Loss:  0.66012514\n",
      "Training Step:  9857 Loss:  0.66010916\n",
      "Training Step:  9858 Loss:  0.6600937\n",
      "Training Step:  9859 Loss:  0.660078\n",
      "Training Step:  9860 Loss:  0.66006243\n",
      "Training Step:  9861 Loss:  0.66004765\n",
      "Training Step:  9862 Loss:  0.66003174\n",
      "Training Step:  9863 Loss:  0.6600163\n",
      "Training Step:  9864 Loss:  0.6600006\n",
      "Training Step:  9865 Loss:  0.6599851\n",
      "Training Step:  9866 Loss:  0.65996945\n",
      "Training Step:  9867 Loss:  0.6599539\n",
      "Training Step:  9868 Loss:  0.65993863\n",
      "Training Step:  9869 Loss:  0.65992296\n",
      "Training Step:  9870 Loss:  0.6599075\n",
      "Training Step:  9871 Loss:  0.6598929\n",
      "Training Step:  9872 Loss:  0.6598767\n",
      "Training Step:  9873 Loss:  0.6598618\n",
      "Training Step:  9874 Loss:  0.6598462\n",
      "Training Step:  9875 Loss:  0.6598306\n",
      "Training Step:  9876 Loss:  0.65981495\n",
      "Training Step:  9877 Loss:  0.65979964\n",
      "Training Step:  9878 Loss:  0.65978396\n",
      "Training Step:  9879 Loss:  0.65976804\n",
      "Training Step:  9880 Loss:  0.65975296\n",
      "Training Step:  9881 Loss:  0.65973693\n",
      "Training Step:  9882 Loss:  0.65972173\n",
      "Training Step:  9883 Loss:  0.65970594\n",
      "Training Step:  9884 Loss:  0.6596905\n",
      "Training Step:  9885 Loss:  0.6596751\n",
      "Training Step:  9886 Loss:  0.65966\n",
      "Training Step:  9887 Loss:  0.6596444\n",
      "Training Step:  9888 Loss:  0.659629\n",
      "Training Step:  9889 Loss:  0.6596129\n",
      "Training Step:  9890 Loss:  0.65959793\n",
      "Training Step:  9891 Loss:  0.65958226\n",
      "Training Step:  9892 Loss:  0.6595675\n",
      "Training Step:  9893 Loss:  0.65955114\n",
      "Training Step:  9894 Loss:  0.6595358\n",
      "Training Step:  9895 Loss:  0.6595203\n",
      "Training Step:  9896 Loss:  0.6595044\n",
      "Training Step:  9897 Loss:  0.65948945\n",
      "Training Step:  9898 Loss:  0.6594743\n",
      "Training Step:  9899 Loss:  0.65945935\n",
      "Training Step:  9900 Loss:  0.6594431\n",
      "Training Step:  9901 Loss:  0.6594281\n",
      "Training Step:  9902 Loss:  0.6594128\n",
      "Training Step:  9903 Loss:  0.659397\n",
      "Training Step:  9904 Loss:  0.65938157\n",
      "Training Step:  9905 Loss:  0.6593659\n",
      "Training Step:  9906 Loss:  0.65935016\n",
      "Training Step:  9907 Loss:  0.659335\n",
      "Training Step:  9908 Loss:  0.6593198\n",
      "Training Step:  9909 Loss:  0.6593039\n",
      "Training Step:  9910 Loss:  0.659289\n",
      "Training Step:  9911 Loss:  0.65927345\n",
      "Training Step:  9912 Loss:  0.6592586\n",
      "Training Step:  9913 Loss:  0.65924275\n",
      "Training Step:  9914 Loss:  0.6592272\n",
      "Training Step:  9915 Loss:  0.6592119\n",
      "Training Step:  9916 Loss:  0.6591954\n",
      "Training Step:  9917 Loss:  0.6591806\n",
      "Training Step:  9918 Loss:  0.65916526\n",
      "Training Step:  9919 Loss:  0.6591504\n",
      "Training Step:  9920 Loss:  0.65913427\n",
      "Training Step:  9921 Loss:  0.65911925\n",
      "Training Step:  9922 Loss:  0.6591038\n",
      "Training Step:  9923 Loss:  0.6590885\n",
      "Training Step:  9924 Loss:  0.65907264\n",
      "Training Step:  9925 Loss:  0.6590572\n",
      "Training Step:  9926 Loss:  0.65904236\n",
      "Training Step:  9927 Loss:  0.65902644\n",
      "Training Step:  9928 Loss:  0.6590115\n",
      "Training Step:  9929 Loss:  0.65899575\n",
      "Training Step:  9930 Loss:  0.65898085\n",
      "Training Step:  9931 Loss:  0.65896463\n",
      "Training Step:  9932 Loss:  0.65894896\n",
      "Training Step:  9933 Loss:  0.658934\n",
      "Training Step:  9934 Loss:  0.65891904\n",
      "Training Step:  9935 Loss:  0.65890354\n",
      "Training Step:  9936 Loss:  0.65888786\n",
      "Training Step:  9937 Loss:  0.65887284\n",
      "Training Step:  9938 Loss:  0.6588576\n",
      "Training Step:  9939 Loss:  0.65884197\n",
      "Training Step:  9940 Loss:  0.6588265\n",
      "Training Step:  9941 Loss:  0.6588108\n",
      "Training Step:  9942 Loss:  0.6587957\n",
      "Training Step:  9943 Loss:  0.65878046\n",
      "Training Step:  9944 Loss:  0.65876526\n",
      "Training Step:  9945 Loss:  0.6587496\n",
      "Training Step:  9946 Loss:  0.6587345\n",
      "Training Step:  9947 Loss:  0.6587189\n",
      "Training Step:  9948 Loss:  0.65870315\n",
      "Training Step:  9949 Loss:  0.65868795\n",
      "Training Step:  9950 Loss:  0.6586725\n",
      "Training Step:  9951 Loss:  0.6586577\n",
      "Training Step:  9952 Loss:  0.6586425\n",
      "Training Step:  9953 Loss:  0.6586265\n",
      "Training Step:  9954 Loss:  0.65861136\n",
      "Training Step:  9955 Loss:  0.6585957\n",
      "Training Step:  9956 Loss:  0.65858054\n",
      "Training Step:  9957 Loss:  0.65856504\n",
      "Training Step:  9958 Loss:  0.6585504\n",
      "Training Step:  9959 Loss:  0.6585341\n",
      "Training Step:  9960 Loss:  0.6585188\n",
      "Training Step:  9961 Loss:  0.65850383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step:  9962 Loss:  0.6584882\n",
      "Training Step:  9963 Loss:  0.65847266\n",
      "Training Step:  9964 Loss:  0.6584578\n",
      "Training Step:  9965 Loss:  0.6584428\n",
      "Training Step:  9966 Loss:  0.6584271\n",
      "Training Step:  9967 Loss:  0.6584114\n",
      "Training Step:  9968 Loss:  0.6583963\n",
      "Training Step:  9969 Loss:  0.6583805\n",
      "Training Step:  9970 Loss:  0.65836596\n",
      "Training Step:  9971 Loss:  0.65835035\n",
      "Training Step:  9972 Loss:  0.65833515\n",
      "Training Step:  9973 Loss:  0.65831935\n",
      "Training Step:  9974 Loss:  0.65830386\n",
      "Training Step:  9975 Loss:  0.6582889\n",
      "Training Step:  9976 Loss:  0.6582737\n",
      "Training Step:  9977 Loss:  0.6582587\n",
      "Training Step:  9978 Loss:  0.65824336\n",
      "Training Step:  9979 Loss:  0.65822726\n",
      "Training Step:  9980 Loss:  0.6582124\n",
      "Training Step:  9981 Loss:  0.6581971\n",
      "Training Step:  9982 Loss:  0.6581821\n",
      "Training Step:  9983 Loss:  0.6581663\n",
      "Training Step:  9984 Loss:  0.658151\n",
      "Training Step:  9985 Loss:  0.658136\n",
      "Training Step:  9986 Loss:  0.6581203\n",
      "Training Step:  9987 Loss:  0.6581054\n",
      "Training Step:  9988 Loss:  0.6580898\n",
      "Training Step:  9989 Loss:  0.65807414\n",
      "Training Step:  9990 Loss:  0.6580591\n",
      "Training Step:  9991 Loss:  0.65804374\n",
      "Training Step:  9992 Loss:  0.65802926\n",
      "Training Step:  9993 Loss:  0.6580132\n",
      "Training Step:  9994 Loss:  0.6579979\n",
      "Training Step:  9995 Loss:  0.65798223\n",
      "Training Step:  9996 Loss:  0.657967\n",
      "Training Step:  9997 Loss:  0.65795225\n",
      "Training Step:  9998 Loss:  0.65793705\n",
      "Training Step:  9999 Loss:  0.6579213\n",
      "[array([-0.04080853], dtype=float32), array([-0.5468793], dtype=float32), array([-0.0242051], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.placeholder(tf.float32, name = 'x')\n",
    "y = tf.placeholder(tf.float32, name = 'y')\n",
    "a = tf.Variable([.5], tf.float32, name = 'a')\n",
    "b = tf.Variable([.1], tf.float32, name = 'b')\n",
    "c = tf.Variable([.1], tf.float32, name = 'c')\n",
    "\n",
    "linear_model = a * x * x + b * x + c\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "init = tf.global_variables_initializer()\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.00001)\n",
    "train = optimizer.minimize(loss)\n",
    "sess=tf.Session()\n",
    "sess.run(init)\n",
    "for i in range(10000):\n",
    "    loss_val, train_val = sess.run([loss, train], {x: range(1, 10), y: range(0,-9,-1)})\n",
    "    if i%100: print (\"Training Step: \", i, \"Loss: \", loss_val)\n",
    "    if loss_val < 1.0e-6: break \n",
    "print(sess.run([a, b, c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
